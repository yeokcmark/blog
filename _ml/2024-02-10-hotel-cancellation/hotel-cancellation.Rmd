---
title: "hotel cancellation"
description: |
  A short description of the post.
author:
  - name: Mark Y
    url: {}
date: 2024-02-10
categories:
  - classification
  - logistic regression
  - LASSO regression
  - random forest
  - decision tree
  - naive bayes
  - knn
  - xgboost
output:
  distill::distill_article:
    toc: true
    toc_depth: 6
    toc_float: true    
    self_contained: false
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(results = FALSE, echo = TRUE, eval = FALSE, fig.width = 6, layout="l-body-outset")
```
### Introduction

I came across this dataset while reading [Julia Silge's](https://juliasilge.com/blog/hotels-recipes/) blog. It's a real treasure trove of information, especially for someone starting out in Machine Learning like me.

Detailed information on this dataset is available from this [scientific paper](https://www.sciencedirect.com/science/article/pii/S2352340918315191). In her blog, Julia used the data to build a model that predicted which hotel stays included children, and which did not.

I decided to explore something different. One of the challenges every hotel faces is cancellations. Cancellations have a big impact on revenue management strategies, because it determines how aggressively a hotel could overbook it's inventory of rooms.

```{r, eval = TRUE}

rm(list = ls())
sessionInfo()
# Set packages and dependencies
pacman::p_load("tidyverse", #for tidy data science practice
               "tidymodels", "workflows",# for tidy machine learning
               "pacman", #package manager
               "devtools", #developer tools
               "Hmisc", "skimr", "broom", "modelr",#for EDA
               "jtools", "huxtable", "interactions", # for EDA
               "ggthemes", "ggstatsplot", "GGally",
               "scales", "gridExtra", "patchwork", "ggalt", "vip",
               "ggstance", "ggfortify", # for ggplot
               "DT", "plotly", #interactive Data Viz
               # Lets install some ML related packages that will help tidymodels::
               "usemodels", "poissonreg", "agua", "sparklyr", "dials",#load computational engines
               "doParallel", # for parallel processing (speedy computation)
               "ranger", "xgboost", "glmnet", "kknn", "earth", "klaR", "discrim", "naivebayes",#random forest
               "janitor", "lubridate")
load("hotel_cancellation_data.RData")
```
### Importing the data

We start by importing the data, and do a quick summary of it using `skim`.

```{r}
df <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")
skim(df)
```

### Data Wrangling

I'm still a bit confused as to whether to handle the data wrangling "here", or at the recipe stage. Recall that there are numerous `step_*` functions to perform similar tasks as `dplyr`. I will make a note to ask Prof at our class next week.

For now, I'll use `mutate` and `dmy()` from `lubridate` to get the arrival date, as well as mutate `is_canceled` and other character predictors into factors.

```{r}
#lets get variables into correct class
data <-
  df %>%
  janitor::clean_names() %>%
  mutate(arrival_date = dmy(paste0(arrival_date_day_of_month, "-", arrival_date_month, "-", arrival_date_year)),
         #day_of_week = factor(wday(arrival_date, label = TRUE)),
         is_canceled = factor(ifelse(is_canceled == 0, "stayed", "cancelled")),
         is_repeated_guest = factor(ifelse(is_repeated_guest == 0, "first-time", "repeat")),
         children = children + babies) %>% 
  dplyr::select(-babies) %>% 
  mutate_if(is.character, as.factor)

```

### EDA


### Correlation check

None of the numeric predictors appear to be highly coorelated with each other. 

```{r}
data %>%
  select_if(is.numeric) %>%
  as.matrix(.) %>%
  rcorr() %>%
  tidy() %>%
  mutate(absCorr = abs(estimate)
  ) %>%
  arrange(desc(absCorr))

```

### Splitting the Data

As Julie Silge would call it, here is where we decide how to spend our "data budget". A word of caution here for those with incompetent hardware. The dataset is considered "large" with respect to the laptop I have, which is a "hand me down" gaming rig I inherited from my son, originally purchased in 2017.

This machine cannot handle model fitting and tuning for a dataset of this size. It would either hang, or the task would be incomplete even after 48 hours. Hence, while I contemplate investing in an upgraded computer, I did the next best thing, which was to work on a smaller sample of the original data (10 percent to be precise). I used `sample_frac()` to randomly sample 10 percent of the data, which was a reasonable size to work on.

```{r}

set.seed(2024012901)
data_split <-
  data %>% 
  dplyr::sample_frac(size = 0.1, replace = FALSE) %>% #use 10% of data due to lack of computing power
  dplyr::select(hotel, is_canceled, arrival_date, 
                market_segment, distribution_channel, reserved_room_type,
                deposit_type, customer_type, lead_time, is_repeated_guest,
                previous_cancellations, previous_bookings_not_canceled, booking_changes,
                days_in_waiting_list, adr) %>% 
  filter(adr >=0, adr < 4000) %>%  #remove negative adr and outlier
  initial_split(strata = is_canceled)

data_train <-
  data_split %>% 
  training()
data_test <-
  data_split %>% 
  testing()

data_fold <-
  data_train %>% 
  vfold_cv(v = 10, strata = is_canceled)

```

### Create recipes

I created four recipes to accommodate the models which I explored.

```{r}

base_rec <-
  recipes::recipe(formula = is_canceled ~.,
                  data = data_train) %>% 
  step_date(arrival_date, features = c("dow", "month", "year"), role = "predictors") %>% 
  update_role(arrival_date, new_role = "date") %>% 
  step_zv(all_predictors())

dummy_rec <-
  base_rec %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_numeric_predictors())

normal_rec <-
  dummy_rec %>% 
  step_normalize(all_predictors())


log_rec <-
  base_rec %>% 
  step_log(all_numeric_predictors())

```
### Build Models

I explored XX models for predicting whether a reservation would be canceled or not. This is a classification problem. The models were random forest (using ranger), classification decision tress (using rpart), k-nearest neighbor (using kknn), xgboost (using xgboost), naive bayes (using naivebayes), logistic regression (using glm) and logistic LASSO regression (using glmnet). I considered models using their preset default, as well as tuned hyper-parameters.


```{r}

# random forest
rf_spec <-
  rand_forest(trees = 1000L) %>% 
  set_engine("ranger",
             importance = "permutation") %>% 
  set_mode("classification")

rf_spec_for_tuning <-
  rf_spec %>% 
  set_args(mtry = tune(),
           min_n = tune())

# Classification Tree Model
ct_spec <- 
  decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_mode('classification') 

ct_spec_for_tuning <-
  ct_spec %>% 
  set_args(tree_depth = tune(),
           min_n = tune(), 
           cost_complexity = tune())

# knn
knn_spec <-
  nearest_neighbor() %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

knn_spec_for_tuning <-
  knn_spec %>% 
  set_args(neighbors = tune(),
           weight_func = tune(),
           dist_power = tune())

# xgboost
xgb_spec <-
  boost_tree(trees = 1000L) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec_for_tuning <-
  xgb_spec %>% 
  set_args(tree_depth = tune(),
           min_n = tune(),
           loss_reduction = tune(),                     ## first three: model complexity
           sample_size = tune(),
           mtry = tune(),         ## randomness
           learn_rate = tune())        ## randomness

# # naive bayes

naive_spec <-
  naive_Bayes() %>%
  set_engine("naivebayes",
             usepoisson = TRUE) %>%
  set_mode("classification")

naive_spec_for_tuning <-
  naive_spec %>% 
  set_args(smoothness = tune(),
           Laplace = tune())

# Logistic Regression Model
logistic_spec <- 
  logistic_reg() %>%
  set_engine(engine = 'glm') %>%
  set_mode('classification') 

# Lasso Logistic Regression Model

logistic_lasso_spec <-
  logistic_reg(mixture = 1, penalty = 1) %>% 
  set_engine(engine = 'glmnet') %>%
  set_mode('classification') 


logistic_lasso_spec_for_tuning <- 
  logistic_lasso_spec %>% 
  set_args(penalty = tune()) #we could let penalty = tune()


```
I created 3 workflow sets as the various models had different requirements, as reflected in their recipe. These were then combined together using `bind_rows` into one workflow set, named `model_set`.

```{r}
base_set <- #works
  workflow_set (
    list(base_rec, dummy_rec, log_rec), #preprocessor
    list(rf_spec, ct_spec,
         rf_spec_for_tuning, ct_spec_for_tuning), #model
    cross = TRUE) #default is cross = TRUE

dummy_set <- #works
  workflow_set (
    list(dummy_rec),
    list(knn_spec, xgb_spec, logistic_spec,
         knn_spec_for_tuning, xgb_spec_for_tuning),
    cross = TRUE)

normal_set <-
  workflow_set(
    list(normal_rec),
    list(logistic_lasso_spec,
         logistic_lasso_spec_for_tuning),
    cross = TRUE)

naive_set <- #works
  workflow_set(
    list(base_rec, log_rec),
    list(naive_spec,
         naive_spec_for_tuning),
    cross = TRUE)

model_set <-
  bind_rows(base_set, dummy_set, normal_set, naive_set)

```
### First Fit using workflow_map

Thus far, working with workflow_set and workflow_map have made testing various recipe/workflow combinations quite easy. I fitted the workflow_set to the validation set `data_fold` and tuned hyper-parameters using `tune_grid'.

```{r, eval = FALSE}
# first fit
set.seed(2024020102)
doParallel::registerDoParallel()
model_results <-
  workflow_map(model_set,
               fn = "tune_grid",
               resamples = data_fold,
               grid = 5,
               verbose = TRUE,
               control = control_grid(save_pred = TRUE))


```
You can visualize the results using an `autoplot()` as well as rank the best recipe/workflow combination using `rank_results`. 

```{r, eval = TRUE}
autoplot(model_results) + theme_bw()

# rank results
model_results %>% 
  workflowsets::rank_results(rank_metric = "roc_auc") %>% 
  filter(.metric == "roc_auc") %>% 
  print(n = 10)
```

### Tuning hyper-parameters

The top performing models, in terms of `roc_auc` were random forest, decision trees, k-nearest neighbors, and xgboost. As I had thus far been working with only 10 percent of the full dataset, I created another workflow_set to tune hyper-parameters from these 4 models, using the entire dataset and grid set to 11.

```{r}
second_tree_tuning_set <-
  workflow_set(
    list(dummy_rec),
    list(rf_spec_for_tuning, ct_spec_for_tuning,
         knn_spec_for_tuning, xgb_spec_for_tuning))

```


```{r}
set.seed(2024020701)
second_tree_tuning_results <-
  workflow_map(second_tree_tuning_set,
               fn = "tune_grid",
               resamples = data_fold,
               grid = 11,
               verbose = TRUE,
               control = control_grid(save_pred = TRUE))


autoplot(second_tree_tuning_results) + theme_bw()

second_tree_tuning_results %>% 
  workflowsets::rank_results(rank_metric = "roc_auc")


```

Its been 16 hours since the start of tuning, and my laptop is still on step 3 or 4, tuning the k-nearest neighbor model. I suppose the results would be ready tomorrow morning? Meanwhile, my laptop is feeling HOT and I can hear the cooling fans running at full speed.

Update: After 36 hours, my laptop crashed. Perhaps this was too ambitious.

## Finalize workflow and last fit

Tuning is finally done, let's extract the tuning parameters from the best performing workflow.

```{r}
tuned_parameters <-
  second_tree_tuning_results %>% 
  extract_workflow_set_result(id = "recipe_rand_forest") %>% 
  select_best(metric = "roc_auc")

```

With the tuned parameters, we can "apply" it to be best performing workflow and fit it to `data_split` using `finalize_workflow` and `last_fit` respectively.

```{r}
# finalize workflow
dummy_rec_rf_wflow <-
  workflow() %>% 
  add_recipe(dummy_rec) %>% 
  add_model(rf_spec) %>% 
  finalize_workflow(tuned_parameters)

# last fit
dummy_rec_rf_final_fit <-
  dummy_rec_rf_wflow %>% 
  last_fit(data_split)

```
### Assessing the workflow

With the final fitted workflow, we can obtain metrics for model performance using `collect_metrics`.

```{r, eval = TRUE}

dummy_rec_rf_final_fit %>% 
  collect_metrics()

```
We can also obtain a confusion matrix, which compares predictions vs truth for the whole dataset. This is a 2 by 2 matrix that compares stayed/cancelled reservations for truth vs prediction.

```{r, eval = TRUE}
dummy_rec_rf_final_fit %>% 
  collect_predictions %>% 
  conf_mat(is_canceled, .pred_class)
```

This matrix can also be visualized as a plot:

```{r, eval = TRUE}
dummy_rec_rf_final_fit %>% 
  collect_predictions %>% 
  conf_mat(is_canceled, .pred_class) %>% 
  autoplot() +
  theme_bw()
```

We are also able to analyze the model's accuracy. Since this is a binary problem, guessing would give you a 50% chance of getting it correct. Let's see how the model performed:

```{r, eval = TRUE}
dummy_rec_rf_final_fit %>% 
  collect_predictions %>% 
  accuracy(is_canceled, .pred_class)
```
Not too shabby. 78.2 percent chance of guessing correctly. At least my time and electricity wasn't wasted. But, Julia Silge says that accuracy isn't a great metric, and suggests that we look at sensitivity instead, as sensitivity tells us the "true positive" rate. Ok, let's analyze sensitivity then:

```{r, eval = TRUE}
dummy_rec_rf_final_fit %>% 
  collect_predictions %>% 
  sensitivity(is_canceled, .pred_class)
```
Our model has a sensitivity rate of 0.515. Let's take a look at its specificity, or "true negative" rate.

```{r, eval = TRUE}
dummy_rec_rf_final_fit %>% 
  collect_predictions %>% 
  specificity(is_canceled, .pred_class)
```
This gives us a much higher score of 0.943.

We can also use `autoplot` to visualize the roc_curve.

```{r, eval = TRUE}
dummy_rec_rf_final_fit %>% 
  collect_predictions %>% 
  roc_curve(is_canceled, .pred_cancelled) %>%
  autoplot()
```
Among the 4 models that we tuned the second time, the random forest model performed the best. We can visualize this as well.

```{r, eval = TRUE}
second_tree_tuning_results %>%
  collect_predictions %>%
  group_by(model) %>% 
  roc_curve(is_canceled, .pred_cancelled) %>%
  autoplot()
```

And finally, we can use VIP to visualize which were the most important features in the model.

```{r, eval = TRUE}
dummy_rec_rf_final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(geom = "point") +
  theme_bw()
```
The top 3 features determining cancellations are non-refundable deposit, lead-time, and the number of previous cancellations.

```{r}
save.image("hotel_cancellation_data.RData")
```

