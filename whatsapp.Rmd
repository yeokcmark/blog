---
title: "maRk's blog"
site: distill::distill_website
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include = FALSE, echo = FALSE}
pacman::p_load(tidyverse, lubridate, glue, forcats, knitr, rmarkdown,
               scales, gridExtra, ggthemes, ggrepel, cowplot, magick, plotrix, 
               patchwork, hexbin, distill, kableExtra, 
               data.table, styler, DT,
               quantmod,
               gtrendsR,rvest, Rcrawler, RCurl, httr2, crul, jsonlite, RSelenium,
               mapdata, maps, ggmap,
               tidytext, wordcloud2, igraph, ggraph, textdata, tm,
               jtools, huxtable, broom, modelr
               )
remotes::install_github("lchiffon/wordcloud2")
theme_set(theme_fivethirtyeight())
sessionInfo() # see what packages were loaded

### read whatsapp chat data and store as txt_data ---- ### Proceed with data
#cleaning ---- Whatsapp data is generally quite "neat". You can easily extract
#dates, participant names and text
txt_data <- read_csv("chat.txt")
colnames(txt_data)[1] = "date"
colnames(txt_data)[2] = "text"
glimpse(txt_data)


## Clean the data, extracting date, name of sender, and message
txt_data_cleaned <-
  txt_data %>%
  mutate(date = dmy(str_extract(date,
                            pattern = "[^\\[]+")),
         name = str_extract(text,
                            pattern = "(?<=\\d\\d\\:\\d\\d\\:\\d\\d\\]\\s)\\w*"),
         message = str_extract(text,
                               pattern = "(?<=\\d\\d\\:\\d\\d\\:\\d\\d\\]\\s).*"),
         name = replace(name, name == "Andy", "Andrew"),
         name = replace(name, name == "", "Andrew"),
         message = ifelse(is.na(`not even WhatsApp`),
                          message,
                          paste(message, `not even WhatsApp`)
                          ),
         message = ifelse(is.na(`can read or listen to them.`),
                          message, paste(message, `can read or listen to them.`)
                          )
         ) %>% 
  select(1,5:6) %>% 
  rowid_to_column("message_id") %>% 
  na.omit()


```

### Introduction

I'm attending the Certified Data Analytics (R) Specialist course taught by Professor Sungjong Roh at SMU Academy. In Module 3: Web scraping and Data Insights, we learnt the basics of Natural Language Processing (NLP) and Sentiment Analysis of text.

I was particularly intrigued by the topic of Sentiment Analysis, and furthered my knowledge by reading the book "Text Mining with R: A Tidy Approach" by Julia Silge & David Robinson.

![](images/cover.png){fig-align="center" width="30%"}

You may access read the book by following this [link](https://www.tidytextmining.com/).

### Project

To further hone my skills in R, basic natural language processing (NLP) and sentiment analysis, I thought that it would be cool to do a sentiment analysis of my family's Whatsapp group chat. This group chat was started in June 2019 and comprises 4 members, my wife (Angelina) , my two sons Marc (age 18) and Andrew (age 13) and me.

The chatgroup contains approximately 65,000 messages, and is used mainly to coordinate schedules, update whereabouts of each member, and chatting about anything under the sun. My end goal is to see if the tools I learnt in class could be used to provide an insight into the:

-   overall sentiment of our chatgroup. Do we generally use positive language (I hope so) or is the group environment toxic (that would be a disappointment!).

-   individual sentiment of each member of the chatgroup. Are there members who are positive/negative and does that "gel" with my knowledge of their personality? Were there periods of positive or negative sentiment throughout the year(s), and does that tie-in with any significant events the family was experiencing?

### Exporting Whatsapp Chat

This is fairly easy to do, and the chat exports as a single text file with date/timestamp. Unfortunately, stickers and emojis which are often used in our chatgroup cannot be exported.

### Import and Data Wrangling

The chat file can be imported into R using read_csv(). The chat.txt file imported from Whatsapp was relatively "clean", and required minimal wrangling.

The date of message sent, name of sender, and text message were extracted using a combination of str_extract() and regex patterns. Date was subsequently formatted using dmy(). NA data was filtered using na.omit(). Finally, a "message_id" was assigned to each row using the function rowid_to_column()

### Data Visualization

Let's find out who sent the most messages within the chatgroup.

<center>*Mirror mirror on the wall, who's the most chatty of them all?*</center>

Anyone want's to make a guess?

```{r, include = FALSE, echo = FALSE}
t1<-
  txt_data_cleaned %>%
  filter(name !="M") %>% 
  group_by(name) %>% 
  summarise(number_of_messages = n()
            ) %>% 
  mutate(percentage = round(number_of_messages/sum(number_of_messages) * 100, digits = 1)
         ) %>% 
  arrange(-number_of_messages)
kable(t1, caption = "Number of messages sent by chatgroup members",
        align = "lcc",
      booktabs = TRUE,
      col.names = c("Name", "Number of Messages", "Percentage")) %>% 
kable_styling()

```

Surprise surpise! My wife sent the most messages within the group! Let's take a look at the number of daily messages sent in the chatgroup since its inception.

```{r, include = FALSE, echo = FALSE}
fig1<-
  txt_data_cleaned %>% 
  filter(name != "M") %>% 
  ggplot(aes(x = date, color = name, na.rm = TRUE)
         ) +
  geom_freqpoly (stat =  "bin", 
                 bins = 100,
                 alpha = 0.8) +
  labs(title = "Daily number of messages sent",
       subtitle = "Frequency of messaging within the chatgroup increased after 2022."
       ) +
  theme(legend.position = "bottom",
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.5))) 

```

```{r, fig.cap="Daily Number of Messages Sent in chatgroup (2019 to date)", layout="l-body-outset"}
fig1
```

Interesting! The number of daily number of messages within the chatgroup increased after 2022. Perhaps as life returned to a "post-covid normal", messaging within the group increased as we were "out and about" much more, and required more coordination amidst our activities. Or maybe we were just more active on our phones?

### Tokenize the words

In order to run sentiment analysis on the chat, each message needed to be parsed into individual words. This process of "tokenizing" the words is easily done within R using the function *unnest_tokens()*. This resulted in approximately 515,000 tokens.

```{r, include = FALSE, echo = FALSE}
data_for_NLP_tokenized <-
  txt_data_cleaned %>% 
  unnest_tokens(input = message,
                output = word)
```

Next, I removed "stop-words" using tidytext::stop_words and anti_join(). These stop-words carry "no meaning" in sentiment analysis and are therefore removed prior to analysis. You are also able to remove names or other words you deem irrelevant by adding them to a custom lexicon. You will read more about that later...

```{r, include = FALSE, echo = FALSE}
# Remove stop words
stopwords <- tidytext::stop_words %>%
  # add_row(word = "leave", lexicon = "custom") %>%
  # add_row(word = "swim", lexicon = "custom") %>% 
  add_row(word = "andrew", lexicon = "custom") %>% 
  add_row(word = "marc", lexicon = "custom") %>% 
  add_row(word = "aaangelina", lexicon = "custom") %>% 
  add_row(word = "starhub", lexicon = "custom") %>% 
  add_row(word = "banana", lexicon = "custom") %>% 
  add_row(word = "papa", lexicon = "custom") %>% 
  add_row(word = "andy", lexicon = "custom") %>% 
  add_row(word = "yeo", lexicon = "custom") %>% 
  add_row(word = "m.a.m.a", lexicon = "custom")

data_for_NLP_no_stopwords <- 
  anti_join(data_for_NLP_tokenized, stopwords, by = join_by(word)
            )

```

### AFINN, BING and NRC

Here's a short introduction of the 3 sentiment lexicons I used to assist with sentiment analysis:

AFINN is a lexicon of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. You can read more about AFINN [here](https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_afinn.html#:~:text=AFINN%20is%20a%20lexicon%20of,and%20plus%20five%20(positive).).

Bing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative. It was first published by Minqing Hu and Bing Liu in 2004. You can read more about Bing [here](https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_bing.html).

The NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). You can read more about NRC [here](https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_nrc.html).

```{r, include = FALSE, echo = FALSE}
  
# Get Afinn, Bing and NRC sentiment lexicons
AFINN <- get_sentiments("afinn")
Bing <- get_sentiments("bing")
NRC <- get_sentiments("nrc")
```

I used these 3 "dictionaries" to perform sentiment analysis on the word tokens.

```{r, include = FALSE, echo = FALSE}
# Sentiment analysis using inner_join with the 3 lexicons

data_for_NLP_AFINN_indexed <-
  data_for_NLP_no_stopwords %>% 
  inner_join(AFINN, by = "word")

data_for_NLP_Bing_indexed <-
  data_for_NLP_no_stopwords %>% 
  inner_join(Bing, by = "word")

data_for_NLP_NRC_indexed <-
  data_for_NLP_no_stopwords %>% 
  inner_join(NRC, by = "word", relationship = "many-to-many")
```

### Lets do some very basic EDA

Using the AFINN indexed list of words (data_for_NLP_AFINN_indexed), here is the top 20 most frequently used words in our chatgroup:

```{r}
t2 <-
  data_for_NLP_AFINN_indexed %>% 
  group_by(word) %>% 
  summarise (count = n()) %>% 
  slice_max(count, n=20)

kable(t2, caption = "Top 20 Most Frequently Used Words in Chatgroup",
        align = "lcc",
      booktabs = TRUE,
      col.names = c("Word", "Count")) %>% 
kable_styling()


```

Now here's a slight problem. The word "leave", which has a frequency of 794 and carries a sentiment score of "-1" in AFINN. As our family frequently uses the chatgroup to coordinate our schedules, the word "leave" is likely taken our of context here.

For example: "Let's leave at 9am" or "We plan to leave at 7pm today." As opposed to " I have going to leave you!" which has a negative context.

Remember I mentioned above about adding "custom" stopwords? To avoid potentially skewing the results, I decided removed the word "leave" by including it as a "custom lexicon" stop-word. Thereafter, I ran the analysis using AFINN again.

Similarly, the word "swim" is found within the NRC sentiments of anticipation, fear (really? why?) and joy. As both my sons are competitive swimmers and the word "swim" is used very frequently within our chat (760 times to be exact!), "swim" was removed as well.

```{r, include = FALSE, echo = FALSE}
stopwords <- tidytext::stop_words %>%
  add_row(word = "leave", lexicon = "custom") %>%
  add_row(word = "swim", lexicon = "custom")
  
data_for_NLP_no_stopwords <- 
  anti_join(data_for_NLP_tokenized, stopwords, by = join_by(word)
            )
data_for_NLP_AFINN_indexed <-
  data_for_NLP_no_stopwords %>% 
  inner_join(AFINN, by = "word")

data_for_NLP_NRC_indexed <-
  data_for_NLP_no_stopwords %>% 
  inner_join(NRC, by = "word", relationship = "many-to-many")
```

Just for fun, and probably to show off what I learnt in class, here is a word cloud after removing the word "leave".

```{r, fig.cap = "WordCloud: Top 50 words after removing \"leave\""}

top50_words <-
  data_for_NLP_AFINN_indexed %>% 
  #filter(name == "Mark") %>% 
  group_by(word) %>% 
  summarise(frequency = n()
            ) %>% 
  arrange(-frequency) %>% 
  slice_max(frequency, n = 50) %>% 
  wordcloud2(color = "random-light",
             backgroundColor = "black")
top50_words
```

As you can see, its a rather nice chatgroup to be a part of, with positive words such as "love", "nice" and "happy". Of course, there are also negative words being used, of which "shit" stands out. I'm cautiously optimistic that the overall AFINN index of our chat will be positive. 🤞

### Have fun with word cloud

Word cloud is rather fun and it's easy to get carried away. So here's a couple more. This is a word cloud of our top 50 most frequently used words with a sentiment score \<0.

```{r, fig.cap="Top 50 Frequently used words with negative sentiment score"}
# Generating a word cloud
f3<-
  data_for_NLP_AFINN_indexed %>% 
filter(value < -0) %>% 
group_by(word)%>% 
summarise(frequency = n()
          ) %>% 
arrange(-frequency) %>% 
slice_max(frequency, n = 50) %>% 
wordcloud2(color = "random-light",
           backgroundColor = "black")

f3
```

This is a word cloud of my top 50 most frequently used words. Hey, I use "love" quite a lot :)

```{r, fig.cap = "WordCloud: Mark's Top 50 most frequently used words"}

mark50_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Mark") %>% 
  group_by(word) %>% 
  summarise(frequency = n()
            ) %>% 
  arrange(-frequency) %>% 
  slice_max(frequency, n = 50) %>% 
  wordcloud2(color = "random-light",
             backgroundColor = "black")
mark50_words
```

Here is Marc's top 50 most frequently used words. Now we know who uses the word "shit" frequently. Still, it looks relatively positive.

```{r, fig.cap = "WordCloud: Marc's Top 50 most frequently used words"}

marc50_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Marc") %>% 
  group_by(word) %>% 
  summarise(frequency = n()
            ) %>% 
  arrange(-frequency) %>% 
  slice_max(frequency, n = 50) %>% 
  wordcloud2(color = "random-light",
             backgroundColor = "black")
marc50_words
```

This is Andrew's top 50 most frequently used words. It would appear that he uses the word "love" quite frequently. 💖

```{r, results = T, fig.cap = "WordCloud: Andrew's Top 50 most frequently used words"}

andrew50_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Andrew") %>% 
  group_by(word) %>% 
  summarise(frequency = n()
            ) %>% 
  arrange(-frequency) %>% 
  slice_max(frequency, n = 50) %>% 
  wordcloud2(color = "random-light",
             backgroundColor = "black")
andrew50_words
```

Last but not least, here is a word cloud of my wife's top 50 most frequently used words.

```{r, fig.cap = "WordCloud: Angelina's Top 50 most frequently used words"}

angelina50_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Aaangelina") %>% 
  group_by(word) %>% 
  summarise(frequency = n()
            ) %>% 
  arrange(-frequency) %>% 
  slice_max(frequency, n = 50) %>% 
  wordcloud2(color = "random-light",
             backgroundColor = "black")
angelina50_words
```

### Sentiment Analysis using AFINN Index

Let's shift gears and take a deeper look at what the AFINN indexed list of words reveal about the sentiment within our chatgroup.

The list of tokenized words were analyzed using the AFINN dictionary, where each word is given a sentiment score ranging from -5 (negative sentiment) to +5 (positive sentiment).

I calculated an average sentiment score and standard error for each member of the family.

```{r}

family_mean = round(mean(data_for_NLP_AFINN_indexed$value), digits = 3)
family_se = round(std.error(data_for_NLP_AFINN_indexed$value), digits = 3)
family_median = round(median(data_for_NLP_AFINN_indexed$value), digits = 3)

mama_family_AFINN_indexed<-
  data_for_NLP_AFINN_indexed %>% 
  group_by(name) %>% 
  summarise(ave_sentiment = round(mean(value), digits = 3),
            std_error = round(std.error(value), digits = 3)) %>% 
  arrange(-ave_sentiment) %>% 
  add_row(name = "Family",
          ave_sentiment = family_mean,
          std_error = family_se)

kable(mama_family_AFINN_indexed, caption = "Average Sentiment of family members",
        align = "lcc",
      booktabs = TRUE,
      col.names = c("Name", "Average Sentiment", "Standard Error")) %>% 
kable_styling()
```

My wife and I had an average sentiment score similar to that of the overall "Family". Andrew had the highest average sentiment score (perhaps he uses the word "love" too often?) while Marc had the lowest average sentiment score.

Here is a boxplot summarizing the results,

```{r, layout="l-body-outset", fig.cap= "Sentiment Analysis of Yeo Family chatgroup (AFINN)"}
family_ave_sentiment <-
  data_for_NLP_AFINN_indexed %>% 
  group_by(name) %>% 
  summarise(ave_sentiment = round(mean(value), digits = 3),
            std_error = round(std.error(value), digits = 3)
  ) %>% 
  arrange(-ave_sentiment)

fig9 <-
  data_for_NLP_AFINN_indexed %>% 
  group_by(date, name) %>% 
  summarise(ave_sentiment = round(mean(value), digits = 3),
            std_error = round(std.error(value), digits = 3)
            ) %>% 
  ggplot(aes(x = name, 
             y = ave_sentiment)
             ) +
  geom_boxplot() +
  stat_summary(fun.data = mean_se,
               geom = "errorbar",
               color = "green",
               width = 0.5) +
  geom_point(data = family_ave_sentiment,
             aes(x = name,
                 y = ave_sentiment),
             color = "purple",
             shape = 19,
             size = 3) +
  geom_label_repel(data = family_ave_sentiment,
                   aes(x = name,
                       y = ave_sentiment,
                       label = ave_sentiment),
                   size = 2
                   )+
  coord_flip()+
  theme(legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.5)))+
  labs(title = "Sentiment Analysis of chatgroup using AFINN",
       subtitle = "Average Daily Sentiment Index for each family member",
       x = "Family members",
       y = "Average Sentiment Index")
fig9
```

The difference in average sentiment score between Marc and Andrew stands out. Andrew's AFINN sentiment score is much higher than Marc's. Does this simply imply the use of more positive language within the chat, or does it make inference to a difference in their personalities? Can the difference be attributed to age where a younger child tends to be more openly expressive with his feelings, while an older child is more "reserved"?

Personally, I am surprised that the AFINN sentiment analysis of our chats was able to quite accutately make inference to the personality of my children. Marc tends to be more reserved and neutral, which may explain his use of words that are less "emotionally charged" and balanced. On the other hand, Andrew is more outgoing, bubbly and optimistic, which may explain his higher average sentiment score. Or maybe he simply loves the word "love" which carries a sentiment score of +3.

### Cumulative Average Daily Sentiment

Next, I plotted the cumulative average daily sentiment for each member of the family. I'm not sure what I was hoping to find. If you had a neutral average daily sentiment score, the line should be flat. If you were generally positive, I'd expect an upward sloping line. Conversely, periods of sustained negative sentiment would result in a downward trend.

```{r, layout="l-body-outset", fig.cap="Cumulative Average Daily Sentiment Score (AFINN)"}
fig11<-
  data_for_NLP_AFINN_indexed %>% 
  #filter(name == "Marc") %>% 
  #filter(word == "love") %>% 
  group_by(name, date) %>% 
  summarise(ave_sentiment = round(mean(value), digits = 3)
            ) %>% 
  mutate(cumulative_sentiment = cumsum(ave_sentiment)
         ) %>% 
  ggplot(aes(x = date,
             y = cumulative_sentiment,
             group = name,
             color = name,
             na.rm = T)
         ) +
  geom_point(alpha = 0.7
             )+
  geom_line(alpha = 0.7)+
  theme(legend.position = "bottom",
        panel.background = element_blank(),
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.5))
        ) +
  labs(title = "Cumulative Average Daily Sentiment",
       color = NULL)
fig11
```

A couple of interesting observations:

1.  Andrew had a significant increase in cumulative average daily sentiment score between 2022 and 2023. We discussed this, and brainstormed for possible reasons. I felt that possibly "covid reopening" and "life back to normal" was a contributing factor to his improved sentiment. Being more outgoing, perhaps covid-restrictions had a larger impact on his sentiment. Andrew thinks that his move to a different competitive swim club with a "less toxic" culture and people might have been a contributing factor in lifting his general mood.

2.  The plot for Marc is almost flat throughout the year, reaching a peak score of only 75. This could imply days of positive average sentiment cancelling out days of negative sentiment, or simply the use of language that is "emotionally neutral". Alarmingly, the plot showed a decline from mid-2022 to the start of 2023. We brainstormed for possible reasons to explain this, but none come to mind.

Here is Marc's plot.

```{r, layout="l-body-outset", fig.cap="Marc's Cumulative Average Daily Sentiment Score (AFINN)"}
marc_fig11<-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Marc") %>% 
  #filter(word == "love") %>% 
  group_by(name, date) %>% 
  summarise(ave_sentiment = round(mean(value), digits = 3)
            ) %>% 
  mutate(cumulative_sentiment = cumsum(ave_sentiment)
         ) %>% 
  ggplot(aes(x = date,
             y = cumulative_sentiment,
             group = name,
             color = name,
             na.rm = T)
         ) +
  geom_point(alpha = 0.7
             )+
  geom_line(alpha = 0.7)+
  theme(legend.position = "bottom",
        panel.background = element_blank(),
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.5))
        ) +
  labs(title = "Marc's Cumulative Average Daily Sentiment",
       color = NULL)
marc_fig11
```

### Sentiment Analysis: Marc vs Andrew

```{r, include = F, echo = F}
andrew_total_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Andrew"
  ) %>% 
  count(name)
andrew_frequent_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Andrew"
         ) %>% 
  group_by(word) %>% 
  summarise(word_use_a = n(),
            frequency_a = n()/andrew_total_words$n
            ) %>% 
  arrange(-word_use_a)
marc_total_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Marc"
  ) %>% 
  count(name)
marc_frequent_words <-
  data_for_NLP_AFINN_indexed %>% 
  filter(name == "Marc"
  ) %>% 
  group_by(word) %>% 
  summarise(word_use_m = n(),
            frequency_m = n()/marc_total_words$n
  ) %>% 
  arrange(-word_use_m)
```

I am curious to explore the difference in sentiment scores between Marc and Andrew. To investigate further, I looked for words that they used in common, and analyzed to see if the frequency of usage of these words contributed to the difference in their sentiment scores.

To do this, I first created individual AFINN-indexed word lists for each child. Marc used 1783 AFINN-indexed words, while Andrew used 921 AFINN-indexed words. I then calculated their respective frequency of AFINN-indexed word usage.

Next, I created a list of AFINN-indexed words that they used in common using inner_join(). This resulted in a list of 138 AFINN-indexed words that they used in common.

I created a plot of these 138 words that they used in common, with the frequency of word usage by Andrew on the x-axis, and the frequency of word usage by Marc on the y-axis.

```{r, layout="l-body-outset", fig.cap="Frequency of Usage (Words in Common) Marc vs Andrew"}

marc_andrew_common_frequent_words <-
  inner_join(marc_frequent_words, andrew_frequent_words, by = "word")
marc_andrew_common_frequent_words_AFINN <-
  inner_join(marc_andrew_common_frequent_words, AFINN, by = "word") %>% 
  mutate(freq_m_value = frequency_m * value,
         freq_a_value = frequency_a * value)


fig12<-
  marc_andrew_common_frequent_words %>% 
  #filter(word != "love") %>% 
  ggplot(aes(x = frequency_a,
             y = frequency_m)
  ) +
  geom_text(aes(label = word),
            check_overlap = T,
            vjust = 1.5,
            size = 2.5) +
  geom_jitter(alpha = 0.2,
              size = 1,
              color = "red")+
  geom_abline(color = "dodgerblue",
              alpha = 0.7)+
  labs(x = "Frequency of Word Usage by Andrew",
       y = "Frequency of Word Usage by Marc",
       title = "Frequency of Usage (Words in Common)"
  ) +
  theme(legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text.x = element_text(size = rel(0.6)),
        axis.text.y = element_text(size = rel(0.6)),
        axis.title.x = element_text(size = rel(0.6)),
        axis.title.y = element_text(size = rel(0.6)))
fig12
```

Its quite apparent that the frequency of usage of the word "love" by Andrew is an outlier (32% vs 2% usage by Marc). In order to get a clearer picture of the other words in use, I removed the word "love" before generating the same plot again.

```{r, layout="l-body-outset", fig.cap="Frequency of Word Usage (Words in Common) Marc vs Andrew. The word `love` has been removed."}

fig13<-
  marc_andrew_common_frequent_words %>% 
  filter(word != "love") %>% 
  ggplot(aes(x = frequency_a,
             y = frequency_m)
  ) +
  geom_text(aes(label = word),
            check_overlap = T,
            vjust = 1.5,
            size = 2.5) +
  geom_jitter(alpha = 0.2,
              size = 1,
              color = "red")+
  geom_abline(color = "dodgerblue",
              alpha = 0.7)+
  labs(x = "Frequency of Word Usage by Andrew",
       y = "Frequency of Word Usage by Marc",
       title = "Frequency of Usage (Words in Common)",
       subtitle = "The word 'love' has been removed."
  ) +
  theme(legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.6)),
        axis.title = element_text(size = rel(0.6)))
  
fig13
```

You may notice (or if you actually counted), not all 138 (or 137 minus the word "love") words have been plotted. That is because "check_overlap" in geom_text() has been set to TRUE to avoid the text from overlapping.

Words that are close to or on the blue line (AB Line with slope = 1, intercept = 0) represent words that both children use with similar frequency. Examples of these words are "super", "died", "stupid", "happy", and "lol". Marc uses the words "nice" and "shit" more frequently than Andrew, while Andrew uses the words "haha", "yeah", and "care" more frequently than Marc.

Is it apparent now why Andrew might have a higher AFINN sentiment score in his chats? Maybe the next plot will give a clearer indication. I multiplied the frequency of each word used by its AFINN sentiment value.

```{r, layout="l-body-outset", fig.cap="Frequency of word used x AFINN Sentiment Value"}
fig14<-
marc_andrew_common_frequent_words_AFINN %>% 
  mutate(freq_value_color = ifelse(freq_a_value > 0, "positive", "negative")
         ) %>% 
  #filter(word != "love") %>% 
  ggplot(aes(x = freq_a_value,
             y = freq_m_value,
             )
         ) +
  geom_text(aes(label = word,
                color = freq_value_color),
            check_overlap = T,
            vjust = 1.5,
            size = 2.5) +
  scale_color_manual (values = c("positive" = "darkgreen",
                                "negative" = "tomato3")
                      )+
  geom_jitter(alpha = 0.2,
              size = 1
              )+
  geom_abline(color = "dodgerblue",
              alpha = 0.7)+
  scale_x_continuous(n.breaks = 20,
                     limits = c(-0.06, 1)
                     )+
  scale_y_continuous(n.breaks = 20,
                     limits = c(-0.15, 0.15)
                     ) +
  theme(legend.position = "none",
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.6)),
        axis.title = element_text(size = rel(0.6))) +
  labs(x = "Andrew",
       y = "Marc",
       title = "Frequency * AFINN Sentiment Value")
  
fig14
```

Again, we see the use or excessive use of the word "love" most likely skewing Andrew's sentiment score vs Marc, which is not a bad thing :). Let's look at the same plot with the word 'love' removed.

```{r, layout="l-body-outset", fig.cap="Frequency of word used x AFINN Sentiment Value. The word 'love' has been removed."}
fig15<-
marc_andrew_common_frequent_words_AFINN %>% 
  mutate(freq_value_color = ifelse(freq_a_value > 0, "positive", "negative")
         ) %>% 
  filter(word != "love") %>% 
  ggplot(aes(x = freq_a_value,
             y = freq_m_value,
             )
         ) +
  geom_text(aes(label = word,
                color = freq_value_color),
            check_overlap = T,
            vjust = 1.5,
            size = 2.5) +
  scale_color_manual (values = c("positive" = "darkgreen",
                                "negative" = "tomato3")
                      )+
  geom_jitter(alpha = 0.2,
              size = 1
              )+
  geom_abline(color = "dodgerblue",
              alpha = 0.7)+
  scale_x_continuous(n.breaks = 20,
                     limits = c(-0.06, 0.1)
                     )+
  scale_y_continuous(n.breaks = 20,
                     limits = c(-0.15, 0.15)
                     ) +
  theme(legend.position = "none",
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.6)),
        axis.title = element_text(size = rel(0.6))) +
  labs(x = "Andrew",
       y = "Marc",
       title = "Frequency * AFINN Sentiment Value")
  
fig15
```

### Tokenizing by n-grams

Thus far, I've only considered words as individual units, and evaluated their relationships to sentiments. However, tokenizing by single-words has its drawbacks because relationships exist between words that follow one-another.

This is because words correlate with one another, and the choice of first word can significantly change the meaning of the second word. For example, putting "not" before "love" significantly changes the meaning and therefore sentiment.

For the purpose of this project, I shall limit my analysis to pairs of words, or "bigrams". You can certainly analyze further by using "trigrams" (n=3). Hence more generally, a token comprising n words is called an "n-gram or ngram". Tokenizing on bigrams would allow me to capture and examine the immediate context around each word.

The output is word-pairs in the column bigram. The bigrams are then separated into columns word1 and word2, before filtering for numbers and stopwords. You can choose to recombine word1 and word2 back into bigrams, but I chose to leave them separated.

Here's a table of the top 20 "paired words" in terms of frequency of usage:

```{r}
stopwords <- tidytext::stop_words %>%
  # add_row(word = "leave", lexicon = "custom") %>%
  # add_row(word = "swim", lexicon = "custom") %>% 
  add_row(word = "andrew", lexicon = "custom") %>% 
  add_row(word = "marc", lexicon = "custom") %>% 
  add_row(word = "aaangelina", lexicon = "custom") %>% 
  add_row(word = "starhub", lexicon = "custom") %>% 
  add_row(word = "banana", lexicon = "custom") %>% 
  add_row(word = "papa", lexicon = "custom") %>% 
  add_row(word = "andy", lexicon = "custom") %>% 
  add_row(word = "yeo", lexicon = "custom") %>% 
  add_row(word = "m.a.m.a", lexicon = "custom")
bigram_data_for_NLP_tokenized <-
  txt_data_cleaned %>% 
  unnest_tokens(bigram, message, token = "ngrams", n=2) %>% 
  separate(bigram, c("word1", "word2"),
           sep = " ") %>% 
  #filter(!str_detect(word1, pattern = "[:punct:]")) %>% 
  #filter(!str_detect(word2, pattern = "[:punct:]")) %>% 
  filter(!str_detect(word1, pattern = "[0-9]")) %>% 
  filter(!str_detect(word2, pattern = "[0-9]")) %>% 
  filter(!word1 %in% stopwords$word) %>% 
  filter(!word2 %in% stopwords$word) %>% 
  count(word1, word2, sort = T)

t20<-
  bigram_data_for_NLP_tokenized %>% 
  slice_max(n, n=20)
kable(t20, caption = "Top 20 Bigrams",
      align = "lcc",
      booktabs = TRUE,
      col.names = c("word1", "word2", "frequency")) %>% 
kable_styling()

```

Nope! We don't have bed bugs at home. Instead, the bigrams come from our favorite good-night phrase "sleep tight, don't let the bed bugs bite".

Going beyond counting, we can actually visualize the connections and correlations between words in the chatgroup thanks to igraph.

```{r, layout="l-body-outset", fig.cap = "igraph Plot of Top 50 bigrams"}

fig16<-
bigram_data_for_NLP_tokenized %>% 
  slice_max(n, n = 50) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n),
                 show.legend = F,
                 arrow = grid::arrow(type = "closed",
                                     length = unit(0.1, "inches")),
                 end_cap = circle(0.07, "inches")
                 ) +
  geom_node_point(color = "dodgerblue", size = 3) +
  geom_node_text(aes(label = name),
                 vjust = 1,
                 hjust = 1)
fig16
```

And here's the cool looking igraph, where arrows show the direction of correlation between words. The weights of the arrows shoe the number of times each bigram is used. The graph plots the top 50 bigrams used in our chat. That's so cool!

As you can see, there aren't any "negating words" such as "not love" or bigrams with "negative-positive" sentiment within the top 50 bigrams. As the bulk of this chat happened during the covid and post-covid period, you'll see interesting bigrams like "wear mask", "stay safe" and "sore throat".

### Sentiment Analysis using Bing

Recall that Bing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.

Here is the top 10 most frequently used words in our chatgroup, indexed using Bing. Within this list, there are 9 positive sentiment words and 1 negative sentiment word.

```{r}
mama_family_bing_word_count <-
  data_for_NLP_Bing_indexed %>% 
  group_by(sentiment) %>% 
  count(word, sentiment, sort = TRUE) 

t10<-
  mama_family_bing_word_count %>% 
  ungroup() %>% 
  slice_max(n, n=10)

kable(t10, caption = "Top 10 most frequently used words, indexed by Bing",
      align = "lcc",
      booktabs = TRUE,
      col.names = c("word1", "word2", "frequency")) %>% 
kable_styling()

```

Here is a plot comparing the top 10 negative and positive sentiment words used in our chat. I'm happy to see that the use of positive sentiment words far overwhelm the use of negative sentiment words.

```{r, layout="l-body-outset", fig.cap="Top 10 Negative and Positive Sentiment Words, indexed by Bing"}

fig17<-
mama_family_bing_word_count %>% 
  slice_max(n, n=10) %>% 
  ungroup() %>% 
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)
         ) +
  geom_col()+
  coord_flip()+
  facet_wrap(.~sentiment,
             scales = "free")+
  labs(y = "Number of times used",
       x = "Words indexed by Bing",
       title = "Top 10 Negative and Positive Sentiment Words indexed by Bing") +
  theme(legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.6)),
        axis.title = element_text(size = rel(0.6))
        )
fig17
```

Here's a different way of looking at the same data, this time the usage of negative sentiment words is represented on the negative x-axis. Which way of visualization do you prefer?

```{r, layout="l-body-outset", fig.cap="Top 10 Negative and Positive Sentiment Words, indexed by Bing, represented on negative-positive scale"}
fig18<-
mama_family_bing_word_count %>% 
  slice_max(n, n=10) %>% 
  mutate(n = ifelse(sentiment == "negative", -n, n)
         ) %>% 
  ungroup() %>% 
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)
  ) +
  geom_col()+
  coord_flip()+
  labs(y = "Number of times used",
       x = "Words indexed by Bing",
       title = "Top 10 Negative and Positive Sentiment Words\n indexed by Bing") +
  theme(legend.position = "none",
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.6)),
        axis.title = element_text(size = rel(0.6)))
fig18
```

### Net Sentiment Using Bing

Although Bing classifies words in a binary manner, positive or negative, it is still possible to calculate a score that gives you an indication of the overall sentiment. Each occurrence of a positive word is given a sentiment score of "+1", while a negative word is given a sentiment score of "-1". This allows me to calculate a net sentiment score, and in this case, I did so for each member of the family.

What do you think this analysis will reveal? Are you surprised by the result?

```{r}
mama_family_net_sentiment_bing <-
  data_for_NLP_Bing_indexed %>% 
  mutate(sentiment_score = ifelse(sentiment == "negative", -1, 1)
         ) %>% 
  group_by(name) %>% 
  summarise(net_sentiment = sum(sentiment_score)
            )

kable(mama_family_net_sentiment_bing, caption = "Net Sentiment Score, indexed by Bing",
      align = "lcc",
      booktabs = TRUE,
      col.names = c("Family Member", "Net Sentiment")) %>% 
kable_styling()
```

Next, I investigated the top 10 most frequently used positive and negative words by each member of the family. The chart was easily created using facet_wrap().

```{r, layout="l-body-outset", fig.cap="Top 10 Frequently used Positive and Negative words, indexed by Bing, for each family member"}

# lets see who used what words most often (Fig 19)
top10_positive_negative_words_family_members_bing <-
  data_for_NLP_Bing_indexed %>% 
  group_by(name, sentiment) %>% 
  count(sentiment, word, sort = TRUE) %>% 
  slice_max(n, n = 10) %>% 
  mutate(n = ifelse(sentiment == "negative", -n, n)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)
         )+
  geom_col()+
  coord_flip()+
  labs(title = " ",
       subtitle = "Let's see what are the favorite words for each member of the family") +
  theme(legend.position = "none",
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.6)),
        ) +
  facet_wrap(vars(name),
             ncol = 2,
             nrow = 2,
             scales = "free"
             )

top10_positive_negative_words_family_members_bing
```

Recall from earlier, I am the top user of the word "love" within the chat group. Well, Bing picked that up too. Now, why isn't my AFINN average sentiment index much higher? Hmmm, the only plausible explanation is that I must be using negative sentiment words quite frequently as well. 🤷‍♂️

As for Marc, his number of negative sentiment words used (1038) outnumbers the number of positive sentiment words used (870). But his top 10 negative and positive words used is quite balanced.

### Sentiment Analysis using NRC

The NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). A word can be listed under multiple emotions. For example, the word "bad" is listed under the emotions anger, disgust, fear, and sadness, as well as the negative sentiment.

A total of 35,688 words from the chatgroup were indexed by NRC, and here is a summary by the eight basic emotions:

```{r}
family_nrc_emotion_proportion <-
  data_for_NLP_NRC_indexed %>% 
  filter(sentiment != "positive" & sentiment != "negative"
         ) %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(proportion = round(n / sum(n) *100, digits = 2)
  ) %>% 
  arrange (-proportion)

kable(family_nrc_emotion_proportion, caption = "Proportion of words in each NRC sentiment",
      align = "lcc",
      booktabs = TRUE,
      col.names = c("Sentiment", "Number of Words", "Frequency")) %>% 
kable_styling()
```

I always prefer to look at things visually, so here's a plot of the Top 10 words for each of the eight emotions.

```{r, layout="l-body-outset", fig.cap="Top 10 Words Categorized by NRC into Eight Basic Emotions"}

mana_family_NRC_word_count <-
  data_for_NLP_NRC_indexed %>% 
  group_by(sentiment) %>% 
  count(sentiment, word, sort = TRUE)

fig20<-
mana_family_NRC_word_count %>% 
  filter(sentiment != "positive" & sentiment != "negative"
  ) %>% 
  slice_max(n, n=10) %>% 
  ungroup() %>% 
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)
  ) +
  geom_col()+
  coord_flip()+
  facet_wrap(.~sentiment,
             scales = "free_y", nrow = 2)+
  labs(title = "Top 10 Words Categorized by NRC into Eight Basic Emotions")+
  theme(legend.position = "none",
        legend.text = element_text(size = rel(0.6)),
        legend.title = element_text(size = rel(0.6)),
        plot.title = element_text(size = rel(0.9)),
        plot.subtitle = element_text(size = rel(0.8)),
        axis.text = element_text(size = rel(0.6)),
        )
fig20
```

Its comes as no surprise to me that about 27 percent of words were categorized under the "anticipation" emotion. After all, we do use the chat quite frequently to coordinate our schedules and transportation needs. Its interesting that NRC was able to pick this up.

However, I noticed that NRC does have its quirks. Take a look at the top word in the "fear" emotion. Mum?? I know my wife is a scary person, but how did NRC know that? "Mum" to refer to my wife or my mother (or their mother, if you were my children). Perhaps in this case, NRC mistook it as "to keep quiet"? Perhaps we should spell it as "mom" from now on in our chats?

Do you notice any other strange classifications? battery \<-\> Anger, mother \<-\> Sadness are a few more that may have been incorrectly classified.

The next plot shows the relative frequency of each family member's word usage by the eight basic NRC sentiments.

```{r, layout="l-body-outset", fig.cap="Relative Frequency of Family Members' Word Usage by Sentiment, indexed by NRC", layout="l-body-outset"}

total <- sum(family_nrc_emotion_proportion$n)

# lets make a plot
fig21<- 
data_for_NLP_NRC_indexed %>% 
  filter(sentiment != "positive" & sentiment != "negative"
  ) %>% 
  group_by(sentiment, name) %>% 
  summarise (m =n()) %>% 
  mutate(frequency = m/total) %>% 
  ggplot(aes(x = fct_reorder(sentiment, frequency), y = frequency, fill = sentiment)
         ) +
  geom_col() +
  facet_wrap (.~name, ncol = 2, nrow = 2)+
  coord_flip() +
  guides(fill = guide_legend(nrow = 1)
         ) +
  theme(legend.text = element_text(size = rel(0.5)),
        legend.title = element_text(size = rel(0.5)),
        axis.title.x = element_text(size = rel(0.5)),
        axis.title.y = element_text(size = rel(0.5)),
        axis.text.y = element_text(size = rel(0.6)),
        axis.text.x = element_text(size = rel(0.5)),
        legend.position = "bottom",
        plot.title = element_text(size = rel(0.7))
        
        ) +
  labs(title = "Relative Frequency of Family Members' Word Usage by Sentiment",
       x = "Sentiments (NRC)",
       y = "Relative Frequency")

fig21
```

What do you make of the results? Well, as parents, I suppose its normal that Angelina and I have the highest frequency of words in the "anticipation" category. Its also heartening to see that categories associated with negative sentiment (fear, anger, sadness and disgust) do not rank highly among our family members.

### Final Thoughts

I had fun working on this project. Not only did my skills in R impRove, it also revealed some interesting insights about my family members.

Was the analysis ground-breaking? Definitely not, and admittedly, there was a lot more I could have done, eg: tf-idf, analysis by tri-grams, by topic, or even at the sentence level. Maybe I'll do a more comprehensive analysis after I complete the next module on Machine Learning next year. Thanks for reading!
