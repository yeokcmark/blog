[
  {
    "path": "ml/2024-02-10-hotel-cancellation/",
    "title": "hotel reservation cancellations",
    "description": "In this next exercise, we try and predict the probability that a hotel's reservations will eventually be cancelled, given information we have on hand, such as ADR, customer segment, and deposit type...",
    "author": [
      {
        "name": "Mark Y",
        "url": {}
      }
    ],
    "date": "2024-02-10",
    "categories": [
      "classification",
      "logistic regression",
      "LASSO regression",
      "random forest",
      "decision tree",
      "naive bayes",
      "knn",
      "xgboost"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nImporting the data\r\nData Wrangling\r\nEDA\r\nCorrelation check\r\nSplitting the Data\r\nCreate recipes\r\nBuild Models\r\nFirst Fit using workflow_map\r\nTuning hyper-parameters\r\nFinalize workflow and last fit\r\nAssessing the workflow\r\n\r\nIntroduction\r\nI came across this dataset while reading Julia Silge’s blog. It’s a real treasure trove of information, especially for someone starting out in Machine Learning like me.\r\nDetailed information on this dataset is available from this scientific paper. In her blog, Julia used the data to build a model that predicted which hotel stays included children, and which did not.\r\nI decided to explore something different. One of the challenges every hotel faces is cancellations. Cancellations have a big impact on revenue management strategies, because it determines how aggressively a hotel could overbook it’s inventory of rooms.\r\n\r\n\r\nrm(list = ls())\r\nsessionInfo()\r\n# Set packages and dependencies\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\",# for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\",#load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", \"kknn\", \"earth\", \"klaR\", \"discrim\", \"naivebayes\",#random forest\r\n               \"janitor\", \"lubridate\")\r\nload(\"hotel_cancellation_data.RData\")\r\n\r\n\r\nImporting the data\r\nWe start by importing the data, and do a quick summary of it using skim. Once again, I am also importing hotel_cancellation_data.RData and setting eval = FALSE for many of the code chunks to reduce the computational resources and time required to render this page.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\r\nskim(df)\r\n\r\n\r\nData Wrangling\r\nI’m still a bit confused as to whether to handle the data wrangling “here”, or at the recipe stage. Recall that there are numerous step_* functions to perform similar tasks as dplyr. I will make a note to ask Prof at our class next week.\r\nFor now, I’ll use mutate and dmy() from lubridate to get the arrival date, as well as mutate is_canceled and other character predictors into factors.\r\n\r\n\r\n#lets get variables into correct class\r\ndata <-\r\n  df %>%\r\n  janitor::clean_names() %>%\r\n  mutate(arrival_date = dmy(paste0(arrival_date_day_of_month, \"-\", arrival_date_month, \"-\", arrival_date_year)),\r\n         #day_of_week = factor(wday(arrival_date, label = TRUE)),\r\n         is_canceled = factor(ifelse(is_canceled == 0, \"stayed\", \"cancelled\")),\r\n         is_repeated_guest = factor(ifelse(is_repeated_guest == 0, \"first-time\", \"repeat\")),\r\n         children = children + babies) %>% \r\n  dplyr::select(-babies) %>% \r\n  mutate_if(is.character, as.factor)\r\n\r\n\r\nEDA\r\nCorrelation check\r\nNone of the numeric predictors appear to be highly correlated with each other.\r\n\r\n\r\ndata %>%\r\n  select_if(is.numeric) %>%\r\n  as.matrix(.) %>%\r\n  rcorr() %>%\r\n  tidy() %>%\r\n  mutate(absCorr = abs(estimate)) %>%\r\n  arrange(desc(absCorr)) %>% \r\n  dplyr::select(-estimate, -n, - p.value) %>% \r\n  DT::datatable() %>% \r\n  formatRound(\"absCorr\", digits = 3)\r\n\r\n\r\n\r\nSplitting the Data\r\nAs Julie Silge would call it, here is where we decide how to spend our “data budget”. A word of caution here for those with incompetent hardware. The dataset is considered “large” with respect to the laptop I have, which is a “hand me down” gaming rig I inherited from my son, originally purchased in 2017.\r\nThis machine cannot handle model fitting and tuning for a dataset of this size. It would either hang, or the task would be incomplete even after 48 hours. Hence, while I contemplate investing in an upgraded computer, I did the next best thing, which was to work on a smaller sample of the original data (10 percent to be precise). I used sample_frac() to randomly sample 10 percent of the data, which was a reasonable size to work on.\r\n\r\n\r\nset.seed(2024012901)\r\ndata_split <-\r\n  data %>% \r\n  dplyr::sample_frac(size = 0.1, replace = FALSE) %>% #use 10% of data due to lack of computing power\r\n  dplyr::select(hotel, is_canceled, arrival_date, \r\n                market_segment, distribution_channel, reserved_room_type,\r\n                deposit_type, customer_type, lead_time, is_repeated_guest,\r\n                previous_cancellations, previous_bookings_not_canceled, booking_changes,\r\n                days_in_waiting_list, adr) %>% \r\n  filter(adr >=0, adr < 4000) %>%  #remove negative adr and outlier\r\n  initial_split(strata = is_canceled)\r\n\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\n\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = is_canceled)\r\n\r\n\r\nCreate recipes\r\nI created four recipes to accommodate the models which I explored.\r\n\r\n\r\nbase_rec <-\r\n  recipes::recipe(formula = is_canceled ~.,\r\n                  data = data_train) %>% \r\n  step_date(arrival_date, features = c(\"dow\", \"month\", \"year\"), role = \"predictors\") %>% \r\n  update_role(arrival_date, new_role = \"date\") %>% \r\n  step_zv(all_predictors())\r\n\r\ndummy_rec <-\r\n  base_rec %>% \r\n  step_dummy(all_nominal_predictors()) %>% \r\n  step_zv(all_numeric_predictors())\r\n\r\nnormal_rec <-\r\n  dummy_rec %>% \r\n  step_normalize(all_predictors())\r\n\r\n\r\nlog_rec <-\r\n  base_rec %>% \r\n  step_log(all_numeric_predictors())\r\n\r\n\r\nBuild Models\r\nI explored 7 models for predicting whether a reservation would be canceled or not. This is a classification problem. The models were random forest (using ranger), classification decision tress (using rpart), k-nearest neighbor (using kknn), xgboost (using xgboost), naive bayes (using naivebayes), logistic regression (using glm) and logistic LASSO regression (using glmnet). I considered models using their preset default, as well as tuned hyper-parameters.\r\n\r\n\r\n# random forest\r\nrf_spec <-\r\n  rand_forest(trees = 1000L) %>% \r\n  set_engine(\"ranger\",\r\n             importance = \"permutation\") %>% \r\n  set_mode(\"classification\")\r\n\r\nrf_spec_for_tuning <-\r\n  rf_spec %>% \r\n  set_args(mtry = tune(),\r\n           min_n = tune())\r\n\r\n# Classification Tree Model\r\nct_spec <- \r\n  decision_tree() %>%\r\n  set_engine(engine = 'rpart') %>%\r\n  set_mode('classification') \r\n\r\nct_spec_for_tuning <-\r\n  ct_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(), \r\n           cost_complexity = tune())\r\n\r\n# knn\r\nknn_spec <-\r\n  nearest_neighbor() %>% \r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"classification\")\r\n\r\nknn_spec_for_tuning <-\r\n  knn_spec %>% \r\n  set_args(neighbors = tune(),\r\n           weight_func = tune(),\r\n           dist_power = tune())\r\n\r\n# xgboost\r\nxgb_spec <-\r\n  boost_tree(trees = 1000L) %>% \r\n  set_engine(\"xgboost\") %>% \r\n  set_mode(\"classification\")\r\n\r\nxgb_spec_for_tuning <-\r\n  xgb_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(),\r\n           loss_reduction = tune(),\r\n           sample_size = tune(),\r\n           mtry = tune(),\r\n           learn_rate = tune())\r\n\r\n# # naive bayes\r\n\r\nnaive_spec <-\r\n  naive_Bayes() %>%\r\n  set_engine(\"naivebayes\",\r\n             usepoisson = TRUE) %>%\r\n  set_mode(\"classification\")\r\n\r\nnaive_spec_for_tuning <-\r\n  naive_spec %>% \r\n  set_args(smoothness = tune(),\r\n           Laplace = tune())\r\n\r\n# Logistic Regression Model\r\nlogistic_spec <- \r\n  logistic_reg() %>%\r\n  set_engine(engine = 'glm') %>%\r\n  set_mode('classification') \r\n\r\n# Lasso Logistic Regression Model\r\n\r\nlogistic_lasso_spec <-\r\n  logistic_reg(mixture = 1, penalty = 1) %>% \r\n  set_engine(engine = 'glmnet') %>%\r\n  set_mode('classification') \r\n\r\n\r\nlogistic_lasso_spec_for_tuning <- \r\n  logistic_lasso_spec %>% \r\n  set_args(penalty = tune()) #we could let penalty = tune()\r\n\r\n\r\nI created 3 workflow sets as the various models had different requirements, as reflected in their recipe. These were then combined together using bind_rows into one workflow set, named model_set.\r\n\r\n\r\nbase_set <- #works\r\n  workflow_set (\r\n    list(base_rec, dummy_rec, log_rec), #preprocessor\r\n    list(rf_spec, ct_spec,\r\n         rf_spec_for_tuning, ct_spec_for_tuning), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\ndummy_set <- #works\r\n  workflow_set (\r\n    list(dummy_rec),\r\n    list(knn_spec, xgb_spec, logistic_spec,\r\n         knn_spec_for_tuning, xgb_spec_for_tuning),\r\n    cross = TRUE)\r\n\r\nnormal_set <-\r\n  workflow_set(\r\n    list(normal_rec),\r\n    list(logistic_lasso_spec,\r\n         logistic_lasso_spec_for_tuning),\r\n    cross = TRUE)\r\n\r\nnaive_set <- #works\r\n  workflow_set(\r\n    list(base_rec, log_rec),\r\n    list(naive_spec,\r\n         naive_spec_for_tuning),\r\n    cross = TRUE)\r\n\r\nmodel_set <-\r\n  bind_rows(base_set, dummy_set, normal_set, naive_set)\r\n\r\n\r\nFirst Fit using workflow_map\r\nThus far, working with workflow_set and workflow_map have made testing various recipe/workflow combinations quite easy. I fitted the workflow_set to the validation set data_fold and tuned hyper-parameters using `tune_grid’.\r\n\r\n\r\n# first fit\r\nset.seed(2024020102)\r\ndoParallel::registerDoParallel()\r\nmodel_results <-\r\n  workflow_map(model_set,\r\n               fn = \"tune_grid\",\r\n               resamples = data_fold,\r\n               grid = 5,\r\n               verbose = TRUE,\r\n               control = control_grid(save_pred = TRUE))\r\n\r\n\r\nYou can visualize the results using an autoplot() as well as rank the best recipe/workflow combination using rank_results.\r\n\r\n\r\nautoplot(model_results) + theme_bw() + theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\n\r\n# rank results\r\nmodel_results %>% \r\n  workflowsets::rank_results(rank_metric = \"roc_auc\") %>% \r\n  filter(.metric == \"roc_auc\") %>% \r\n  print(n = 10)\r\n\r\n# A tibble: 67 × 9\r\n   wflow_id     .config .metric  mean std_err     n preprocessor model\r\n   <chr>        <chr>   <chr>   <dbl>   <dbl> <int> <chr>        <chr>\r\n 1 recipe_3_ra… Prepro… roc_auc 0.856 0.00531    10 recipe       rand…\r\n 2 recipe_1_ra… Prepro… roc_auc 0.856 0.00531    10 recipe       rand…\r\n 3 recipe_3_ra… Prepro… roc_auc 0.855 0.00548    10 recipe       rand…\r\n 4 recipe_1_ra… Prepro… roc_auc 0.855 0.00548    10 recipe       rand…\r\n 5 recipe_1_ra… Prepro… roc_auc 0.855 0.00554    10 recipe       rand…\r\n 6 recipe_3_ra… Prepro… roc_auc 0.855 0.00552    10 recipe       rand…\r\n 7 recipe_2_ra… Prepro… roc_auc 0.855 0.00538    10 recipe       rand…\r\n 8 recipe_2_ra… Prepro… roc_auc 0.854 0.00541    10 recipe       rand…\r\n 9 recipe_2_ra… Prepro… roc_auc 0.853 0.00540    10 recipe       rand…\r\n10 recipe_3_ra… Prepro… roc_auc 0.853 0.00526    10 recipe       rand…\r\n# ℹ 57 more rows\r\n# ℹ 1 more variable: rank <int>\r\n\r\nTuning hyper-parameters\r\nThe top performing models, in terms of roc_auc were random forest, decision trees, k-nearest neighbors, and xgboost. As I had thus far been working with only 10 percent of the full dataset, I created another workflow_set to tune hyper-parameters from these 4 models, using the entire dataset and grid set to 11.\r\n\r\n\r\nsecond_tree_tuning_set <-\r\n  workflow_set(\r\n    list(dummy_rec),\r\n    list(rf_spec_for_tuning, ct_spec_for_tuning,\r\n         knn_spec_for_tuning, xgb_spec_for_tuning))\r\n\r\n\r\n\r\n\r\nset.seed(2024020701)\r\nsecond_tree_tuning_results <-\r\n  workflow_map(second_tree_tuning_set,\r\n               fn = \"tune_grid\",\r\n               resamples = data_fold,\r\n               grid = 11,\r\n               verbose = TRUE,\r\n               control = control_grid(save_pred = TRUE))\r\n\r\n\r\n\r\n\r\nautoplot(second_tree_tuning_results) + theme_bw() + theme(legend.position = \"bottom\")\r\n\r\n\r\nsecond_tree_tuning_results %>% \r\n  workflowsets::rank_results(rank_metric = \"roc_auc\") %>% \r\n  print(n=10)\r\n\r\n# A tibble: 88 × 9\r\n   wflow_id     .config .metric  mean std_err     n preprocessor model\r\n   <chr>        <chr>   <chr>   <dbl>   <dbl> <int> <chr>        <chr>\r\n 1 recipe_rand… Prepro… accura… 0.786 0.00474    10 recipe       rand…\r\n 2 recipe_rand… Prepro… roc_auc 0.855 0.00541    10 recipe       rand…\r\n 3 recipe_rand… Prepro… accura… 0.788 0.00448    10 recipe       rand…\r\n 4 recipe_rand… Prepro… roc_auc 0.854 0.00545    10 recipe       rand…\r\n 5 recipe_rand… Prepro… accura… 0.785 0.00456    10 recipe       rand…\r\n 6 recipe_rand… Prepro… roc_auc 0.853 0.00540    10 recipe       rand…\r\n 7 recipe_rand… Prepro… accura… 0.786 0.00445    10 recipe       rand…\r\n 8 recipe_rand… Prepro… roc_auc 0.853 0.00546    10 recipe       rand…\r\n 9 recipe_boos… Prepro… accura… 0.783 0.00524    10 recipe       boos…\r\n10 recipe_boos… Prepro… roc_auc 0.852 0.00515    10 recipe       boos…\r\n# ℹ 78 more rows\r\n# ℹ 1 more variable: rank <int>\r\n\r\nIts been 16 hours since the start of tuning, and my laptop is still on step 3 or 4, tuning the k-nearest neighbor model. I suppose the results would be ready tomorrow morning? Meanwhile, my laptop is feeling HOT and I can hear the cooling fans running at full speed.\r\nUpdate: After 36 hours, my laptop crashed. Perhaps this was too ambitious. To to recap, the above “tuned results” were based on the “original” 10 percent dataset. I think I will need to upgrade my computing resources sooner rather than later.\r\nFinalize workflow and last fit\r\nTuning is finally done, let’s extract the tuning parameters from the best performing workflow.\r\n\r\n\r\ntuned_parameters <-\r\n  second_tree_tuning_results %>% \r\n  extract_workflow_set_result(id = \"recipe_rand_forest\") %>% \r\n  select_best(metric = \"roc_auc\")\r\n\r\n\r\nWith the tuned parameters, we can “apply” it to be best performing workflow and fit it to data_split using finalize_workflow and last_fit respectively.\r\n\r\n\r\n# finalize workflow\r\ndummy_rec_rf_wflow <-\r\n  workflow() %>% \r\n  add_recipe(dummy_rec) %>% \r\n  add_model(rf_spec) %>% \r\n  finalize_workflow(tuned_parameters)\r\n\r\n# last fit\r\ndummy_rec_rf_final_fit <-\r\n  dummy_rec_rf_wflow %>% \r\n  last_fit(data_split)\r\n\r\n\r\nAssessing the workflow\r\nWith the final fitted workflow, we can obtain metrics for model performance using collect_metrics.\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  collect_metrics()\r\n\r\n    ┌────────────────────────────────────────────────────┐\r\n    │ .metric    .estimator   .estimate   .config        │\r\n    ├────────────────────────────────────────────────────┤\r\n    │ accuracy   binary           0.782   Preprocessor1_ │\r\n    │                                     Model1         │\r\n    │ roc_auc    binary           0.837   Preprocessor1_ │\r\n    │                                     Model1         │\r\n    └────────────────────────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate, .config\r\n\r\nWe can also obtain a confusion matrix, which compares predictions vs truth for the whole dataset. This is a 2 by 2 matrix that compares stayed/cancelled reservations for truth vs prediction.\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  conf_mat(is_canceled, .pred_class)\r\n\r\n           Truth\r\nPrediction  cancelled stayed\r\n  cancelled       577    106\r\n  stayed          544   1759\r\n\r\nThis matrix can also be visualized as a plot:\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  conf_mat(is_canceled, .pred_class) %>% \r\n  autoplot() +\r\n  theme_bw()\r\n\r\n\r\n\r\nWe are also able to analyze the model’s accuracy. Since this is a binary problem, guessing would give you a 50% chance of getting it correct. Let’s see how the model performed:\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  accuracy(is_canceled, .pred_class)\r\n\r\n            ┌───────────────────────────────────┐\r\n            │ .metric    .estimator   .estimate │\r\n            ├───────────────────────────────────┤\r\n            │ accuracy   binary           0.782 │\r\n            └───────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate\r\n\r\nNot too shabby. 78.2 percent chance of guessing correctly. At least my time and electricity wasn’t wasted. But, Julia Silge says that accuracy isn’t a great metric, and suggests that we look at sensitivity instead, as sensitivity tells us the “true positive” rate. Ok, let’s analyze sensitivity then:\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  sensitivity(is_canceled, .pred_class)\r\n\r\n           ┌──────────────────────────────────────┐\r\n           │ .metric       .estimator   .estimate │\r\n           ├──────────────────────────────────────┤\r\n           │ sensitivity   binary           0.515 │\r\n           └──────────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate\r\n\r\nOur model has a sensitivity rate of 0.515. Let’s take a look at its specificity, or “true negative” rate.\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  specificity(is_canceled, .pred_class)\r\n\r\n           ┌──────────────────────────────────────┐\r\n           │ .metric       .estimator   .estimate │\r\n           ├──────────────────────────────────────┤\r\n           │ specificity   binary           0.943 │\r\n           └──────────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate\r\n\r\nThis gives us a much higher score of 0.943.\r\nWe can also use autoplot to visualize the roc_curve.\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  roc_curve(is_canceled, .pred_cancelled) %>%\r\n  autoplot()\r\n\r\n\r\n\r\nAmong the 4 models that we tuned the second time, the random forest model performed the best. We can visualize the roc_curve for all 4 models.\r\n\r\n\r\nsecond_tree_tuning_results %>%\r\n  collect_predictions %>%\r\n  group_by(model) %>% \r\n  roc_curve(is_canceled, .pred_cancelled) %>%\r\n  autoplot() %>% \r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\nAnd finally, we can use VIP to visualize which were the most important features in the model.\r\n\r\n\r\ndummy_rec_rf_final_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  vip(geom = \"point\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nThe top 3 features determining cancellations are non-refundable deposit, lead-time, and the number of previous cancellations.\r\n\r\n\r\nsave.image(\"hotel_cancellation_data.RData\")\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-02-10-hotel-cancellation/hotel-cancellation_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2024-02-11T11:50:14+08:00",
    "input_file": "hotel-cancellation.knit.md",
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "ml/2024-02-09-hdb-rental-prices/",
    "title": "HDB Rental Prices",
    "description": "Let's try and predict the mediaa rental price per sqft of HDA flats given information about their location, age, distance to MRT station....",
    "author": [
      {
        "name": "Mark Y",
        "url": {}
      }
    ],
    "date": "2024-02-09",
    "categories": [
      "regression",
      "linear regression",
      "random forest",
      "knn",
      "xgboost"
    ],
    "contents": "\r\n\r\nContents\r\nPredicting HDB Rental Prices\r\nSet dependenies and load necessary packages\r\nImport the data\r\nCorrelation check\r\nEDA\r\nSplit the data\r\nLet’s make some recipes\r\nMake some models\r\nFirst fit and tuning\r\nVisualize results\r\nFinalize workflow and last fit\r\nCollect predictions\r\nVisualize important features\r\n\r\nPredicting HDB Rental Prices\r\nI started Module 1 of SMU Academy’s Predictive Analytics and Machine Learning class in January 2024. This data set was given in class for homework. I shall use it as practice to document what was learnt in class.\r\nAt this stage of my learning, the primary objective of this exercise is to familiarize myself with the complete workflow from start to finish. Repeatedly fine-tuning hyper-parameters, or complex feature engineering in order to come up with the BEST predictive model, is not an objective at this stage.\r\nSet dependenies and load necessary packages\r\nAs some of the code takes quite a bit of time to run, I’ve loaded the RData file to speed things up.\r\n\r\n\r\nrm(list = ls())\r\nsessionInfo()\r\n\r\n# Set packages and dependencies\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\",# for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\", #load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", #random forest\r\n               \"janitor\")\r\nload(\"rental_data.RData\")\r\n\r\n\r\nImport the data\r\nRead in the dataset rental_price.csv, and use skim() to provide a quick summary and overview of it.\r\n\r\n\r\n# read in data\r\ndf <- read_csv(\"rental_price.csv\")\r\nskim(df)\r\n\r\n\r\nProperty name and region (which are currently characters) will need to be reclassified as factors. The same goes for ref_year and district. unit_id will be assigned a role of “an id” later in the recipe.\r\n\r\n\r\n# make factors\r\ndata <-\r\n  df %>% \r\n  mutate(across(c(district, region, ref_year, property), as.factor))\r\n\r\n\r\nCorrelation check\r\nLet’s check for correlation among numeric variables.\r\n\r\n\r\n# check correlation between numeric, exclude loc_x, loc_y\r\n\r\ndata %>% \r\n  select (-unit_id, -loc_x, -loc_y) %>% \r\n  select_if(is.numeric) %>% \r\n  as.matrix(.) %>% \r\n  rcorr() %>% \r\n  tidy() %>% \r\n  arrange(desc(abs(estimate)))\r\n\r\n   ┌─────────────────────────────────────────────────────┐\r\n   │ column1      column2      estimate      n   p.value │\r\n   ├─────────────────────────────────────────────────────┤\r\n   │ price_medi   age           -0.479    3128    0      │\r\n   │ an                                                  │\r\n   │ price_medi   dist_to_mr    -0.203    3128    0      │\r\n   │ an           t                                      │\r\n   │ dist_to_mr   age            0.0331   3128    0.0644 │\r\n   │ t                                                   │\r\n   └─────────────────────────────────────────────────────┘\r\nColumn names: column1, column2, estimate, n, p.value\r\n\r\nCorrelation among numeric variables appear to be fine. None seem to be highly correlated with one another.\r\nEDA\r\nBefore we proceed with splitting the data, let’s do some basic EDA.\r\n\r\n\r\n# price by region\r\n\r\ndata %>% \r\n  ggplot(aes(x = district,\r\n              y = price_median)\r\n         ) + \r\n  geom_boxplot(outlier.shape = NA) +\r\n  geom_point(alpha = 0.15, aes(color = region))+\r\n  theme_bw() +\r\n  labs(x = \"District\",\r\n       y = \"Price Median $ psf\")+\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\nHere we see the influence of location (as specified by district and region) on price.\r\n\r\n\r\ndata %>% \r\n  ggplot(aes(x = dist_to_mrt,\r\n             y = price_median,\r\n             color = region)\r\n         ) +\r\n  geom_point(alpha = 0.2) +\r\n  geom_smooth(aes(color = region),\r\n              method = \"lm\")+\r\n  theme_bw()+\r\n  labs(x = \"Distance to MRT Station (km)\",\r\n       y = \"Price Median $ psf\") +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\nDistance to MRT appears to have a stronger influence on price in region_CCR, and a more muted influence on price in region_OCR and region_RCR.\r\nSplit the data\r\nLet’s proceed with splitting the data into a training and testing set. At the same time, let’s create folds for tuning/cross validation purposes.\r\n\r\n\r\n# split data\r\nset.seed(2024020101)\r\ndata_split <-\r\n  data %>% \r\n  initial_split(strata = region) # strata by region\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = region)\r\n\r\n\r\nLet’s make some recipes\r\nLet’s make a few recipes for modeling.\r\n\r\n\r\nbase_rec <-\r\n  recipes::recipe(formula = price_median ~ .,\r\n                 data = data_train) %>% \r\n  update_role(unit_id, new_role = \"unit_id\") %>% \r\n  update_role(property, new_role = \"name_id\") %>% \r\n  step_rm(loc_x, loc_y) %>%  # remove geo locator, property name\r\n  step_dummy(all_nominal_predictors())\r\n\r\nlog_rec <-\r\n  base_rec %>%\r\n  step_log(price_median, age, dist_to_mrt) # age, dist_mrt\r\n\r\n\r\nMake some models\r\nLet’s make a few models to predict price_median, which is the rental price per square foot for each property. We shall set up 4 models: a simple linear regression model, random forest, xgboost, and k-nearest neighbor model. I will set up a basic model, as well as a model for tuning of hyper-parameters.\r\n\r\n\r\n# linear regression\r\nlm_spec <-\r\n  linear_reg()\r\n\r\n# random forest\r\nrf_spec <-\r\n  rand_forest(trees = 1000L) %>% \r\n  set_engine(\"ranger\") %>% \r\n  set_mode(\"regression\")\r\n\r\nrf_spec_for_tuning <-\r\n  rf_spec %>% \r\n  set_args(mtry = tune(),\r\n           min_n = tune())\r\n\r\n# knn\r\nknn_spec <-\r\n  nearest_neighbor() %>% \r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"regression\")\r\n\r\nknn_spec_for_tuning <-\r\n  knn_spec %>% \r\n  set_args(neighbors = tune(),\r\n           weight_func = \"optimal\",\r\n           dist_power = tune())\r\n\r\n# xgboost\r\nxgb_spec <-\r\n  boost_tree(trees = 1000L) %>% \r\n  set_engine(\"xgboost\") %>% \r\n  set_mode(\"regression\")\r\n\r\nxgb_spec_for_tuning <-\r\n  xgb_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(),\r\n           loss_reduction = tune(),                     \r\n           sample_size = tune(),\r\n           mtry = tune(),        \r\n           learn_rate = tune())        \r\n\r\n\r\nLet’s combine everything into a workflow set. xgb needs a separate workflow set as it only works with dummy variables. Combine both using bind_rows into 1 model_set.\r\n\r\n\r\nbase_set <-\r\n  workflow_set (\r\n    preproc = list(base_rec, log_rec), #preprocessor\r\n    models = list(rf_spec, knn_spec, xgb_spec,\r\n                  rf_spec_for_tuning, knn_spec_for_tuning, xgb_spec_for_tuning), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\n\r\nFirst fit and tuning\r\nLet’s use fit_resamples to do a first fit as well as tune the hyper-parameters. There are still lots of things I don’t know. For example, what does “grid = 11” mean? Why not 5? Why not 25?\r\n\r\n\r\n# first fit\r\nset.seed(2024020102)\r\ndoParallel::registerDoParallel()\r\nbase_results <-\r\n  workflow_map(base_set,\r\n               fn = \"tune_grid\",\r\n               resamples = data_fold,\r\n               grid = 11,\r\n               verbose = TRUE)\r\n\r\n\r\nOnce your machine is done tuning for hyper-parameters, you can collect the results, by metrics. Here we used “rmse”. My machine is rather incompetent, so this process is taking a while. Fortunately, the dataset is rather small.\r\n\r\n\r\nbase_results %>% \r\n  collect_metrics() %>% \r\n  filter(.metric == \"rmse\") %>% \r\n  arrange(mean) %>% \r\n  print(n = 5)\r\n\r\n# A tibble: 72 × 9\r\n  wflow_id       .config preproc model .metric .estimator   mean     n\r\n  <chr>          <chr>   <chr>   <chr> <chr>   <chr>       <dbl> <int>\r\n1 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0559    10\r\n2 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0565    10\r\n3 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0598    10\r\n4 recipe_2_rand… Prepro… recipe  rand… rmse    standard   0.0670    10\r\n5 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0684    10\r\n# ℹ 67 more rows\r\n# ℹ 1 more variable: std_err <dbl>\r\n\r\nVisualize results\r\nWe can also visualize the results using a basic autoplot(), or rank the results using workflowsets::rank_results().\r\n\r\n\r\nautoplot(base_results) +\r\n  theme_bw() +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\n\r\n# rank results\r\nbase_results %>% \r\n  workflowsets::rank_results(rank_metric = \"rmse\") %>% \r\n  filter(.metric == \"rmse\") %>% \r\n  print(n = 5)\r\n\r\n# A tibble: 72 × 9\r\n  wflow_id     .config .metric   mean std_err     n preprocessor model\r\n  <chr>        <chr>   <chr>    <dbl>   <dbl> <int> <chr>        <chr>\r\n1 recipe_2_bo… Prepro… rmse    0.0559 1.15e-3    10 recipe       boos…\r\n2 recipe_2_bo… Prepro… rmse    0.0565 9.01e-4    10 recipe       boos…\r\n3 recipe_2_bo… Prepro… rmse    0.0598 1.56e-3    10 recipe       boos…\r\n4 recipe_2_ra… Prepro… rmse    0.0670 1.36e-3    10 recipe       rand…\r\n5 recipe_2_bo… Prepro… rmse    0.0684 1.81e-3    10 recipe       boos…\r\n# ℹ 67 more rows\r\n# ℹ 1 more variable: rank <int>\r\n\r\nThe best recipe-model combination is recipe 2, which is log_rec and parameters from boost_tree_3. If you recall, boost_tree_3 is the basic xgboost model with no hyper-parameters for tuning. Let’s extract the hyper-parameters using select_best.\r\n\r\n\r\ntune_param <-\r\n  base_results %>% \r\n  extract_workflow_set_result(\"recipe_2_boost_tree_3\") %>% \r\n  select_best(metric = \"rmse\")\r\n\r\n\r\nFinalize workflow and last fit\r\nLet’s apply the parameters and finalize the workflow for the best recipe/model combination.\r\n\r\n\r\nlog_rec_xbg_boost_wflow <-\r\n  workflow() %>% \r\n  add_recipe(log_rec) %>% \r\n  add_model(xgb_spec) %>% \r\n  finalize_workflow(tune_param)\r\n\r\n\r\nFinally, we can fit the model by performing last_fit to the split data, data_split.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit <-\r\n  log_rec_xbg_boost_wflow %>% \r\n  last_fit(data_split)\r\n\r\n\r\nCollect predictions\r\nWe can collect metrics as well as predictions from the last fitted model.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  collect_metrics()\r\n\r\n    ┌───────────────────────────────────────────────────┐\r\n    │ .metric   .estimator   .estimate   .config        │\r\n    ├───────────────────────────────────────────────────┤\r\n    │ rmse      standard         0.057   Preprocessor1_ │\r\n    │                                    Model1         │\r\n    │ rsq       standard         0.959   Preprocessor1_ │\r\n    │                                    Model1         │\r\n    └───────────────────────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate, .config\r\n\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  collect_predictions() %>% \r\n  ggplot(aes(x = price_median,\r\n             y = .pred)) +\r\n  geom_point(alpha = 0.2)+\r\n  geom_abline(lty = 2,\r\n              color = \"dodgerblue\") +\r\n  labs(x = \"Actual Median Price $psf\",\r\n       y = \"Predicted Median Price $psf\")+\r\n  theme_bw()\r\n\r\n\r\n\r\nVisualize important features\r\nWe can use the vip() to see which were the most important features.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  vip(geom = \"point\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nAs we can see, location location location, or in this case region is the most important predictor of price, followed by age and distance to MRT station.\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-02-09-hdb-rental-prices/hdb-rental-prices_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2024-02-11T11:47:03+08:00",
    "input_file": "hdb-rental-prices.knit.md",
    "preview_width": 1152,
    "preview_height": 768
  }
]
