[
  {
    "path": "ml/2024-02-09-hdb-rental-prices/",
    "title": "HDB Rental Prices",
    "description": {},
    "author": [
      {
        "name": "Mark Y",
        "url": {}
      }
    ],
    "date": "2024-02-09",
    "categories": [
      "regression",
      "linear regression",
      "random forest",
      "knn",
      "xgboost"
    ],
    "contents": "\r\n\r\nContents\r\nPredicting HDB Rental Prices\r\nSet dependenies and load necessary packages\r\nImport the data\r\nCorrelation check\r\nEDA\r\nSplit the data\r\nLet’s make some recipes\r\nMake some models\r\nFirst fit and tuning\r\nVisualize results\r\nFinalize workflow and last fit\r\nCollect predictions\r\nVisualize important features\r\n\r\n\r\nPredicting HDB Rental Prices\r\nI started Module 1 of SMU Academy’s Predictive Analytics and Machine Learning class in January 2024. This data set was given in class for homework. I shall use it as practice to document what was learnt in class.\r\nAt this stage of my learning, the primary objective of this exercise is to familiarize myself with the complete workflow from start to finish. Repeatedly fine-tuning hyper-parameters, or complex feature engineering in order to come up with the BEST predictive model, is not an objective at this stage.\r\nSet dependenies and load necessary packages\r\nAs some of the code takes quite a bit of time to run, I’ve loaded the RData file to speed things up.\r\n\r\n\r\nrm(list = ls())\r\nsessionInfo()\r\n\r\n# Set packages and dependencies\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\",# for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\", #load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", #random forest\r\n               \"janitor\")\r\nload(\"rental_data.RData\")\r\n\r\n\r\nImport the data\r\nRead in the dataset rental_price.csv, and use skim() to provide a quick summary and overview of it.\r\n\r\n\r\n# read in data\r\ndf <- read_csv(\"rental_price.csv\")\r\nskim(df)\r\n\r\n\r\nProperty name and region (which are currently characters) will need to be reclassified as factors. The same goes for ref_year and district. unit_id will be assigned a role of “an id” later in the recipe.\r\n\r\n\r\n# make factors\r\ndata <-\r\n  df %>% \r\n  mutate(across(c(district, region, ref_year, property), as.factor))\r\n\r\n\r\nCorrelation check\r\nLet’s check for correlation among numeric variables.\r\n\r\n\r\n# check correlation between numeric, exclude loc_x, loc_y\r\n\r\ndata %>% \r\n  select (-unit_id, -loc_x, -loc_y) %>% \r\n  select_if(is.numeric) %>% \r\n  as.matrix(.) %>% \r\n  rcorr() %>% \r\n  tidy() %>% \r\n  arrange(desc(abs(estimate)))\r\n\r\n   ┌─────────────────────────────────────────────────────┐\r\n   │ column1      column2      estimate      n   p.value │\r\n   ├─────────────────────────────────────────────────────┤\r\n   │ price_medi   age           -0.479    3128    0      │\r\n   │ an                                                  │\r\n   │ price_medi   dist_to_mr    -0.203    3128    0      │\r\n   │ an           t                                      │\r\n   │ dist_to_mr   age            0.0331   3128    0.0644 │\r\n   │ t                                                   │\r\n   └─────────────────────────────────────────────────────┘\r\nColumn names: column1, column2, estimate, n, p.value\r\n\r\nCorrelation among numeric variables appear to be fine. None seem to be highly correlated with one another.\r\nEDA\r\nBefore we proceed with splitting the data, let’s do some basic EDA.\r\n\r\n\r\n# price by region\r\n\r\ndata %>% \r\n  ggplot(aes(x = district,\r\n              y = price_median)\r\n         ) + \r\n  geom_boxplot(outlier.shape = NA) +\r\n  geom_point(alpha = 0.15, aes(color = region))+\r\n  theme_bw() +\r\n  labs(x = \"District\",\r\n       y = \"Price Median $ psf\")\r\n\r\n\r\n\r\nHere we see the influence of location (as specified by district and region) on price.\r\n\r\n\r\ndata %>% \r\n  ggplot(aes(x = dist_to_mrt,\r\n             y = price_median,\r\n             color = region)\r\n         ) +\r\n  geom_point() +\r\n  geom_smooth(aes(color = region),\r\n              method = \"lm\")+\r\n  theme_bw()+\r\n  labs(x = \"Distance to MRT Station (km)\",\r\n       y = \"Price Median $ psf\") +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\nDistance to MRT appears to have a stronger influence on price in region_CCR, and a more muted influence on price in region_OCR and region_RCR.\r\nSplit the data\r\nLet’s proceed with splitting the data into a training and testing set. At the same time, let’s create folds for tuning/cross validation purposes.\r\n\r\n\r\n# split data\r\nset.seed(2024020101)\r\ndata_split <-\r\n  data %>% \r\n  initial_split(strata = region) # strata by region\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = region)\r\n\r\n\r\nLet’s make some recipes\r\nLet’s make a few recipes for modeling.\r\n\r\n\r\nbase_rec <-\r\n  recipes::recipe(formula = price_median ~ .,\r\n                 data = data_train) %>% \r\n  update_role(unit_id, new_role = \"unit_id\") %>% \r\n  update_role(property, new_role = \"name_id\") %>% \r\n  step_rm(loc_x, loc_y) %>%  # remove geo locator, property name\r\n  step_dummy(all_nominal_predictors())\r\n\r\nlog_rec <-\r\n  base_rec %>%\r\n  step_log(price_median, age, dist_to_mrt) # age, dist_mrt\r\n\r\n\r\nMake some models\r\nLet’s make a few models to predict price_median, which is the rental price per square foot for each property. We shall set up 4 models: a simple linear regression model, random forest, xgboost, and k-nearest neighbor model. I will set up a basic model, as well as a model for tuning of hyper-parameters.\r\n\r\n\r\n# linear regression\r\nlm_spec <-\r\n  linear_reg()\r\n\r\n# random forest\r\nrf_spec <-\r\n  rand_forest(trees = 1000L) %>% \r\n  set_engine(\"ranger\") %>% \r\n  set_mode(\"regression\")\r\n\r\nrf_spec_for_tuning <-\r\n  rf_spec %>% \r\n  set_args(mtry = tune(),\r\n           min_n = tune())\r\n\r\n# knn\r\nknn_spec <-\r\n  nearest_neighbor() %>% \r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"regression\")\r\n\r\nknn_spec_for_tuning <-\r\n  knn_spec %>% \r\n  set_args(neighbors = tune(),\r\n           weight_func = \"optimal\",\r\n           dist_power = tune())\r\n\r\n# xgboost\r\nxgb_spec <-\r\n  boost_tree(trees = 1000L) %>% \r\n  set_engine(\"xgboost\") %>% \r\n  set_mode(\"regression\")\r\n\r\nxgb_spec_for_tuning <-\r\n  xgb_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(),\r\n           loss_reduction = tune(),                     \r\n           sample_size = tune(),\r\n           mtry = tune(),        \r\n           learn_rate = tune())        \r\n\r\n\r\nLet’s combine everything into a workflow set. xgb needs a separate workflow set as it only works with dummy variables. Combine both using bind_rows into 1 model_set.\r\n\r\n\r\nbase_set <-\r\n  workflow_set (\r\n    preproc = list(base_rec, log_rec), #preprocessor\r\n    models = list(rf_spec, knn_spec, xgb_spec,\r\n                  rf_spec_for_tuning, knn_spec_for_tuning, xgb_spec_for_tuning), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\n\r\nFirst fit and tuning\r\nLet’s use fit_resamples to do a first fit as well as tune the hyper-parameters. There are still lots of things I don’t know. For example, what does “grid = 11” mean? Why not 5? Why not 25?\r\n\r\n\r\n# first fit\r\nset.seed(2024020102)\r\ndoParallel::registerDoParallel()\r\nbase_results <-\r\n  workflow_map(base_set,\r\n               fn = \"tune_grid\",\r\n               resamples = data_fold,\r\n               grid = 11,\r\n               verbose = TRUE)\r\n\r\n\r\nOnce your machine is done tuning for hyper-parameters, you can collect the results, by metrics. Here we used “rmse”. My machine is rather incompetent, so this process is taking a while. Fortunately, the dataset is rather small.\r\n\r\n\r\nbase_results %>% \r\n  collect_metrics() %>% \r\n  filter(.metric == \"rmse\") %>% \r\n  arrange(mean) %>% \r\n  print(n = 5)\r\n\r\n# A tibble: 72 × 9\r\n  wflow_id       .config preproc model .metric .estimator   mean     n\r\n  <chr>          <chr>   <chr>   <chr> <chr>   <chr>       <dbl> <int>\r\n1 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0559    10\r\n2 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0565    10\r\n3 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0598    10\r\n4 recipe_2_rand… Prepro… recipe  rand… rmse    standard   0.0670    10\r\n5 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0684    10\r\n# ℹ 67 more rows\r\n# ℹ 1 more variable: std_err <dbl>\r\n\r\nVisualize results\r\nWe can also visualize the results using a basic autoplot(), or rank the results using workflowsets::rank_results().\r\n\r\n\r\nautoplot(base_results) +\r\n  theme_bw() +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\n\r\n# rank results\r\nbase_results %>% \r\n  workflowsets::rank_results(rank_metric = \"rmse\") %>% \r\n  filter(.metric == \"rmse\") %>% \r\n  print(n = 5)\r\n\r\n# A tibble: 72 × 9\r\n  wflow_id     .config .metric   mean std_err     n preprocessor model\r\n  <chr>        <chr>   <chr>    <dbl>   <dbl> <int> <chr>        <chr>\r\n1 recipe_2_bo… Prepro… rmse    0.0559 1.15e-3    10 recipe       boos…\r\n2 recipe_2_bo… Prepro… rmse    0.0565 9.01e-4    10 recipe       boos…\r\n3 recipe_2_bo… Prepro… rmse    0.0598 1.56e-3    10 recipe       boos…\r\n4 recipe_2_ra… Prepro… rmse    0.0670 1.36e-3    10 recipe       rand…\r\n5 recipe_2_bo… Prepro… rmse    0.0684 1.81e-3    10 recipe       boos…\r\n# ℹ 67 more rows\r\n# ℹ 1 more variable: rank <int>\r\n\r\nThe best recipe-model combination is recipe 2, which is log_rec and parameters from boost_tree_3. If you recall, boost_tree_3 is the basic xgboost model with no hyper-parameters for tuning. Let’s extract the hyper-parameters using select_best.\r\n\r\n\r\ntune_param <-\r\n  base_results %>% \r\n  extract_workflow_set_result(\"recipe_2_boost_tree_3\") %>% \r\n  select_best(metric = \"rmse\")\r\n\r\n\r\nFinalize workflow and last fit\r\nLet’s apply the parameters and finalize the workflow for the best recipe/model combination.\r\n\r\n\r\nlog_rec_xbg_boost_wflow <-\r\n  workflow() %>% \r\n  add_recipe(log_rec) %>% \r\n  add_model(xgb_spec) %>% \r\n  finalize_workflow(tune_param)\r\n\r\n\r\nFinally, we can fit the model by performing last_fit to the split data, data_split.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit <-\r\n  log_rec_xbg_boost_wflow %>% \r\n  last_fit(data_split)\r\n\r\n\r\nCollect predictions\r\nWe can collect metrics as well as predictions from the last fitted model.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  collect_metrics()\r\n\r\n    ┌───────────────────────────────────────────────────┐\r\n    │ .metric   .estimator   .estimate   .config        │\r\n    ├───────────────────────────────────────────────────┤\r\n    │ rmse      standard         0.057   Preprocessor1_ │\r\n    │                                    Model1         │\r\n    │ rsq       standard         0.959   Preprocessor1_ │\r\n    │                                    Model1         │\r\n    └───────────────────────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate, .config\r\n\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  collect_predictions() %>% \r\n  ggplot(aes(x = price_median,\r\n             y = .pred)) +\r\n  geom_point(alpha = 0.2)+\r\n  geom_abline(lty = 2,\r\n              color = \"dodgerblue\") +\r\n  labs(x = \"Actual Median Price $psf\",\r\n       y = \"Predicted Median Price $psf\")+\r\n  theme_bw()\r\n\r\n\r\n\r\nVisualize important features\r\nWe can use the vip() to see which were the most important features.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  vip(geom = \"point\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nAs we can see, location location location, or in this case region is the most important predictor of price, followed by age and distance to MRT station.\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-02-09-hdb-rental-prices/hdb-rental-prices_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2024-02-09T10:56:05+08:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  }
]
