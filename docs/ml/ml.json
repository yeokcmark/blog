[
  {
    "path": "ml/2024-03-26-exploring-principal-component-analysis-with-tidymodels-and-prcomp/",
    "title": "Exploring Principal Component Analysis with tidymodels and prcomp",
    "description": "We ran out of time during class due to the immense amount of material that needed to be covered. Hence, these assessment questions became homework assignments. Within, you will find explorations of PCA using the tidymodel way, as well as using prcomp from Base R.",
    "author": [],
    "date": "2024-03-26",
    "categories": [
      "PCA",
      "tidymodels",
      "prcomp"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSet Dependencies and Import Data\r\nQuestion 1:\r\nQuestion 2:\r\nQuestion 3:\r\nQuestion 4:\r\n\r\n\r\n\r\nIntroduction\r\nThere are 4 questions to be answered. I didn’t list them here, but if you follow along, I’ve explored most concepts of PCA using the tidymodel way, as well as using prcomp from Base R.\r\nNote that aesthetics were NOT a priority for me - hence most of the plots could definitely be improved upon.\r\nSet Dependencies and Import Data\r\nLet’s begin by importing the necessary packages and data.\r\n\r\n\r\nrm(list = ls())\r\npacman::p_load(tidyverse, # tidy DS\r\n               tidymodels, # tidy ML\r\n               skimr, GGally, Hmisc, broom, modelr, ggstatsplot, # EDA 1\r\n               scales, ggthemes, gridExtra, # ggplot2::\r\n               DT, # for interactive data display\r\n               janitor, themis # recipes::\r\n)\r\n\r\n\r\nQuestion 1:\r\nLoad the data. We see that “Type” is probably an encoding for different types of wine with 3 levels. Change to a factor.\r\n\r\n\r\nwine <- read_csv(\"wine.csv\")\r\nskim(wine)\r\n\r\nTable 1: Data summary\r\nName\r\nwine\r\nNumber of rows\r\n178\r\nNumber of columns\r\n14\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n14\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nType\r\n0\r\n1\r\n1.94\r\n0.78\r\n1.00\r\n1.00\r\n2.00\r\n3.00\r\n3.00\r\n▆▁▇▁▆\r\nAlcohol\r\n0\r\n1\r\n13.00\r\n0.81\r\n11.03\r\n12.36\r\n13.05\r\n13.68\r\n14.83\r\n▂▇▇▇▃\r\nMalic\r\n0\r\n1\r\n2.34\r\n1.12\r\n0.74\r\n1.60\r\n1.87\r\n3.08\r\n5.80\r\n▇▅▂▂▁\r\nAsh\r\n0\r\n1\r\n2.37\r\n0.27\r\n1.36\r\n2.21\r\n2.36\r\n2.56\r\n3.23\r\n▁▂▇▅▁\r\nAlcalinity\r\n0\r\n1\r\n19.49\r\n3.34\r\n10.60\r\n17.20\r\n19.50\r\n21.50\r\n30.00\r\n▁▆▇▃▁\r\nMagnesium\r\n0\r\n1\r\n99.74\r\n14.28\r\n70.00\r\n88.00\r\n98.00\r\n107.00\r\n162.00\r\n▅▇▃▁▁\r\nPhenols\r\n0\r\n1\r\n2.30\r\n0.63\r\n0.98\r\n1.74\r\n2.36\r\n2.80\r\n3.88\r\n▅▇▇▇▁\r\nFlavanoids\r\n0\r\n1\r\n2.03\r\n1.00\r\n0.34\r\n1.20\r\n2.13\r\n2.88\r\n5.08\r\n▆▆▇▂▁\r\nNonflavanoids\r\n0\r\n1\r\n0.36\r\n0.12\r\n0.13\r\n0.27\r\n0.34\r\n0.44\r\n0.66\r\n▃▇▅▃▂\r\nProanthocyanins\r\n0\r\n1\r\n1.59\r\n0.57\r\n0.41\r\n1.25\r\n1.56\r\n1.95\r\n3.58\r\n▃▇▆▂▁\r\nColor\r\n0\r\n1\r\n5.06\r\n2.32\r\n1.28\r\n3.22\r\n4.69\r\n6.20\r\n13.00\r\n▆▇▃▂▁\r\nHue\r\n0\r\n1\r\n0.96\r\n0.23\r\n0.48\r\n0.78\r\n0.96\r\n1.12\r\n1.71\r\n▅▇▇▃▁\r\nDilution\r\n0\r\n1\r\n2.61\r\n0.71\r\n1.27\r\n1.94\r\n2.78\r\n3.17\r\n4.00\r\n▆▃▆▇▃\r\nProline\r\n0\r\n1\r\n746.89\r\n314.91\r\n278.00\r\n500.50\r\n673.50\r\n985.00\r\n1680.00\r\n▇▇▅▃▁\r\n\r\nwine <-\r\n  wine %>% \r\n  mutate(Type = as.factor(Type))\r\n\r\n\r\nFor the EDA portion, let’s do a histogram to see the distribution of wine by alcalinity. I will also do a scatter plot to explore the relationship between alcalinity and color.\r\n\r\n\r\nwine %>% \r\n  ggplot(aes(x = Alcalinity)) +\r\n  geom_histogram()\r\n\r\n\r\n# probably no relationship\r\nwine %>% \r\n  ggplot(aes(x = Color,\r\n             y = Alcalinity)\r\n         ) +\r\n  geom_smooth(method = \"lm\",\r\n              formula = y~x,\r\n              color = \"grey70\",\r\n              se = F\r\n              ) +\r\n  geom_point(aes(color = Alcohol),\r\n             alpha = 0.7) +\r\n  theme_bw()\r\n\r\n\r\n\r\nNormalize the dataset and execute PCA using the tidymodels framework.\r\n\r\n\r\n# tidy models way\r\nrec_wine <-\r\n  recipe(formula = ~.,\r\n         data = wine) %>% \r\n  # change roles to \"id\", instead could use step_rm to completely remove\r\n  update_role(Type,\r\n              new_role = \"id\") %>% \r\n  step_zv(all_predictors()) %>% \r\n  step_normalize(all_predictors()) %>% \r\n  step_pca(all_predictors(), id=\"pca_wine\", threshold = 0.9)\r\n\r\nprep_wine <- # note that PCA is step 3\r\n  rec_wine %>% \r\n  prep(verbose = T)\r\n\r\noper 1 step zv [training] \r\noper 2 step normalize [training] \r\noper 3 step pca [training] \r\nThe retained training set is ~ 0.01 Mb  in memory.\r\n\r\nprep_wine %>% \r\n  tidy(id = \"pca_wine\", type = \"variance\") %>% \r\n  print(n = nrow(.))\r\n\r\n# A tibble: 52 × 4\r\n   terms                         value component id      \r\n   <chr>                         <dbl>     <int> <chr>   \r\n 1 variance                      4.71          1 pca_wine\r\n 2 variance                      2.50          2 pca_wine\r\n 3 variance                      1.45          3 pca_wine\r\n 4 variance                      0.919         4 pca_wine\r\n 5 variance                      0.853         5 pca_wine\r\n 6 variance                      0.642         6 pca_wine\r\n 7 variance                      0.551         7 pca_wine\r\n 8 variance                      0.348         8 pca_wine\r\n 9 variance                      0.289         9 pca_wine\r\n10 variance                      0.251        10 pca_wine\r\n11 variance                      0.226        11 pca_wine\r\n12 variance                      0.169        12 pca_wine\r\n13 variance                      0.103        13 pca_wine\r\n14 cumulative variance           4.71          1 pca_wine\r\n15 cumulative variance           7.20          2 pca_wine\r\n16 cumulative variance           8.65          3 pca_wine\r\n17 cumulative variance           9.57          4 pca_wine\r\n18 cumulative variance          10.4           5 pca_wine\r\n19 cumulative variance          11.1           6 pca_wine\r\n20 cumulative variance          11.6           7 pca_wine\r\n21 cumulative variance          12.0           8 pca_wine\r\n22 cumulative variance          12.3           9 pca_wine\r\n23 cumulative variance          12.5          10 pca_wine\r\n24 cumulative variance          12.7          11 pca_wine\r\n25 cumulative variance          12.9          12 pca_wine\r\n26 cumulative variance          13            13 pca_wine\r\n27 percent variance             36.2           1 pca_wine\r\n28 percent variance             19.2           2 pca_wine\r\n29 percent variance             11.1           3 pca_wine\r\n30 percent variance              7.07          4 pca_wine\r\n31 percent variance              6.56          5 pca_wine\r\n32 percent variance              4.94          6 pca_wine\r\n33 percent variance              4.24          7 pca_wine\r\n34 percent variance              2.68          8 pca_wine\r\n35 percent variance              2.22          9 pca_wine\r\n36 percent variance              1.93         10 pca_wine\r\n37 percent variance              1.74         11 pca_wine\r\n38 percent variance              1.30         12 pca_wine\r\n39 percent variance              0.795        13 pca_wine\r\n40 cumulative percent variance  36.2           1 pca_wine\r\n41 cumulative percent variance  55.4           2 pca_wine\r\n42 cumulative percent variance  66.5           3 pca_wine\r\n43 cumulative percent variance  73.6           4 pca_wine\r\n44 cumulative percent variance  80.2           5 pca_wine\r\n45 cumulative percent variance  85.1           6 pca_wine\r\n46 cumulative percent variance  89.3           7 pca_wine\r\n47 cumulative percent variance  92.0           8 pca_wine\r\n48 cumulative percent variance  94.2           9 pca_wine\r\n49 cumulative percent variance  96.2          10 pca_wine\r\n50 cumulative percent variance  97.9          11 pca_wine\r\n51 cumulative percent variance  99.2          12 pca_wine\r\n52 cumulative percent variance 100            13 pca_wine\r\n\r\nFor the wine dataset, PC1 captures 36.2% of the variance and PC2 captures 19.2 percent of the variance in the dataset. Cumulative, that’s 55.4 percent of the dataset. It looks like we will need more PCs. To achieve 90% data “coverage”, we will need 8 principal components.\r\n\r\n\r\ntidy_wine_pca_loadings <-\r\n  prep_wine %>% \r\n  tidy(id = \"pca_wine\")\r\n\r\n# plot the loadings for first 8 PCs\r\ntidy_wine_pca_loadings %>% \r\n  filter(component %in% c(paste0(\"PC\", 1:8))) %>% \r\n  ggplot (aes(x = value,\r\n              y = terms,\r\n              fill = terms\r\n              )\r\n          ) +\r\n  geom_col(show.legend = F) +\r\n  facet_wrap(.~ component,\r\n             ncol = 2) +\r\n  theme_bw()\r\n\r\n\r\n\r\nQuestion 2:\r\nLoad the bc dataset and check for any missing values.\r\nExclude the categorical variable, normalize the remaining data, and run PCA using tidymodels::.\r\nInterpret the loadings of the first two principal components. What do these loadings tell us about the data?\r\nVisualize the PCA results, with points colored by Class. Can PCA help differentiate between benign and malignant samples based on the first two principal components?\r\nLoad the data.\r\n\r\n\r\nbc <- read_csv(\"bc.csv\")\r\nskim(bc)\r\n\r\nTable 2: Data summary\r\nName\r\nbc\r\nNumber of rows\r\n699\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nnumeric\r\n10\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nClass\r\n0\r\n1\r\n6\r\n9\r\n0\r\n2\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nId\r\n0\r\n1.00\r\n1071704.10\r\n617095.73\r\n61634\r\n870688.5\r\n1171710\r\n1238298\r\n13454352\r\n▇▁▁▁▁\r\nCl.thickness\r\n0\r\n1.00\r\n4.42\r\n2.82\r\n1\r\n2.0\r\n4\r\n6\r\n10\r\n▇▇▇▃▃\r\nCell.size\r\n0\r\n1.00\r\n3.13\r\n3.05\r\n1\r\n1.0\r\n1\r\n5\r\n10\r\n▇▂▁▁▂\r\nCell.shape\r\n0\r\n1.00\r\n3.21\r\n2.97\r\n1\r\n1.0\r\n1\r\n5\r\n10\r\n▇▂▁▁▁\r\nMarg.adhesion\r\n0\r\n1.00\r\n2.81\r\n2.86\r\n1\r\n1.0\r\n1\r\n4\r\n10\r\n▇▂▁▁▁\r\nEpith.c.size\r\n0\r\n1.00\r\n3.22\r\n2.21\r\n1\r\n2.0\r\n2\r\n4\r\n10\r\n▇▂▂▁▁\r\nBare.nuclei\r\n16\r\n0.98\r\n3.54\r\n3.64\r\n1\r\n1.0\r\n1\r\n6\r\n10\r\n▇▁▁▁▂\r\nBl.cromatin\r\n0\r\n1.00\r\n3.44\r\n2.44\r\n1\r\n2.0\r\n3\r\n5\r\n10\r\n▇▅▁▂▁\r\nNormal.nucleoli\r\n0\r\n1.00\r\n2.87\r\n3.05\r\n1\r\n1.0\r\n1\r\n4\r\n10\r\n▇▁▁▁▁\r\nMitoses\r\n0\r\n1.00\r\n1.59\r\n1.72\r\n1\r\n1.0\r\n1\r\n1\r\n10\r\n▇▁▁▁▁\r\n\r\n16 missing values in feature Bare.nucleoi. I will use step_impute_knn to impute the missing values.\r\n\r\n\r\nrec_bc <-\r\n  recipe(formula = ~.,\r\n         data = bc) %>% \r\n  # change roles to \"id\", instead could use step_rm to completely remove\r\n  update_role(Class, Id,\r\n              new_role = \"id\") %>% \r\n  step_zv(all_predictors()) %>% \r\n  step_impute_knn(Bare.nuclei) %>% \r\n  step_normalize(all_predictors()) %>% \r\n  step_pca(all_predictors(), id=\"pca_bc\", threshold = 0.9)\r\n\r\nprep_bc <-\r\n  rec_bc %>% \r\n  prep(verbose = T)\r\n\r\noper 1 step zv [training] \r\noper 2 step impute knn [training] \r\noper 3 step normalize [training] \r\noper 4 step pca [training] \r\nThe retained training set is ~ 0.04 Mb  in memory.\r\n\r\ntidy_bc_pca_loadings <-\r\n  prep_bc %>% \r\n  tidy(id = \"pca_bc\")\r\n\r\n# lets make a plot of PC1 and PC2\r\ntidy_bc_pca_loadings %>% \r\n  filter(component %in% c(paste0(\"PC\", 1:2))) %>% \r\n  ggplot (aes(x = value,\r\n              y = terms,\r\n              fill = terms\r\n              )\r\n          ) +\r\n  geom_col(show.legend = F) +\r\n  facet_wrap(.~ component,\r\n             ncol = 2) +\r\n  theme_bw()\r\n\r\n\r\n\r\nIn PC1, all terms have negative loading values. In PC2, “Mitoses” is dominant in its effects. That’s interesting, lets examine a scatter plot. This is “extra credit”.\r\n\r\n\r\nprep_bc %>% \r\n  juice() %>% \r\n  ggplot(aes(x = PC1,\r\n         y = PC2,\r\n         color = Class)) + # can change to cut, or color as well\r\n  geom_point(alpha = 0.3)+\r\n  labs(caption = \"Benign tumors tend to have positive PC1 loadings, and are centered close to 0 on PC2 loadings\")+\r\n  theme(legend.position = \"bottom\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nQuestion 3:\r\nLoad the diamonds dataset and check the summary statistics of each variable.\r\nSelect only the numeric variables, normalize the data, and then perform PCA using tidymodels::.\r\nHow do you interpret the number of principal components? Do you think two principal components are sufficient to capture the major patterns in this dataset?\r\nGenerate a scatter plot of the first two principal components. Can you identify any clusters or outliers in the data?\r\nImport the data and conduct brief EDA.\r\n\r\n\r\ndiamonds <- read_csv(\"diamonds.csv\")\r\n\r\n# change character to factor\r\ndiamonds <-\r\n  diamonds %>% \r\n  mutate_if(is.character, as.factor)\r\n\r\n# both summary and skim provide summary statistics\r\nsummary(diamonds)\r\n\r\n     carat               cut        color        clarity     \r\n Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065  \r\n 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258  \r\n Median :0.7000   Ideal    :21551   F: 9542   SI2    : 9194  \r\n Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171  \r\n 3rd Qu.:1.0400   Very Good:12082   H: 8304   VVS2   : 5066  \r\n Max.   :5.0100                     I: 5422   VVS1   : 3655  \r\n                                    J: 2808   (Other): 2531  \r\n     depth           table           price             x         \r\n Min.   :43.00   Min.   :43.00   Min.   :  326   Min.   : 0.000  \r\n 1st Qu.:61.00   1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710  \r\n Median :61.80   Median :57.00   Median : 2401   Median : 5.700  \r\n Mean   :61.75   Mean   :57.46   Mean   : 3933   Mean   : 5.731  \r\n 3rd Qu.:62.50   3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540  \r\n Max.   :79.00   Max.   :95.00   Max.   :18823   Max.   :10.740  \r\n                                                                 \r\n       y                z         \r\n Min.   : 0.000   Min.   : 0.000  \r\n 1st Qu.: 4.720   1st Qu.: 2.910  \r\n Median : 5.710   Median : 3.530  \r\n Mean   : 5.735   Mean   : 3.539  \r\n 3rd Qu.: 6.540   3rd Qu.: 4.040  \r\n Max.   :58.900   Max.   :31.800  \r\n                                  \r\n\r\nskim(diamonds)\r\n\r\nTable 3: Data summary\r\nName\r\ndiamonds\r\nNumber of rows\r\n53940\r\nNumber of columns\r\n10\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n3\r\nnumeric\r\n7\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\ncut\r\n0\r\n1\r\nFALSE\r\n5\r\nIde: 21551, Pre: 13791, Ver: 12082, Goo: 4906\r\ncolor\r\n0\r\n1\r\nFALSE\r\n7\r\nG: 11292, E: 9797, F: 9542, H: 8304\r\nclarity\r\n0\r\n1\r\nFALSE\r\n8\r\nSI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\ncarat\r\n0\r\n1\r\n0.80\r\n0.47\r\n0.2\r\n0.40\r\n0.70\r\n1.04\r\n5.01\r\n▇▂▁▁▁\r\ndepth\r\n0\r\n1\r\n61.75\r\n1.43\r\n43.0\r\n61.00\r\n61.80\r\n62.50\r\n79.00\r\n▁▁▇▁▁\r\ntable\r\n0\r\n1\r\n57.46\r\n2.23\r\n43.0\r\n56.00\r\n57.00\r\n59.00\r\n95.00\r\n▁▇▁▁▁\r\nprice\r\n0\r\n1\r\n3932.80\r\n3989.44\r\n326.0\r\n950.00\r\n2401.00\r\n5324.25\r\n18823.00\r\n▇▂▁▁▁\r\nx\r\n0\r\n1\r\n5.73\r\n1.12\r\n0.0\r\n4.71\r\n5.70\r\n6.54\r\n10.74\r\n▁▁▇▃▁\r\ny\r\n0\r\n1\r\n5.73\r\n1.14\r\n0.0\r\n4.72\r\n5.71\r\n6.54\r\n58.90\r\n▇▁▁▁▁\r\nz\r\n0\r\n1\r\n3.54\r\n0.71\r\n0.0\r\n2.91\r\n3.53\r\n4.04\r\n31.80\r\n▇▁▁▁▁\r\n\r\nPerform PCA the tidymodel way. As we can see from the summary table, 2 PCs cover about 86.4% of the data, while 3 PCs cover about 96.3%. Hence, if we use 90% as the threshold, 3 PCs would be sufficient.\r\n\r\n\r\n# tidy models way\r\nrec_diamonds <-\r\n  recipe(formula = ~.,\r\n         data = diamonds) %>% \r\n  # change roles to \"id\", instead could use step_rm to completely remove\r\n  update_role(cut, color, clarity,\r\n              new_role = \"id\") %>% \r\n  step_zv(all_predictors()) %>% \r\n  step_normalize(all_predictors()) %>% \r\n  step_pca(all_predictors(), id=\"pca\", threshold = 0.9)\r\n\r\nprep_diamond <- # note that PCA is step 3\r\n  rec_diamonds %>% \r\n  prep(verbose = T)\r\n\r\noper 1 step zv [training] \r\noper 2 step normalize [training] \r\noper 3 step pca [training] \r\nThe retained training set is ~ 1.86 Mb  in memory.\r\n\r\n# to obtain loadings. Here you're telling tidy to tidy the step of recipe with id=pca\r\ntidy_diamond_pca_loadings <-\r\n  prep_diamond %>% \r\n  tidy(id = \"pca\") %>% \r\n  pivot_wider(id_cols = c(terms, id),\r\n              names_from = component,\r\n              values_from = value)\r\n\r\n# this also obtains loadings. Here you are tidy-ing the 3rd step of the recipe\r\nprep_diamond %>% tidy(3)\r\n\r\n# A tibble: 49 × 4\r\n   terms     value component id   \r\n   <chr>     <dbl> <chr>     <chr>\r\n 1 carat  0.452    PC1       pca  \r\n 2 depth -0.000916 PC1       pca  \r\n 3 table  0.0995   PC1       pca  \r\n 4 price  0.426    PC1       pca  \r\n 5 x      0.453    PC1       pca  \r\n 6 y      0.447    PC1       pca  \r\n 7 z      0.446    PC1       pca  \r\n 8 carat -0.0347   PC2       pca  \r\n 9 depth -0.731    PC2       pca  \r\n10 table  0.675    PC2       pca  \r\n# ℹ 39 more rows\r\n\r\n# this is how you \"extract\" standard deviation, variance, and cumulative from a prep recipe using the tidymodels way\r\n# info is stored in a list, so you need to go inside and \"dig\" for it\r\nprep_diamond$steps[[3]]$res %>% summary()\r\n\r\nImportance of components:\r\n                          PC1    PC2     PC3     PC4     PC5     PC6\r\nStandard deviation     2.1826 1.1340 0.83115 0.41684 0.20077 0.18151\r\nProportion of Variance 0.6806 0.1837 0.09869 0.02482 0.00576 0.00471\r\nCumulative Proportion  0.6806 0.8642 0.96294 0.98776 0.99352 0.99823\r\n                           PC7\r\nStandard deviation     0.11135\r\nProportion of Variance 0.00177\r\nCumulative Proportion  1.00000\r\n\r\n#this will get you similar information, as a tibble, so you can make a screeplot\r\ndiamond_variance <-\r\n  prep_diamond %>% \r\n  tidy(id = \"pca\", type = \"variance\") %>% \r\n  print(n = nrow(.))\r\n\r\n# A tibble: 28 × 4\r\n   terms                          value component id   \r\n   <chr>                          <dbl>     <int> <chr>\r\n 1 variance                      4.76           1 pca  \r\n 2 variance                      1.29           2 pca  \r\n 3 variance                      0.691          3 pca  \r\n 4 variance                      0.174          4 pca  \r\n 5 variance                      0.0403         5 pca  \r\n 6 variance                      0.0329         6 pca  \r\n 7 variance                      0.0124         7 pca  \r\n 8 cumulative variance           4.76           1 pca  \r\n 9 cumulative variance           6.05           2 pca  \r\n10 cumulative variance           6.74           3 pca  \r\n11 cumulative variance           6.91           4 pca  \r\n12 cumulative variance           6.95           5 pca  \r\n13 cumulative variance           6.99           6 pca  \r\n14 cumulative variance           7.00           7 pca  \r\n15 percent variance             68.1            1 pca  \r\n16 percent variance             18.4            2 pca  \r\n17 percent variance              9.87           3 pca  \r\n18 percent variance              2.48           4 pca  \r\n19 percent variance              0.576          5 pca  \r\n20 percent variance              0.471          6 pca  \r\n21 percent variance              0.177          7 pca  \r\n22 cumulative percent variance  68.1            1 pca  \r\n23 cumulative percent variance  86.4            2 pca  \r\n24 cumulative percent variance  96.3            3 pca  \r\n25 cumulative percent variance  98.8            4 pca  \r\n26 cumulative percent variance  99.4            5 pca  \r\n27 cumulative percent variance  99.8            6 pca  \r\n28 cumulative percent variance 100              7 pca  \r\n\r\n## basic scree plot\r\ndiamond_variance %>% \r\n  filter(terms == \"percent variance\") %>% \r\n  ggplot(aes(x = component,\r\n             y = value)\r\n         ) +\r\n  geom_col() +\r\n  theme_bw()\r\n\r\n\r\n\r\nScatter plot of PC1 and PC2. We can see that outliers exist.\r\n\r\n\r\n# when you juice a prep recipe, you get the individual data and their values on the principal components. You can use this to  make a scatter plot\r\njuice_diamonds <-\r\n  prep_diamond %>% \r\n  juice()\r\n\r\n# this plot shows individual diamonds plotted on the new scale PC1, PC2\r\njuice_diamonds %>% \r\n  ggplot(aes(x = PC1,\r\n         y = PC2,\r\n         color = clarity)) + # can change to cut, or color as well\r\n  geom_point(alpha = 0.3)+\r\n  # here we use geom text to label potential outliers by color\r\n  geom_text(data = juice_diamonds %>% \r\n              filter(PC1 > 10 | PC1 < -5 | PC2 > 7 | PC2 < -7),\r\n            aes(label = color),\r\n            check_overlap = T, \r\n            color = \"black\",\r\n            size = 2) +\r\n  theme_bw()+\r\n  theme(legend.position = \"bottom\") \r\n\r\n\r\n\r\nQuestion 4:\r\nLoad the housing dataset and investigate its correlations.\r\n\r\n\r\nhousing <- read_csv(\"housing.csv\")\r\nskim(housing)\r\n\r\nTable 4: Data summary\r\nName\r\nhousing\r\nNumber of rows\r\n506\r\nNumber of columns\r\n14\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n14\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\ncrim\r\n0\r\n1\r\n3.61\r\n8.60\r\n0.01\r\n0.08\r\n0.26\r\n3.68\r\n88.98\r\n▇▁▁▁▁\r\nzn\r\n0\r\n1\r\n11.36\r\n23.32\r\n0.00\r\n0.00\r\n0.00\r\n12.50\r\n100.00\r\n▇▁▁▁▁\r\nindus\r\n0\r\n1\r\n11.14\r\n6.86\r\n0.46\r\n5.19\r\n9.69\r\n18.10\r\n27.74\r\n▇▆▁▇▁\r\nchas\r\n0\r\n1\r\n0.07\r\n0.25\r\n0.00\r\n0.00\r\n0.00\r\n0.00\r\n1.00\r\n▇▁▁▁▁\r\nnox\r\n0\r\n1\r\n0.55\r\n0.12\r\n0.38\r\n0.45\r\n0.54\r\n0.62\r\n0.87\r\n▇▇▆▅▁\r\nrm\r\n0\r\n1\r\n6.28\r\n0.70\r\n3.56\r\n5.89\r\n6.21\r\n6.62\r\n8.78\r\n▁▂▇▂▁\r\nage\r\n0\r\n1\r\n68.57\r\n28.15\r\n2.90\r\n45.02\r\n77.50\r\n94.07\r\n100.00\r\n▂▂▂▃▇\r\ndis\r\n0\r\n1\r\n3.80\r\n2.11\r\n1.13\r\n2.10\r\n3.21\r\n5.19\r\n12.13\r\n▇▅▂▁▁\r\nrad\r\n0\r\n1\r\n9.55\r\n8.71\r\n1.00\r\n4.00\r\n5.00\r\n24.00\r\n24.00\r\n▇▂▁▁▃\r\ntax\r\n0\r\n1\r\n408.24\r\n168.54\r\n187.00\r\n279.00\r\n330.00\r\n666.00\r\n711.00\r\n▇▇▃▁▇\r\nptratio\r\n0\r\n1\r\n18.46\r\n2.16\r\n12.60\r\n17.40\r\n19.05\r\n20.20\r\n22.00\r\n▁▃▅▅▇\r\nblack\r\n0\r\n1\r\n356.67\r\n91.29\r\n0.32\r\n375.38\r\n391.44\r\n396.22\r\n396.90\r\n▁▁▁▁▇\r\nlstat\r\n0\r\n1\r\n12.65\r\n7.14\r\n1.73\r\n6.95\r\n11.36\r\n16.96\r\n37.97\r\n▇▇▅▂▁\r\nmedv\r\n0\r\n1\r\n22.53\r\n9.20\r\n5.00\r\n17.02\r\n21.20\r\n25.00\r\n50.00\r\n▂▇▅▁▁\r\n\r\n## chas appears to be a binary feature. Will exclude in correlation analysis\r\n\r\nhousing %>% \r\n  dplyr::select(-chas) %>% \r\n  as.matrix(.) %>% \r\n  Hmisc::rcorr(.) %>% \r\n  tidy() %>% \r\n  mutate(absCorr = abs(estimate)) %>% \r\n  dplyr::select(column1, column2, absCorr) %>% \r\n  datatable() %>% \r\n  formatRound(columns = \"absCorr\",\r\n              digits = 3)\r\n\r\n\r\n\r\nSince I’ve already done 3 examples using tidymodels method, for Question 4, I will practice using prcomp for the PCA explorations.\r\n\r\n\r\n##check for NA\r\nhousing %>% \r\n  map_dbl(\r\n    function (x) sum(is.na(x))\r\n  )\r\n\r\n   crim      zn   indus    chas     nox      rm     age     dis \r\n      0       0       0       0       0       0       0       0 \r\n    rad     tax ptratio   black   lstat    medv \r\n      0       0       0       0       0       0 \r\n\r\nhousing_PCA <-\r\n  housing %>% \r\n  prcomp(scale = T)\r\n\r\n# we can use tidy(matrix = \"xxxx\") to get some useful info\r\n\r\n# self explanatory, we obtain loadings\r\nhousing_PCA %>% \r\n  tidy(matrix = \"loadings\")\r\n\r\n# A tibble: 196 × 3\r\n   column    PC    value\r\n   <chr>  <dbl>    <dbl>\r\n 1 crim       1  0.242  \r\n 2 crim       2 -0.0659 \r\n 3 crim       3  0.395  \r\n 4 crim       4  0.100  \r\n 5 crim       5 -0.00496\r\n 6 crim       6  0.225  \r\n 7 crim       7 -0.777  \r\n 8 crim       8  0.157  \r\n 9 crim       9 -0.254  \r\n10 crim      10 -0.0714 \r\n# ℹ 186 more rows\r\n\r\n# self explanatory again, except you need to square the std.dev to get eigenvalues\r\n\r\nscale <- 10\r\nhousing_PCA %>% \r\n  tidy(matrix = \"eigenvalues\") %>% \r\n  mutate(eigenvalues = std.dev^2) %>% \r\n#we can continue to do a scree plot. Lets put in some effort and do a SUPER nice one\r\n  slice_max(order_by = percent, n = 10) %>% \r\n  ggplot(aes(x = PC)\r\n         ) +\r\n  geom_point(aes(y = eigenvalues,\r\n                 size = eigenvalues),\r\n             color = \"dodgerblue\",\r\n             show.legend = F\r\n             ) +\r\n  geom_line(aes(y = eigenvalues),\r\n            color = \"tomato3\",\r\n            linewidth = 1)+ \r\n  geom_col(aes(y = percent*scale)\r\n           ) +\r\n  scale_y_continuous(sec.axis = sec_axis(~./scale, name=\"percent\")) +\r\n  theme_bw()+\r\n  labs(title = \"A nicer scree plot - Housing PCA\")\r\n\r\n\r\n  theme(legend.position = \"none\") \r\n\r\nList of 1\r\n $ legend.position: chr \"none\"\r\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\r\n - attr(*, \"complete\")= logi FALSE\r\n - attr(*, \"validate\")= logi TRUE\r\n\r\n# Same as doing this\r\nhousing_PCA %>% \r\n  tidy(matrix = \"scores\")\r\n\r\n# A tibble: 7,084 × 3\r\n     row    PC   value\r\n   <int> <dbl>   <dbl>\r\n 1     1     1 -2.09  \r\n 2     1     2  0.492 \r\n 3     1     3 -0.336 \r\n 4     1     4 -0.0281\r\n 5     1     5 -1.01  \r\n 6     1     6  0.262 \r\n 7     1     7 -0.328 \r\n 8     1     8  0.160 \r\n 9     1     9  0.471 \r\n10     1    10 -0.206 \r\n# ℹ 7,074 more rows\r\n\r\nIn order to achieve a threshold of 90%, I will need 8 principal components.\r\nPC1 explains 46.8 percent of the variance, while PC2 explains 11.8 percent. Cumulatively, PC1 and PC2 explain about 58.5% of the variance.\r\nLet’s visualize the PCA results.\r\n\r\n\r\nhousing_PCA %>% \r\n  tidy(matrix = \"loadings\") %>%\r\n  filter(PC %in% c(1:8)) %>% \r\n  ggplot (aes(x = value,\r\n              y = column,\r\n              fill = column\r\n              )\r\n          ) +\r\n  geom_col(show.legend = F) +\r\n  facet_wrap(.~ PC,\r\n             ncol = 2) +\r\n  theme_bw()\r\n\r\n\r\n\r\nNot sure how much I can see from the above plot :( Let’s try something else\r\n\r\n\r\nhousing_PCA %>% \r\n  tidy(matrix = \"scores\") %>% \r\n  filter(PC <= 2) %>% \r\n  pivot_wider(id_cols = \"row\",\r\n              names_from = PC,\r\n              names_prefix = \"PC\",\r\n              values_from = value) %>% \r\n  cbind(housing) %>% \r\n  ggplot(aes(x = PC1,\r\n             y = PC2,\r\n             color = age)) +\r\n  geom_point() +\r\n  guides(color = guide_colourbar(barwidth = 1,\r\n                                barheight = 10))+\r\n  labs(caption = \"Older properties tend to have positive loadings in PC1,\\n\r\n       while newer properties tend to have negative PC1 & PC2 loadings\")+\r\n  theme_bw()\r\n\r\n\r\n\r\nThis is the end of the homework assignment.\r\n\r\n\r\nsave.image(\"group_assignment3.RData\")\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-03-26-exploring-principal-component-analysis-with-tidymodels-and-prcomp/exploring-principal-component-analysis-with-tidymodels-and-prcomp_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2024-03-26T11:55:27+08:00",
    "input_file": "exploring-principal-component-analysis-with-tidymodels-and-prcomp.knit.md",
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "ml/2024-03-07-using-ml-to-predict-customer-attrition/",
    "title": "Using ML to predict customer attrition",
    "description": "This short exercise was assigned as homework. Let's practice my ML skills and try and predict customer attrition from this dataset.",
    "author": [],
    "date": "2024-03-07",
    "categories": [
      "classification",
      "logistic regression",
      "random forest",
      "lightgbm",
      "knn",
      "xgboost"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSet Dependencies and Import Data\r\nSplit the data\r\nCreate recipes\r\nBuild Models\r\nEvaluate multiple recipes and models using workflowset\r\nTune hyper-parameters using tune_grid\r\nSpecifying better hyper-parameter ranges\r\nBetter “space-filling” grid using grid_latin_hypercube\r\nfunetune using simulated annealing with tune_sim_anneal\r\nFit and predict\r\n\r\n\r\n\r\nIntroduction\r\nThis dataset was assigned to us as homework by Prof Roh. Let’s use it to refine my ML skills. I will make use of workflowsets to tune 4 models: random forest, xgboost, knn, and lightgbm. Thereafter, I will select the best model for further fine-tuning using tune_sim_anneal from the finetune package. This should be exciting!\r\nSet Dependencies and Import Data\r\nLet’s begin by importing the necessary packages and data.\r\n\r\n\r\nrm(list=ls())\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\", \"finetune\", \"themis\", \"embed\", \"butcher\",# for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\", \"bonsai\",#load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", \"kknn\", \"earth\", \"klaR\", \"discrim\", \"naivebayes\", \"baguette\", \"kernlab\", \"lightgbm\",#random forest\r\n               \"janitor\", \"lubridate\")\r\nload(\"customer_attrition.RData\")\r\n\r\n\r\nImport the data and get them into the correct class. Note there there are missing values in TotalCharges, which we will impute as a recipe step. There is also some class imbalance, so we will use strata = Attrition when splitting the data, as well as address it as a recipe step using step_upsampling.\r\n\r\n\r\n# import  data\r\nset.seed(2024030701)\r\ndata <- read_csv(\"https://www.talktoroh.com/s/attrition_classification.csv\")\r\n\r\n# check missing data\r\ntable(is.na(data))\r\nskim(data)\r\n# check imbalance\r\ndata %>% count(Attrition) %>% mutate(prop = n/sum(n)) # there is some class imbalance\r\n\r\n# get variables into correct class\r\ndata <-\r\n  data %>% \r\n  mutate_if(is.character, as.factor) %>% \r\n  mutate(SeniorCitizen = as.factor(SeniorCitizen),\r\n         Attrition = fct_relevel(Attrition, \"Yes\"),\r\n         )\r\n\r\n\r\nSplit the data\r\nSplit the data into train and test sets, and create folds for resampling.\r\n\r\n\r\n# split the data\r\nset.seed(2024030702)\r\n\r\ndata_split <-\r\n  data %>% \r\n  initial_split(strata = Attrition)\r\n\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\n\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\n\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = Attrition)\r\n\r\n\r\nCreate recipes\r\nI created 2 recipes to be evaluated, a base recipe without upsampling, and another with step_upsample.\r\n\r\n\r\n# create recipes\r\n\r\nrec_base <-\r\n  recipes::recipe(formula = Attrition ~.,\r\n                  data = data_train) %>% \r\n  update_role(customerID, new_role = \"id\") %>% \r\n  step_zv(all_predictors()) %>% # remove zero variance\r\n  step_impute_knn(TotalCharges) %>% \r\n  step_YeoJohnson(all_numeric_predictors()) %>% \r\n  step_normalize(all_numeric_predictors()) %>% \r\n  step_dummy(all_nominal_predictors())\r\n\r\nrec_upsample <-\r\n    recipes::recipe(formula = Attrition ~.,\r\n                  data = data_train) %>% \r\n  update_role(customerID, new_role = \"id\") %>% \r\n  step_zv(all_predictors()) %>% # remove zero variance\r\n  step_impute_knn(TotalCharges) %>% \r\n  step_upsample(Attrition) %>% \r\n  step_YeoJohnson(all_numeric_predictors()) %>% \r\n  step_normalize(all_numeric_predictors()) %>% \r\n  step_dummy(all_nominal_predictors())\r\n\r\n\r\nBuild Models\r\nLet’s build some models. Remember to set the model to classification\r\n\r\n\r\n#build models\r\n\r\n# random forest\r\nrf_spec <-\r\n  rand_forest() %>% \r\n  set_engine(\"ranger\",\r\n             importance = \"impurity\") %>% \r\n  set_mode(\"classification\") %>% \r\n  set_args(trees = tune(),\r\n           mtry = tune(),\r\n           min_n = tune())\r\n\r\n# xgboost\r\nxgb_spec <-\r\n  boost_tree() %>% \r\n  set_engine(\"xgboost\") %>% \r\n  set_mode(\"classification\") %>% \r\n  set_args(trees = tune(),\r\n           tree_depth = tune(),\r\n           min_n = tune(),\r\n           loss_reduction = tune(),\r\n           sample_size = tune(),\r\n           mtry = tune(),\r\n           learn_rate = tune(),\r\n           stop_iter = 10)\r\n\r\n# knn\r\nknn_spec <-\r\n  nearest_neighbor() %>% \r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"classification\") %>% \r\n  set_args(neighbors = tune(),\r\n           weight_func = \"optimal\",\r\n           dist_power = tune())\r\n\r\n#lightgbm\r\nlgb_spec <-\r\n  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(),\r\n             learn_rate = tune(), stop_iter = 10) %>%\r\n  set_engine(\"lightgbm\") %>%\r\n  set_mode(\"classification\")\r\n\r\n\r\n# Logistic Regression Model\r\nlogistic_spec <- \r\n  logistic_reg() %>%\r\n  set_engine(engine = 'glm') %>%\r\n  set_mode('classification') \r\n\r\nnull_spec <-\r\n  null_model() %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"parsnip\")\r\n\r\n\r\nEvaluate multiple recipes and models using workflowset\r\nWith workflowset, its super convenient to evaluate multiple recipes and models in one step.\r\n\r\n\r\nbase_set <- \r\n  workflow_set (\r\n    list(base_recipe = rec_base,\r\n         upsample_recipe = rec_upsample), #preprocessor\r\n    list(null = null_spec,\r\n         logistic = logistic_spec,\r\n         knn = knn_spec,\r\n         lightgbm = lgb_spec,\r\n         xgboost = xgb_spec,\r\n         rand_forest = rf_spec\r\n    ), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\n\r\nTune hyper-parameters using tune_grid\r\nAre you ready to tune hyper-parameters? I hope my laptop doesn’t crash! Since the dataset is small, and we’re only tuning 12 recipe-model combinations, I will use tune_grid rather than tune_race_anova.\r\n\r\n\r\n# tune_grid\r\nset.seed(2024030703)\r\ndoParallel::registerDoParallel(cl=3, cores = 6)\r\n\r\nfirst_tune_results <-\r\n  workflow_map(base_set,\r\n               fn = \"tune_grid\",\r\n               verbose = TRUE,\r\n               seed = 2024030703,\r\n               grid = 11,\r\n               resamples = data_fold,\r\n               metrics = metric_set(roc_auc, accuracy, f_meas),\r\n               control = control_grid(verbose = TRUE,\r\n                                      allow_par = TRUE,\r\n                                      parallel_over = \"everything\"))\r\n\r\n\r\nLet’s take a look at the results. Any guesses for which model performed best?\r\n\r\n\r\nfirst_tune_results %>% \r\n  autoplot(\"roc_auc\") + \r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\n\r\nfirst_tune_results %>% \r\n  workflowsets::rank_results(rank_metric = \"roc_auc\") %>% \r\n  filter(.metric == \"roc_auc\") %>% \r\n  dplyr::select(wflow_id, mean, std_err, rank) %>% \r\n  datatable() %>% \r\n  formatRound(columns = c(\"mean\", \"std_err\"),\r\n              digits = 3)\r\n\r\n\r\n\r\nI am surprised indeed, a simple Logistic Regression model performed the best, followed by xgboost. Upsampling did not appear to improve model performance. Let’s see if we can further finetune the xgboost model to improve its results.\r\nSpecifying better hyper-parameter ranges\r\nLet’s see if we can further improve on the xgboost model by fine-tuning hyper-parameters even further. I start by examining the hyper-parameters.\r\n\r\n\r\nfirst_tune_results %>% \r\n  extract_workflow_set_result(\"base_recipe_xgboost\") %>% \r\n  autoplot()\r\n\r\n\r\n\r\nBetter “space-filling” grid using grid_latin_hypercube\r\nLet’s define better ranges for each hyper-parameter to be tuned. I will also use grid_latin_hypercube to create a better “space-filling” grid.\r\n\r\n\r\n# xgboost wflow\r\nxgboost_wflow <-\r\n  workflow() %>% \r\n  add_model(xgb_spec) %>% \r\n  add_recipe(rec_base) \r\n\r\nparam <-\r\n  xgboost_wflow %>% \r\n  extract_parameter_set_dials() %>% \r\n  finalize(data_train) %>% \r\n  update(mtry = mtry(c(20L,30L)),\r\n         trees = trees(c(300L,2000L)),\r\n         min_n = min_n(c(30L,50L)),\r\n         tree_depth = tree_depth(c(5L,8L)),\r\n         learn_rate = learn_rate(c(-2.2,-1.5)),\r\n         loss_reduction = loss_reduction(c(-10.1,-6.1))\r\n         )\r\n\r\n\r\nset.seed(2024030704)\r\ndoParallel::registerDoParallel(cl=3, cores = 6)\r\n\r\ngrid_xgboost <- grid_latin_hypercube(param, size = 50)\r\n\r\nxgboost_tune_grid <-\r\n  tune_grid(\r\n    object = xgboost_wflow,\r\n    resamples = data_fold,\r\n    metrics = xgboost_metrics,\r\n    grid = grid_xgboost,\r\n    control = control_grid(verbose = TRUE)\r\n  )\r\n\r\nxgboost_tune_grid %>% \r\n  collect_metrics() %>% \r\n  filter(.metric == \"roc_auc\") %>% \r\n  arrange(-mean)\r\n\r\n\r\nI managed to marginally improve xgboost model performance from 0.843 to a roc_auc score of 0.845, which is still behind the logistic model.\r\nfunetune using simulated annealing with tune_sim_anneal\r\nLet’s see if simulated annealing using tune_sim_anneal can improve roc_auc further. We can use the results from tune_grid as initial values.\r\n\r\n\r\nset.seed(2024030704)\r\ndoParallel::registerDoParallel(cl=3, cores = 6)\r\n\r\nxgboost_sim_anneal_result <-\r\n  tune_sim_anneal(\r\n    object = xgboost_wflow,\r\n    resamples = data_fold,\r\n    iter = 100,\r\n    initial = xgboost_tune_grid,\r\n    metrics = xgboost_metrics,\r\n    param_info = param,\r\n    control = control_sim_anneal(verbose = TRUE,\r\n                                 verbose_iter = TRUE,\r\n                                 no_improve = 20L,\r\n                                 allow_par = TRUE,\r\n                                 restart = 10L,\r\n                                 parallel_over = \"everything\")\r\n  )\r\n\r\nxgboost_sim_anneal_result %>%\r\n  collect_metrics() %>% \r\n  filter(.metric == \"roc_auc\") %>% \r\n  arrange(-mean)\r\n\r\n\r\nNope, there was no imporvement, we are still stuck on 0.845. Let’s extract this set of hyper-parameters.\r\n\r\n\r\nxgboost_param <-\r\n  xgboost_sim_anneal_result %>% \r\n  select_best(metric = \"roc_auc\")\r\n\r\n\r\nFit and predict\r\nLet’s fit both models and use them to predict on the test data, and assess model performance.\r\n\r\n\r\n# finalize workflow\r\nxgboost_tune_wflow <-\r\n  xgboost_wflow %>% \r\n  finalize_workflow(xgboost_param)\r\n\r\n# last fit\r\nxgboost_final_fit <-\r\n  xgboost_tune_wflow %>% \r\n  last_fit(data_split)\r\n\r\nlogistic_final_fit <-\r\n  workflow() %>% \r\n  add_model(logistic_spec) %>% \r\n  add_recipe(rec_base) %>% \r\n  last_fit(data_split)\r\n\r\nNULL_final_fit <-\r\n  workflow() %>% \r\n  add_model(null_spec) %>% \r\n  add_recipe(rec_base) %>% \r\n  last_fit(data_split)\r\n\r\n\r\nLet’s collect the results metrics.\r\n\r\n\r\nmetrics_xgboost <-\r\n  xgboost_final_fit %>% \r\n  collect_metrics() %>% \r\n  mutate(algo = \"Base Recipe xgboost\")\r\n\r\nmetrics_logistic <-\r\n  logistic_final_fit %>% \r\n  collect_metrics() %>% \r\n  mutate(algo = \"Base Recipe Logistic\")\r\n\r\nmetrics_NULL <-\r\n  NULL_final_fit %>% \r\n  collect_metrics() %>% \r\n  mutate(algo = \"Base Recipe NULL\")\r\n\r\nmetrics_all <-\r\n  bind_rows(metrics_xgboost, metrics_logistic, metrics_NULL) %>% \r\n  pivot_wider(names_from = \".metric\",\r\n              values_from = \".estimate\") %>% \r\n  dplyr::select(-.estimator, -.config)\r\n\r\n\r\nInterestingly, although the Logistic Regression model performed better on the training data, xgboost was able to trump it on the test data. In this case, both models performed better on the test data. Let’s collect predictions and plot an roc_auc curve.\r\n\r\n\r\npred_xgboost <-\r\n  xgboost_final_fit %>% \r\n  collect_predictions() %>% \r\n  mutate(algo = \"Base Recipe xgboost\")\r\n\r\npred_logistic <-\r\n  logistic_final_fit %>% \r\n  collect_predictions() %>% \r\n  mutate(algo = \"Base Recipe Logistic\")\r\n\r\npred_NULL <-\r\n  NULL_final_fit %>% \r\n  collect_predictions() %>% \r\n  mutate(algo = \"Base Recipe NULL\")\r\n\r\npred_all <-\r\n  bind_rows(pred_xgboost, pred_logistic, pred_NULL) \r\n\r\n\r\n\r\n\r\npred_all %>% \r\n  group_by(algo) %>% \r\n  roc_curve(Attrition,\r\n            .pred_Yes) %>% \r\n  autoplot()\r\n\r\n\r\n\r\nWe can also visualize features by importance.\r\n\r\n\r\nvipplot_xgboost <-\r\n  xgboost_final_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  vip(geom = \"col\",\r\n      num_features = 10L) +\r\n  labs(x = \"Importance\",\r\n       y = \"Features of Importance\",\r\n       title = \"Features of Importance - xgboost\") +\r\n  theme_bw()\r\n\r\nvipplot_logistic <-\r\n  logistic_final_fit %>% \r\n    extract_fit_parsnip() %>% \r\n  vip(geom = \"col\",\r\n      num_features = 10L) +\r\n    labs(x = \"Importance\",\r\n       y = \"Features of Importance\",\r\n       title = \"Features of Importance - Logistic\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nvipplot_xgboost + vipplot_logistic + plot_layout(nrow = 2)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-03-07-using-ml-to-predict-customer-attrition/using-ml-to-predict-customer-attrition_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2024-03-10T09:13:51+08:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "ml/2024-03-03-pharmaceutical-machine-learning-with-tidymodels/",
    "title": "Pharmaceutical machine learning with tidymodels",
    "description": "Use Machine Learning to develop a model to determine if a proposed drug could be a mutagen.",
    "author": [],
    "date": "2024-03-03",
    "categories": [
      "classification",
      "random forest",
      "logistic regression",
      "tune_grid",
      "tune_bayes",
      "pca"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nImporting the data\r\nSplitting the Data\r\nModeling approach\r\nCreate recipes\r\nSpecify models\r\nWorkflowset\r\nTune hyper-parameters using tune_grid\r\nLast_fit and collect predictions\r\nWhich features were important?\r\nMaterial added 10 March 2024\r\n\r\n\r\n\r\nIntroduction\r\nI learnt about this dataset while watching Ep. 7: End-to-end machine learning workflow with tidymodels & Posit Team on Posit’s youtube channel. You can also read about it on Posit’s blogpost.\r\nThis episode was hosted by Simon Couch whom I first learnt about while researching about tidymodel stacks. Here is a link to Simon’s github page where you can obtain the dataset for practice.\r\nHere is a brief description of the project. A group of scientists investigate whether they can use drug information to predict if a proposed drug could be a mutagen (i.e., toxicity caused by damage to DNA). In pharmaceutical research, mutagenicity refers to a drug’s tendency to increase the rate of mutations due to the damage of genetic material, a key indicator that a drug may be a carcinogen. Mutagenicity can be evaluated using a lab test, though the test requires experienced scientists and time in the lab. A group of scientists are studying whether, instead, they can use known information to quickly predict the mutagenicity of new drugs.\r\nImporting the data\r\n\r\n\r\nrm(list=ls())\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\", \"finetune\", \"themis\", \"embed\", \"fastICA\", \"dimRed\", \"uwot\", \"butcher\", # for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\",#load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", \"kknn\", \"earth\", \"klaR\", \"discrim\", \"naivebayes\", \"baguette\", \"kernlab\",#random forest\r\n               \"janitor\", \"lubridate\")\r\nload(\"mutagen_results.RData\")\r\n\r\n\r\n\r\n\r\ndf <-read_csv(\"mutagen.csv\")\r\ndf %>% count(outcome)\r\ndata <-\r\n  df %>% \r\n  dplyr::select(-1) %>% \r\n  mutate(outcome = fct_relevel(outcome, \"mutagen\")) %>% \r\n  janitor::clean_names()\r\n# confirm no NA data\r\ntable(is.na(data))\r\n\r\n\r\nThis is a “wide” data set, with 1580 columns and 4335 rows. It appears to be an ideal dataset to investigate dimensionality reduction and feature extraction, which I will be learning about next week in Module 3 of my class.\r\nThere is no missing data, so we do not have to address it in a recipe step. I will skip the EDA portion, and move straight into splitting the data.\r\nA note for those who intend to follow along, or make use of this dataset. It is computationally intensive. My desktop running a ryzen 3950X with 64gb ram ran first_tune in approx 4 hours. Subsequently, 10 iterations of tune_bayes took approx 12 hours.\r\nSplitting the Data\r\nI split the data into a training and a test set, strata by outcome. I also create a cross validation dataset for tuning hyper-parameters.\r\n\r\n\r\nset.seed(2024030101)\r\ndata_split <-\r\n  data %>% \r\n  initial_split(strata = outcome)\r\n\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = outcome)\r\n\r\n\r\nModeling approach\r\nSimon approached this problem by evaluating several models, then fine-tuning the chosen model, xgboost, using tune_sim_anneal. Random Forest wasn’t among the models he evaluated.\r\nI will approach this problem slightly differently. I will evaluate 4 recipes: the first is a “basic” rec_base with all features. Then, I will create 3 other recipes which use different methods for dimension reduction and feature extraction. They are:\r\n- rec_pca where I will use step_pca for principal component analysis method,\r\n- rec_ica where I will use step_ica for independent component analysis method,\r\n- rec_umap where I will use step_umap for uniform manifold approximation and projection method.\r\nI also also evaluate only one model, the random forest model, and tune its hyper-parameters using tune_grid.\r\nFinally, I will fine-tune the hyper-parameters further using tune_bayes.\r\nCreate recipes\r\nLet’s create the 4 recipes.\r\n\r\n\r\nrec_base <-\r\n  recipes::recipe(formula = outcome ~.,\r\n                  data = data_train) %>% \r\n  step_zv(all_predictors()) %>% # remove zero variance\r\n  step_YeoJohnson(all_numeric_predictors()) %>% \r\n  step_normalize(all_numeric_predictors())\r\n\r\nrec_pca <-\r\n  rec_base %>% \r\n  step_pca(all_numeric_predictors(), num_comp = tune(), threshold = tune())\r\n# \r\n# rec_pls <-\r\n#   rec_base %>% \r\n#   step_pls(all_numeric_predictors(), outcome = \"outcome\", num_comp = tune())\r\n\r\nrec_ica <-\r\n  rec_base %>% \r\n  step_ica(all_numeric_predictors(), num_comp = tune())\r\n\r\nrec_umap <-\r\n  rec_base %>% \r\n  step_umap(all_numeric_predictors(),\r\n            outcome = \"outcome\",\r\n            num_comp = tune(),\r\n            neighbors = tune(),\r\n            min_dist = tune(),\r\n            learn_rate = tune(),\r\n            epochs = tune())\r\n\r\n\r\nSpecify models\r\nI will evaluate the performance of the random forest model.\r\n\r\n\r\n# random forest\r\nspec_rf <-\r\n  rand_forest() %>% \r\n  set_engine(\"ranger\",\r\n             importance = \"impurity\") %>% \r\n  set_mode(\"classification\") %>% \r\n  set_args(trees = tune(),\r\n           mtry = tune(),\r\n           min_n = tune())\r\n\r\n\r\nWorkflowset\r\nI created a workflowset, which will conveniently allow me to evaluate the random forest model across both 4 recipes. It was necessary to set parameter ranges for the various hyper-parameters that were going to be tuned. I ran into errors without this step.\r\n\r\n\r\n# workflow set\r\nbase_set <- \r\n  workflow_set (\r\n    list(ica = rec_ica,\r\n         pca = rec_pca,\r\n         #PLS = rec_pls,\r\n         umap = rec_umap,\r\n         basic = rec_base), #preprocessor\r\n    list(rf = spec_rf), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\n# set parameters\r\n\r\nparam_basic <-\r\n  base_set %>%\r\n  extract_workflow(id = \"basic_rf\") %>% \r\n  extract_parameter_set_dials() %>% \r\n  finalize(data_train)\r\n\r\nparam_ica <-\r\n  base_set %>%\r\n  extract_workflow(id = \"ica_rf\") %>% \r\n  extract_parameter_set_dials() %>% \r\n  finalize(data_train)\r\n\r\nparam_pca <-\r\n  base_set %>%\r\n  extract_workflow(id = \"pca_rf\") %>% \r\n  extract_parameter_set_dials() %>% \r\n  finalize(data_train)\r\n\r\nparam_umap <-\r\n  base_set %>%\r\n  extract_workflow(id = \"umap_rf\") %>% \r\n  extract_parameter_set_dials() %>% \r\n  finalize(data_train)\r\n\r\nbase_set <-\r\n  base_set %>% \r\n  option_add(param_info = param_basic, id = \"basic_rf\") %>% \r\n  option_add(param_info = param_ica, id = \"ica_rf\") %>% \r\n  option_add(param_info = param_pca, id = \"pca_rf\") %>% \r\n  option_add(param_info = param_umap, id = \"umap_rf\")\r\n\r\n\r\nTune hyper-parameters using tune_grid\r\nLet’s tune hyper-parameters using tune_grid, setting registerDoParallel for speedy processing.\r\n\r\n\r\nset.seed(2024030302)\r\ncl <- (detectCores()/2) - 1\r\ncores <- cl*2\r\n\r\ndoParallel::registerDoParallel(cl, cores)\r\n\r\nfirst_tune <-\r\n  workflow_map(base_set,\r\n               fn = \"tune_grid\",\r\n               verbose = TRUE,\r\n               seed = 2024030302,\r\n               grid = 11,\r\n               resamples = data_fold,\r\n               metrics = metric_set(roc_auc, accuracy),\r\n               control = control_grid(verbose = TRUE,\r\n                                      allow_par = TRUE,\r\n                                      parallel_over = \"everything\"))\r\nsave(first_tune, file = \"first_tune.Rda\")\r\n\r\n\r\nOnce tuning is done, we can use autoplot to view the results.\r\n\r\n\r\nautoplot(first_tune) + theme_bw() + theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\nLet’s rank the results by “roc_auc”.\r\n\r\n\r\nfirst_tune %>% \r\n  workflowsets::rank_results(rank_metric = \"roc_auc\") %>% \r\n  filter(.metric == \"roc_auc\") %>% \r\n  dplyr::select(wflow_id, mean, std_err, rank) %>% \r\n  datatable() %>% \r\n  formatRound(columns = c(\"mean\", \"std_err\"),\r\n              digits = 3)\r\n\r\n\r\n\r\nWe see that the best performing model is a random forest model with a base recipe. It has a roc_auc score of 0.900. Let’s take a look at its hyper-parameters. I’m surprised that none of the other recipes made in into the top 10 positions. The next best recipe rf combination was pca_rf with a roc_auc score of 0.87.\r\n\r\n\r\nfirst_tune_param <-\r\n  first_tune %>% \r\n  extract_workflow_set_result(id = \"pca_rf\") %>% \r\n  select_best(metric = \"roc_auc\")\r\n\r\n\r\nHyper-parameters are mtry of 1546 and min_n of 31. Let’s see if we can improve on these results by performing iterative search using tune_bayes.\r\n\r\n\r\n# base recipe and rf workflow\r\nbasic_rf_res <-\r\n  first_tune %>% \r\n  extract_workflow_set_result(id = \"basic_rf\")\r\nbasic_rf_wflow <-\r\n  workflow() %>% \r\n  add_recipe(rec_base) %>% \r\n  add_model(spec_rf)\r\n\r\nset.seed(2024030303)\r\ncl <- (detectCores()/2) - 1\r\ncores <- cl*2\r\n\r\ndoParallel::registerDoParallel(cl, cores)\r\n\r\nbayes_tune <-\r\n  basic_rf_wflow %>% \r\n  tune_bayes(\r\n  resamples = data_fold,\r\n  metrics = metric_set(roc_auc, accuracy),\r\n  initial = basic_rf_res,\r\n  iter = 25,\r\n  param_info = param_basic,\r\n  control = control_bayes(verbose = TRUE,\r\n                          verbose_iter = TRUE,\r\n                          no_improve = 10L,\r\n                          allow_par = TRUE,\r\n                          parallel_over = \"everything\")\r\n)\r\n\r\nsave(bayes_tune, file = \"bayes_tune.Rda\")\r\n\r\n\r\nThe search is “done”. Well, I actually ended it early as I got tired of waiting. My PC had already been on for over 36 hours and electricity is expensive! Let’s take a look at the results.\r\n\r\n\r\nbayes_tune %>% \r\n  collect_metrics(summarize = FALSE) %>% \r\n  dplyr::select(-.estimator) %>% \r\n  filter(.metric ==\"roc_auc\") %>% \r\n  arrange(desc(.estimate)) %>% \r\n  datatable() %>% \r\n  formatRound(columns = \".estimate\",\r\n              digits = 3)\r\n\r\n\r\n\r\nYes! I have been able to improve the model performance, achieving an roc_auc of 0.918 with hyper-parameters: 1962 trees, mtry 826 and min_n 2. Let’s finalize the workflow\r\nLast_fit and collect predictions\r\nLet’s extract the workflow and last_fit to data_split.\r\n\r\n\r\nbase_rf_tuned_workflow <-\r\n  basic_rf_wflow %>% \r\n  finalize_workflow(bayes_tune %>% select_best(metric = \"roc_auc\"))\r\n\r\nbase_rf_fit <-\r\n  base_rf_tuned_workflow %>% \r\n  last_fit(data_split)\r\n\r\n\r\nLet’s compare it’s metrics on the test set vs training set.\r\n\r\n\r\nbase_rf_fit %>% collect_metrics()\r\n\r\n    ┌────────────────────────────────────────────────────┐\r\n    │ .metric    .estimator   .estimate   .config        │\r\n    ├────────────────────────────────────────────────────┤\r\n    │ accuracy   binary           0.848   Preprocessor1_ │\r\n    │                                     Model1         │\r\n    │ roc_auc    binary           0.912   Preprocessor1_ │\r\n    │                                     Model1         │\r\n    └────────────────────────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate, .config\r\n\r\nWe can also collect predictions and plot a roc_auc curve.\r\n\r\n\r\npred_rf <-\r\n  base_rf_fit %>% \r\n  collect_predictions() %>% \r\n  mutate(algo = \"Basic RF\")\r\n\r\n\r\n\r\n\r\npred_rf %>% \r\n  roc_curve(outcome, .pred_mutagen) %>% \r\n  autoplot()\r\n\r\n\r\n\r\nWhich features were important?\r\nLet’s view the features by importance using extract_fit_parsnip and vip\r\n\r\n\r\nbase_rf_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  vip(geom = \"col\",\r\n      num_features = 20L) +\r\n  labs(x = \"Importance\",\r\n       y = \"Features\",\r\n       title = \"Features of Importance - Basic Recipe Random Forest Model\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nMaterial added 10 March 2024\r\nIn a piece of homework completed recently Using ML to predict customer attrition, a simple Logistic Regression model was able to outperform an xgboost and K-nearest neighbor model when tested against training data.\r\nI wonder how a Logistic Regression model would perform here? Let’s investigate.\r\n\r\n\r\n# Logistic Regression Model\r\nlogistic_spec <- \r\n  logistic_reg() %>%\r\n  set_engine(engine = 'glm') %>%\r\n  set_mode('classification') \r\n\r\nlogistic_set <-\r\n    workflow_set (\r\n    list(ica = rec_ica,\r\n         pca = rec_pca,\r\n         #PLS = rec_pls,\r\n         umap = rec_umap,\r\n         basic = rec_base), #preprocessor\r\n    list(logistic = logistic_spec\r\n    ), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\nset.seed(2024030304)\r\ncl <- (detectCores()/2) - 1\r\ncores <- cl*2\r\ndoParallel::registerDoParallel(cl, cores)\r\n\r\nlogistic_tune <-\r\n  workflow_map(logistic_set,\r\n               fn = \"tune_grid\",\r\n               verbose = TRUE,\r\n               seed = 2024030302,\r\n               grid = 11,\r\n               resamples = data_fold,\r\n               metrics = metric_set(roc_auc, accuracy),\r\n               control = control_grid(verbose = TRUE,\r\n                                      allow_par = TRUE,\r\n                                      parallel_over = \"everything\"))\r\nsave(logistic_tune, file = \"logistic_tune.Rda\")\r\n\r\n\r\nThe performance on training data isnt too shabby, and it actually favored the rec_pca recipe. It achieved a roc_auc of 0.912.\r\n\r\n\r\nlogistic_tune %>% \r\n  collect_metrics(summarize = FALSE) %>% \r\n  dplyr::select(-.estimator) %>% \r\n  filter(.metric ==\"roc_auc\") %>% \r\n  arrange(desc(.estimate)) %>% \r\n  datatable() %>% \r\n  formatRound(columns = \".estimate\",\r\n              digits = 3)\r\n\r\n\r\n\r\nNext, I evaluated its performance on the test data.\r\n\r\n\r\npca_logistic_tuned_workflow <-\r\n  workflow() %>% \r\n  add_recipe(rec_pca) %>% \r\n  add_model(logistic_spec) %>% \r\n  finalize_workflow(logistic_tune %>%\r\n                      extract_workflow_set_result(\"pca_logistic\") %>%\r\n                      select_best(metric = \"roc_auc\"))\r\n\r\npca_logistic_fit <-\r\n  pca_logistic_tuned_workflow %>% \r\n  last_fit(data_split)\r\n\r\npca_logistic_fit %>% collect_metrics()\r\n\r\npred_logistic <-\r\n  pca_logistic_fit %>% \r\n  collect_predictions() %>% \r\n  mutate(algo = \"PCA Logistic\")\r\n\r\npred_logistic %>% \r\n  roc_curve(outcome, .pred_mutagen) %>% \r\n  autoplot()\r\n\r\n\r\nSimilar to the previous exercise, the Logistic Regression model didn’t perform as well on test data. Here’s a plot of both roc_auc curves.\r\n\r\n\r\nresults <-\r\n  bind_rows(pred_rf, pred_logistic)\r\n\r\n\r\n\r\n\r\nresults %>% \r\n  group_by(algo) %>% \r\n  roc_curve(outcome, .pred_mutagen) %>% \r\n  autoplot() +\r\n  theme(legend.position = \"bottom\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-03-03-pharmaceutical-machine-learning-with-tidymodels/pharmaceutical-machine-learning-with-tidymodels_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2024-03-11T15:34:22+08:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "ml/2024-02-14-using-machine-learning-to-predict-risk-of-type2-diabetes/",
    "title": "using machine learning to predict risk of type2 diabetes",
    "description": "Type 2 diabetes is one of the most prevalent chronic diseases in the United States, affecting the health of millions of people, and putting an enormous financial burden on the US economy.",
    "author": [
      {
        "name": "Mark Y",
        "url": {}
      }
    ],
    "date": "2024-02-14",
    "categories": [
      "classification",
      "logistic regression",
      "LASSO regression",
      "random forest",
      "decision tree",
      "naive bayes",
      "knn",
      "xgboost"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData Source\r\n\r\n\r\n\r\nIntroduction\r\nThis “assignment” was inspired on the works of Xie Z, Nikolayeva O, Luo\r\nJ, Li D. Building Risk Prediction Models for Type 2 Diabetes Using\r\nMachine Learning Techniques. Their paper can be accessed via this\r\nlink.\r\nMy objective is to practice and learn how to build predictive models\r\nusing machine learning techniques, in the spirit of the original study,\r\nbut using the most recent survey data (2022). It would be a bonus if my\r\nmodels came close to the performance of Dr Xie’s.\r\nTo recap, the original definition of an individual with Type 2 Diabetes\r\nis: - an individual aged 30 years or older (respondents younger than 30\r\nyears old were excluded as they most likely had Type 1 diabetes), - an\r\nindividual who had been told by a healthcare professional that he/she\r\nhad Type 2 diabetes, - respondents who had pre-diabetes, or respondents\r\nwho had diabetes while pregnant, were excluded from the study.\r\n\r\n\r\nrm(list = ls())\r\nsessionInfo()\r\n# Set packages and dependencies\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\",# for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\",#load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", \"kknn\", \"earth\", \"klaR\", \"discrim\", \"naivebayes\",#random forest\r\n               \"janitor\", \"lubridate\", \"haven\")\r\n\r\n\r\nData Source\r\nI obtained the latest available Behavioral Risk Factor Surveillance\r\nSystem (BRFSS 2022) data available from the Centers for Disease Control\r\nand Prevention.\r\nThe Behavioral Risk Factor Surveillance System (BRFSS) is the US’s\r\npremier system of health-related telephone surveys that collect state\r\ndata about U.S. residents regarding their health-related risk behaviors,\r\nchronic health conditions, and use of preventive services. Established\r\nin 1984 with 15 states, BRFSS now collects data in all 50 states as well\r\nas the District of Columbia and three U.S. territories. BRFSS completes\r\nmore than 400,000 adult interviews each year, making it the largest\r\ncontinuously conducted health survey system in the world.\r\nThe BRFSS 2022 data from CDC was stored in an SAS (.XPT) file format.\r\nThis was imported into R using read_xpt from the haven package. It\r\nhad 445132 rows representing individual survey responses and 328 columns representing variables.\r\n\r\n\r\ndf <- read_xpt(\"LLCP2022.XPT\")\r\n\r\n\r\nI included most of the independent variables from the original study, as\r\nwell as several new variables of interest. Below is a summary of\r\ndependent and independent variables used:\r\n\r\nVariable\r\nDescription\r\nValues\r\ndiabete4\r\n(Ever told) (you had) diabetes?\r\nyes, no\r\nbmi5cat\r\nFour-categories of BMI (body mass index)\r\n1. underweight, 2. normal weight, 3. overweight, 4.\r\nsmoker3\r\nFour-levels of smoker status\r\n1.everyday smoker, 2. someday smoker, 3. former smoker, 4. non-smoker\r\ncvdstrk3\r\n(Ever told) (you had) a stroke?\r\n1.yes, 2. no\r\ncvdcrhd4\r\n(Ever told) (you had) angina or coronary heart disease?\r\n1.yes, 2. no\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nGENHLTH Question: Would you say that in general your health is: 1\r\nExcellent 71,878 16.15 17.40 2 Very good 148,444 33.35 31.84 3 Good\r\n143,598 32.26 32.48 4 Fair 60,273 13.54 13.69 5 Poor 19,741 4.43 4.29 7\r\nDon’t know/Not Sure 810 0.18 0.19 9 Refused 385 0.09 0.10 BLANK Not\r\nasked or Missing 3 . .\r\n_AGEG5YR 1 Age 18 to 24 Notes: 18 <= AGE <= 24 26,941 6.05 11.90 2\r\nAge 25 to 29 Notes: 25 <= AGE <= 29 21,990 4.94 7.72 3 Age 30 to 34\r\nNotes: 30 <= AGE <= 34 25,807 5.80 9.38 4 Age 35 to 39 Notes: 35 <=\r\nAGE <= 39 28,526 6.41 7.63 5 Age 40 to 44 Notes: 40 <= AGE <= 44\r\n29,942 6.73 8.41 6 Age 45 to 49 Notes: 45 <= AGE <= 49 28,531 6.41\r\n6.49 7 Age 50 to 54 Notes: 50 <= AGE <= 54 33,644 7.56 7.72 8 Age 55\r\nto 59 Notes: 55 <= AGE <= 59 36,821 8.27 7.31 9 Age 60 to 64 Notes: 60\r\n<= AGE <= 64 44,511 10.00 8.67 10 Age 65 to 69 Notes: 65 <= AGE <=\r\n69 47,099 10.58 6.98 11 Age 70 to 74 Notes: 70 <= AGE <= 74 43,472\r\n9.77 6.32 12 Age 75 to 79 Notes: 75 <= AGE <= 79 32,518 7.31 4.37 13\r\nAge 80 or older Notes: 80 <= AGE <= 99 36,251 8.14 4.94 14 Don’t\r\nknow/Refused/Missing Notes: 7 <= AGE <= 9 9,079 2.04 2.15\r\n_BMI5CAT Question: Four-categories of Body Mass Index (BMI) 1\r\nUnderweight Notes: _BMI5 < 1850 (_BMI5 has 2 implied decimal places)\r\n6,778 1.71 2.03 2 Normal Weight Notes: 1850 <= _BMI5 < 2500 116,976\r\n29.52 30.50 3 Overweight Notes: 2500 <= _BMI5 < 3000 139,995 35.32\r\n34.14 4 Obese Notes: 3000 <= _BMI5 < 9999 132,577 33.45 33.32\r\nCHECKUP1 Question: About how long has it been since you last visited a\r\ndoctor for a routine checkup? 1 Within past year (anytime less than 12\r\nmonths ago) 350,944 78.84 74.97 2 Within past 2 years (1 year but less\r\nthan 2 years ago) 41,919 9.42 10.74 3 Within past 5 years (2 years but\r\nless than 5 years ago) 24,882 5.59 6.75 4 5 or more years ago 19,079\r\n4.29 5.13 7 Don’t know/Not sure 5,063 1.14 1.39 8 Never 2,509 0.56 0.83\r\n9 Refused 733 0.16 0.20\r\nINCOME3 Question: Is your annual household income from all sources: (If\r\nrespondent refuses at any income level, code ´Refused.´) 1 Less than\r\n$10,000 10,341 2.39 2.95 2 Less than $15,000 ($10,000 to < $15,000)\r\n11,031 2.55 2.43 3 Less than $20,000 ($15,000 to < $20,000) 14,300\r\n3.31 3.44 4 Less than $25,000 ($20,000 to < $25,000) 20,343 4.71\r\n4.71 5 Less than $35,000 ($25,000 to < $35,000) 42,294 9.79 9.92 6\r\nLess than $50,000 ($35,000 to < $50,000) 46,831 10.84 10.20 7 Less\r\nthan $75,000 ($50,000 to < $75,000) 59,148 13.69 12.42 8 Less than\r\n$100,000? ($75,000 to < $100,000) 48,436 11.21 10.42 9 Less than\r\n$150,000? ($100,000 to < $150,000)? 50,330 11.65 11.19 10 Less than\r\n$200,000? ($150,000 to < $200,000) 22,553 5.22 5.39 11 $200,000 or\r\nmore 23,478 5.43 6.13 77 Don’t know/Not sure 36,114 8.36 10.44 99\r\nRefused 47,001 10.87 10.37 BLANK Not asked or Missing 12,932 . .\r\nFLUSHOT7 Question: During the past 12 months, have you had either flu\r\nvaccine that was sprayed in your nose or flu shot injected into your\r\narm? 1 Yes 209,256 52.11 44.53 2 No—Go to Section 15.03 PNEUVAC4 188,755\r\n47.01 54.46 7 Don’t know/Not Sure—Go to Section 15.03 PNEUVAC4 2,455\r\n0.61 0.69 9 Refused—Go to Section 15.03 PNEUVAC4 1,073 0.27 0.32 BLANK\r\n43,593 . .\r\nEMPLOY1 Question: Are you currently…? 1 Employed for wages 186,004 42.38\r\n47.34 2 Self-employed 38,768 8.83 9.46 3 Out of work for 1 year or more\r\n8,668 1.97 2.54 4 Out of work for less than 1 year 8,044 1.83 2.56 5 A\r\nhomemaker 17,477 3.98 4.94 6 A student 11,111 2.53 4.80 7 Retired\r\n137,083 31.23 20.46 8 Unable to work 26,737 6.09 6.41 9 Refused 5,044\r\n1.15 1.48 BLANK Not asked or Missing 6,196 . .\r\nSEXVAR Question: Sex of Respondent 1 Male—Code=1 if LANDSEX1=1 or\r\nCELLSEX1=1 or COLGSEX1=1 209,239 47.01 48.69 2 Female—Code=2 if\r\nLANDSEX1=2 or CELLSEX1=2 or COLGSEX1=2 235,893 52.99 51.31\r\nMARITAL Question: Are you: (marital status) 1 Married 227,424 51.09\r\n49.33 2 Divorced 57,516 12.92 10.20 3 Widowed 48,019 10.79 7.03 4\r\nSeparated 8,702 1.95 2.36 5 Never married 80,001 17.97 24.71 6 A member\r\nof an unmarried couple 18,668 4.19 5.20 9 Refused 4,794 1.08 1.18 BLANK\r\nNot asked or Missing 8 . .\r\nEDUCAG Question: Level of education completed 1 Did not graduate High\r\nSchool Notes: EDUCA = 1 or 2 or 3 26,011 5.84 11.63 2 Graduated High\r\nSchool Notes: EDUCA = 4 108,990 24.48 27.39 3 Attended College or\r\nTechnical School Notes: EDUCA = 5 120,252 27.01 30.04 4 Graduated from\r\nCollege or Technical School Notes: EDUCA = 6 187,496 42.12 30.34 9 Don’t\r\nknow/Not sure/Missing Notes: EDUCA = 9 or Missing 2,383 0.54 0.60\r\nSLEPTIM1 Question: On average, how many hours of sleep do you get in a\r\n24-hour period? 1 - 24 Number of hours [1-24] 439,679 98.78 98.57 77\r\nDon’t know/Not Sure 4,792 1.08 1.23 99 Refused 658 0.15 0.21 BLANK\r\nMissing 3 . .\r\nCVDCRHD4 Question: (Ever told) (you had) angina or coronary heart\r\ndisease? 1 Yes 26,551 5.96 4.40 2 No 414,176 93.05 94.67 7 Don’t\r\nknow/Not sure 4,044 0.91 0.84 9 Refused 359 0.08 0.10 BLANK Not asked or\r\nMissing 2 .\r\nPRIMINSR Question: What is the current primary source of your health\r\ninsurance? 1 A plan purchased through an employer or union (including\r\nplans purchased through another person´s employer) 161,388 36.26 39.07 2\r\nA private nongovernmental plan that you or another family member buys on\r\nyour own 36,931 8.30 9.28 3 Medicare 135,848 30.52 20.78 4 Medigap 536\r\n0.12 0.15 5 Medicaid 29,072 6.53 8.51 6 Children´s Health Insurance\r\nProgram (CHIP) 188 0.04 0.06 7 Military related health care: TRICARE\r\n(CHAMPUS) / VA health care / CHAMP- VA 15,373 3.45 3.28 8 Indian Health\r\nService 1,385 0.31 0.17 9 State sponsored health plan 12,878 2.89 2.76\r\n10 Other government program 10,630 2.39 2.70 88 No coverage of any type\r\n23,018 5.17 8.07 77 Don’t know/Not Sure 9,890 2.22 3.22 99 Refused 7,991\r\n1.80 1.95 BLANK Not asked or Missing 4 . .\r\nMENTHLTH Question: Now thinking about your mental health, which includes\r\nstress, depression, and problems with emotions, for how many days during\r\nthe past 30 days was your mental health not good? 1 - 30 Number of days\r\nNotes: _ _ Number of days 170,836 38.38 41.49 88 None 265,229 59.58\r\n56.10 77 Don’t know/Not sure 6,589 1.48 1.76 99 Refused 2,475 0.56 0.65\r\nBLANK Not asked or Missing 3 . .\r\nCHCKDNY2 Question: Not including kidney stones, bladder infection or\r\nincontinence, were you ever told you had kidney disease? 1 Yes 20,315\r\n4.56 3.68 2 No 422,891 95.00 95.87 7 Don’t know / Not sure 1,581 0.36\r\n0.35 9 Refused 343 0.08 0.10 BLANK Not asked or Missing 2 . .\r\n_TOTINDA Question: Adults who reported doing physical activity or\r\nexercise during the past 30 days other than their regular job 1 Had\r\nphysical activity or exercise Notes: EXERANY2 = 1 337,559 75.83 75.85 2\r\nNo physical activity or exercise in last 30 days Notes: EXERANY2 = 2\r\n106,480 23.92 23.85 9 Don’t know/Refused/Missing Notes: EXERANY2 = 7 or\r\n9 or Missing 1,093 0.25 0.29\r\nADDEPEV3 Question: (Ever told) (you had) a depressive disorder\r\n(including depression, major depression, dysthymia, or minor\r\ndepression)? 1 Yes 91,410 20.54 20.47 2 No 350,910 78.83 78.74 7 Don’t\r\nknow/Not sure 2,140 0.48 0.62 9 Refused 665 0.15 0.17 BLANK Not asked or\r\nMissing 7 . .\r\nRENTHOM1 Question: Do you own or rent your home? 1 Own 310,708 69.80\r\n66.63 2 Rent 108,332 24.34 25.81 3 Other arrangement 21,463 4.82 6.11 7\r\nDon’t know/Not Sure 1,099 0.25 0.49 9 Refused 3,521 0.79 0.96 BLANK Not\r\nasked or Missing Notes: Due to the nature of the data or the size of the\r\ntable for display, this information is not printed for this report 9 . .\r\nEXERANY2 Question: During the past month, other than your regular job,\r\ndid you participate in any physical activities or exercises such as\r\nrunning, calisthenics, golf, gardening, or walking for exercise? 1 Yes\r\n337,559 75.83 75.85 2 No 106,480 23.92 23.85 7 Don’t know/Not Sure 724\r\n0.16 0.18 9 Refused 367 0.08 0.11 BLANK Not asked or Missing 2 . .\r\nBLIND Question: Are you blind or do you have serious difficulty seeing,\r\neven when wearing glasses? 1 Yes 23,658 5.56 5.78 2 No 399,910 94.04\r\n93.75 7 Don’t know/Not Sure 1,042 0.25 0.27 9 Refused 667 0.16 0.20\r\nBLANK Not asked or Missing 19,855 . .\r\nDECIDE Question: Because of a physical, mental, or emotional condition,\r\ndo you have serious difficulty concentrating, remembering, or making\r\ndecisions? 1 Yes 50,100 11.81 13.34 2 No 370,792 87.42 85.81 7 Don’t\r\nknow/Not Sure 2,266 0.53 0.56 9 Refused 988 0.23 0.29 BLANK Not asked or\r\nMissing 20,986 . .\r\nHLTHPLN Question: Adults who had some form of health insurance 1 Have\r\nsome form of insurance Notes: PRIMINSR=1, 2, 3, 4, 5, 6, 7, 8, 9, 10\r\n404,229 90.81 86.77 2 Do not have some form of health insurance Notes:\r\nPRIMINSR=88 23,018 5.17 8.07 9 Don´t know, refused or missing insurance\r\nresponse Notes: PRIMINSR=77, 99, or missing 17,885 4.02 5.16\r\nDIABETE4 Question: (Ever told) (you had) diabetes? (If ´Yes´ and\r\nrespondent is female, ask ´Was this only when you were pregnant?´. If\r\nRespondent says pre-diabetes or borderline diabetes, use response code\r\n4.)\r\n1 Yes 61,158 13.74 12.04 2 Yes, but female told only during pregnancy—Go\r\nto Section 08.01 AGE 3,836 0.86 1.01 3 No—Go to Section 08.01 AGE\r\n368,722 82.83 84.34 4 No, pre-diabetes or borderline diabetes—Go to\r\nSection 08.01 AGE 10,329 2.32 2.27 7 Don’t know/Not Sure—Go to Section\r\n08.01 AGE 763 0.17 0.23 9 Refused—Go to Section 08.01 AGE 321 0.07 0.11\r\nBLANK Not asked or Missing 3 . .\r\n_SMOKER3 Question: Four-level smoker status: Everyday smoker, Someday\r\nsmoker, Former smoker, Non-smoker 1 Current smoker - now smokes every\r\nday Notes: SMOKE100 = 1 and SMOKEDAY = 1 36,003 8.09 8.09 2 Current\r\nsmoker - now smokes some days Notes: SMOKE100 = 1 and SMOKEDAY = 2\r\n13,938 3.13 3.54 3 Former smoker Notes: SMOKE100 = 1 and SMOKEDAY = 3\r\n113,774 25.56 21.87 4 Never smoked Notes: SMOKE100 = 2 245,955 55.25\r\n57.07 9 Don’t know/Refused/Missing Notes: SMOKE100 = 1 and SMOKEDAY = 9\r\nor SMOKE100 = 7 or 9 or Missing 35,462 7.97 9.44\r\nDRNKWK2 Question: Calculated total number of alcoholic beverages\r\nconsumed per week 0 Did not drink Notes: DROCDY4_=0 or AVEDRNK3=88\r\n188,832 42.42 41.91 1 - 98999 Number of drinks per week Notes: 0 <\r\nDROCDY4_ < 990 206,595 46.41 44.78 99900 Don’t know/Not\r\nsure/Refused/Missing Notes: AVEDRNK3=.,77,99 or DROCDY4_=900 49,705\r\n11.17 13.32\r\nDRNKANY6 Question: Adults who reported having had at least one drink of\r\nalcohol in the past 30 days. 1 Yes Notes: 1 <= ALCDAY4 <= 231 210,891\r\n47.38 46.04 2 No Notes: ALCDAY4=888 187,667 42.16 41.60 7 Don’t know/Not\r\nSure Notes: ALCDAY4=777 3,447 0.77 0.94 9 Refused/Missing Notes:\r\nALCDAY4=999, Missing 43,127 9.69 11.43\r\n_CURECI2 Question: Adults who are current e-cigarette users 1 Not\r\ncurrently using E-cigarettes Notes: ECIGNOW2=1, 4 387,356 87.02 83.59 2\r\nCurrent E-cigarette user Notes: ECIGNOW2=2,3 22,116 4.97 6.76 9 Don’t\r\nknow/Refused/Missing Notes: ECIGNOW2=7,9, or missing 35,660 8.01 9.64\r\n_RFSMOK3 Question: Adults who are current smokers 1 No Notes: _SMOKER3\r\n= 3 or 4 359,729 80.81 78.93 2 Yes Notes: _SMOKER3 = 1 or 2 49,941\r\n11.22 11.62 9 Don’t know/Refused/Missing Notes: _SMOKER3 = 9 35,462\r\n7.97 9.44\r\n_HADSIGM Question: Colonoscopy and sigmoidoscopy are exams to check for\r\ncolon cancer. Have you ever had either of these exams? 1 Yes 213,158\r\n72.82 68.17 2 No—Go to Section 11.06 COLNCNCR 76,372 26.09 30.53 7 Don’t\r\nknow/Not Sure—Go to Section 11.06 COLNCNCR 1,811 0.62 0.74 9 Refused—Go\r\nto Section 11.06 COLNCNCR 1,378 0.47 0.55 BLANK Not asked or Missing\r\nNotes: Section 08.01, AGE, is less than 45; 152,413 . .\r\n_INCOMG1 Question: Income categories 1 Less than $15,000 Notes:\r\nINCOME3=1,2 21,372 4.80 5.17 2 $15,000 to < $25,000 Notes:\r\nINCOME3=3,4 34,643 7.78 7.83 3 $25,000 to < $35,000 Notes: INCOME3=5\r\n42,294 9.50 9.54 4 $35,000 to < $50,000 Notes: INCOME3=6 46,831 10.52\r\n9.81 5 $50,000 to < $100,000 Notes: INCOME3=7,8 107,584 24.17 21.96 6\r\n$100,000 to < $200,000 Notes: INCOME3=9,10 72,883 16.37 15.95 7\r\n$200,000 or more Notes: INCOME3=11 23,478 5.27 5.89 9 Don’t know/Not\r\nsure/Missing Notes: INCOME3=77, 99, or missing 96,047 21.58 23.84\r\n_EDUCAG Question: Level of education completed 1 Did not graduate High\r\nSchool Notes: EDUCA = 1 or 2 or 3 26,011 5.84 11.63 2 Graduated High\r\nSchool Notes: EDUCA = 4 108,990 24.48 27.39 3 Attended College or\r\nTechnical School Notes: EDUCA = 5 120,252 27.01 30.04 4 Graduated from\r\nCollege or Technical School Notes: EDUCA = 6 187,496 42.12 30.34 9 Don’t\r\nknow/Not sure/Missing Notes: EDUCA = 9 or Missing 2,383 0.54 0.60\r\n_CHLDCNT Question: Number of children in household 1 No children in\r\nhousehold Notes: CHILDREN = 88 321,907 72.32 64.10 2 One child in\r\nhousehold Notes: CHILDREN = 01 46,241 10.39 13.23 3 Two children in\r\nhousehold Notes: CHILDREN = 02 37,923 8.52 10.83 4 Three children in\r\nhousehold Notes: CHILDREN = 03 15,975 3.59 4.78 5 Four children in\r\nhousehold Notes: CHILDREN = 04 5,521 1.24 1.66 6 Five or more children\r\nin household Notes: 05 <= CHILDREN < 88 3,100 0.70 0.97 9 Don’t\r\nknow/Not sure/Missing Notes: CHILDREN = 99 14,464 3.25 4.43 BLANK 1 . .\r\n_BMI5 Question: Body Mass Index (BMI)\r\nWTKG3 Question: Reported weight in kilograms\r\nHTM4 Question: Reported height in meters\r\n_AGE80 Question: Imputed Age value collapsed above 80 18 - 24 Imputed\r\nAge 18 to 24 26,943 6.05 11.90 25 - 29 Imputed Age 25 to 29 22,000 4.94\r\n7.73 30 - 34 Imputed Age 30 to 34 25,840 5.81 9.41 35 - 39 Imputed Age\r\n35 to 39 28,771 6.46 7.79 40 - 44 Imputed Age 40 to 44 30,403 6.83 8.68\r\n45 - 49 Imputed Age 45 to 49 29,580 6.65 6.86 50 - 54 Imputed Age 50 to\r\n54 37,404 8.40 8.54 55 - 59 Imputed Age 55 to 59 38,059 8.55 7.44 60 -\r\n64 Imputed Age 60 to 64 44,681 10.04 8.71 65 - 69 Imputed Age 65 to 69\r\n47,642 10.70 7.07 70 - 74 Imputed Age 70 to 74 44,940 10.10 6.53 75 - 79\r\nImputed Age 75 to 79 32,616 7.33 4.40 80 - 99 Imputed Age 80 or older\r\n36,253 8.14 4.94\r\n_RACEPR1 Question: Computed race groups used for internet prevalence\r\ntables 1 White only, non-Hispanic Notes: _RACE=1 or _RACE=9 and\r\n_IMPRACE=1 333,514 74.92 59.20 2 Black only, non-Hispanic Notes:\r\n_RACE=2 or _RACE=9 and _IMPRACE=2 35,876 8.06 11.62 3 American Indian\r\nor Alaskan Native only, Non-Hispanic Notes: _RACE=3 or _RACE=9 and\r\n_IMPRACE=4 7,120 1.60 1.21 4 Asian only, non-Hispanic Notes: _RACE=4\r\nor _RACE=9 and _IMPRACE=3 13,487 3.03 6.11 5 Native Hawaiian or other\r\nPacific Islander only, Non-Hispanic Notes: _RACE=5 2,414 0.54 0.48 6\r\nMultiracial, non-Hispanic Notes: _RACE=6 9,744 2.19 3.12 7 Hispanic\r\nNotes: _RACE=7 or _RACE=9 and _IMPRACE==5 42,977 9.65 18.25\r\n_DRDXAR2 Question: Respondents who have had a doctor diagnose them as\r\nhaving some form of arthritis 1 Diagnosed with arthritis Notes: HAVARTH4\r\n= 1 151,148 34.16 26.64 2 Not diagnosed with arthritis Notes: HAVARTH4 =\r\n2 291,351 65.84 73.36 BLANK Don´t know/Not Sure/Refused/Missing Notes:\r\nHAVARTH4 = 7 or 9 or Missing 2,633 . .\r\nASTHMA3 Question: (Ever told) (you had) asthma? 1 Yes 66,694 14.98 15.17\r\n2 No—Go to Section 07.06 CHCSCNC1 376,665 84.62 84.34 7 Don’t know/Not\r\nSure—Go to Section 07.06 CHCSCNC1 1,494 0.34 0.42 9 Refused—Go to\r\nSection 07.06 CHCSCNC1 277 0.06 0.08 BLANK Not asked or Missing 2 . .\r\n_DENVST3 Question: Adults who have visited a dentist, dental hygenist\r\nor dental clinic within the past year 1 Yes Notes: LASTDEN4=1 292,408\r\n65.69 62.66 2 No Notes: LASTDEN4=2 or 3 or 4 145,703 32.73 35.42 9 Don’t\r\nknow/Not Sure Or Refused/Missing Notes: LASTDEN4=7 or 9 or Missing 7,017\r\n1.58 1.93 BLANK Missing 4 . .\r\nSDHISOLT Question: How often do you feel socially isolated from others?\r\nIs it… 1 Always 8,098 3.19 4.06 2 Usually 13,178 5.19 5.63 3 Sometimes\r\n53,072 20.91 21.62 4 Rarely 70,617 27.82 26.18 5 Never 106,160 41.83\r\n41.21 7 Don’t know/Not Sure 1,696 0.67 0.79 9 Refused 969 0.38 0.50\r\nBLANK Not asked or Missing 191,342 . .\r\nLSATISFY Question: In general, how satisfied are you with your life? 1\r\nVery satisfied 114,252 44.89 42.07 2 Satisfied 123,445 48.51 50.46 3\r\nDissatisfied 10,758 4.23 4.67 4 Very dissatisfied 3,062 1.20 1.38 7\r\nDon’t know/Not sure 1,864 0.73 0.90 9 Refused 1,107 0.43 0.51 BLANK Not\r\nasked or Missing 190,644 . .\r\nDIFFWALK Question: Do you have serious difficulty walking or climbing\r\nstairs? 1 Yes 68,081 16.10 13.75 2 No 353,039 83.47 85.78 7 Don’t\r\nknow/Not Sure 1,221 0.29 0.28 9 Refused 636 0.15 0.19 BLANK Not asked or\r\nMissing 22,155 . .\r\nDIFFDRES Question: Do you have difficulty dressing or bathing? 1 Yes\r\n16,813 3.98 3.85 2 No 404,404 95.77 95.81 7 Don’t know/Not Sure 488 0.12\r\n0.15 9 Refused 548 0.13 0.19 BLANK Not asked or Missing 22,879 . .\r\nDEAF Question: Are you deaf or do you have serious difficulty hearing? 1\r\nYes 38,946 9.13 7.06 2 No 385,539 90.40 92.44 7 Don’t know/Not Sure\r\n1,246 0.29 0.27 9 Refused 757 0.18 0.23 BLANK Not asked or Missing\r\n18,644 . .\r\nPHYSHLTH Question: Now thinking about your physical health, which\r\nincludes physical illness and injury, for how many days during the past\r\n30 days was your physical health not good? 1 - 30 Number of days 166,386\r\n37.38 36.75 88 None 267,819 60.17 60.54 77 Don’t know/Not sure 8,875\r\n1.99 2.21 99 Refused 2,047 0.46 0.50 BLANK Not asked or Missing 5 . .\r\nCDASSIST Question: As a result of confusion or memory loss, how often do\r\nyou need assistance with these day-to-day activities? 1 Always 304 4.09\r\n4.57 2 Usually 281 3.78 4.90 3 Sometimes 1,354 18.22 21.00 4 Rarely—Go\r\nto Module 13.05 CDSOCIAL 1,447 19.47 19.25 5 Never—Go to Module 13.05\r\nCDSOCIAL 3,954 53.21 49.12 7 Don’t know/Not sure—Go to Module 13.05\r\nCDSOCIAL 78 1.05 1.04 9 Refused—Go to Module 13.05 CDSOCIAL 13 0.17 0.13\r\nBLANK Not asked or Missing Notes: Section 08.01, AGE, is less than 45;\r\nor Module 13.01, CIMEMLOS, is coded 2 or 9 437,701 . .\r\nCVDSTRK3 Question: (Ever told) (you had) a stroke. 1 Yes 19,239 4.32\r\n3.56 2 No 424,336 95.33 96.01 7 Don’t know/Not sure 1,274 0.29 0.35 9\r\nRefused 281 0.06 0.08 BLANK Not asked or Missing 2 . .\r\nCVDCRHD4 Question: (Ever told) (you had) angina or coronary heart\r\ndisease? 1 Yes 26,551 5.96 4.40 2 No 414,176 93.05 94.67 7 Don’t\r\nknow/Not sure 4,044 0.91 0.84 9 Refused 359 0.08 0.10 BLANK Not asked or\r\nMissing 2 . .\r\n\r\n\r\ndata <-\r\n  df %>% \r\n  dplyr::select(\"DIABETE4\", # response variable\r\n                # personal health\r\n                \"_BMI5CAT\", \"_BMI5\", #bmi cat, bmi numeric\r\n                \"_SMOKER3\", \"CVDSTRK3\", \"CVDCRHD4\", #smoke, stroke, heart disease\r\n                \"_CURECI2\", # e-cig\r\n                #demographics\r\n                # age, income cat, employ, gender, marital, education, home (rent/own)\r\n                \"_AGEG5YR\", \"INCOME3\", \"EMPLOY1\", \"SEXVAR\", \"MARITAL\", \"_EDUCAG\", \"RENTHOM1\",\r\n                # number children, age numeric, race\r\n                \"_CHLDCNT\", \"_AGE80\", \"_RACEPR1\",\r\n                #self assessment\r\n                \"GENHLTH\", \"PRIMINSR\", \"MENTHLTH\", \"BLIND\", \"DECIDE\", \"_HLTHPLN\", \"WTKG3\", \"HTM4\", \r\n                \"DIFFWALK\", \"DIFFDRES\", \"DEAF\", \"PHYSHLTH\",\r\n                #habits\r\n                \"SLEPTIM1\", \"_TOTINDA\", \"EXERANY2\",  \"_DRNKWK2\", \"DRNKANY6\", \r\n                #medical \r\n                \"CHECKUP1\", \"FLUSHOT7\", \"CVDCRHD4\", \"CHCKDNY2\", \"ADDEPEV3\",\r\n                \"_DRDXAR2\", \"ASTHMA3\", \"_DENVST3\") %>% \r\n  janitor::clean_names() %>% \r\n  mutate(diabete4 = as.factor(case_when(diabete4 == 1 ~ \"yes\",\r\n                              diabete4 == 2 ~ \"no\",\r\n                              diabete4 == 3 ~ \"no\",\r\n                              diabete4 == 4 ~ \"no\")\r\n                              ),\r\n         bmi5cat = factor(bmi5cat),\r\n         bmi5 = as.numeric(bmi5/100),\r\n         smoker3 = as.factor(case_when(smoker3 == 1 ~ \"smoker\",\r\n                                       smoker3 == 2 ~ \"smoker\",\r\n                                       smoker3 == 3 ~ \"former smoker\",\r\n                                       smoker3 == 4 ~ \"non-smoker\")\r\n                             ),\r\n         cvdstrk3 = as.factor(case_when(cvdstrk3 == 7 ~ NA_character_,\r\n                                        cvdstrk3 == 9 ~ NA_character_,\r\n                                        .default = as.factor(cvdstrk3)\r\n                                      )\r\n                            ),\r\n         cvdcrhd4 = as.factor(case_when(cvdcrhd4 == 7 ~ NA_character_,\r\n                                        cvdcrhd4 == 9 ~ NA_character_,\r\n                                        .default = as.factor(cvdcrhd4)\r\n                                      )\r\n                            ),\r\n         cureci2 = as.factor(case_when(cureci2 == 9 ~ NA_character_,\r\n                                       .default = as.factor(cureci2)\r\n                                      )\r\n                            ),\r\n         ageg5yr = case_when(ageg5yr == 14 ~ NA_character_,\r\n                                       .default = as.character(ageg5yr)\r\n                                       ),\r\n         ageg5yr = as.numeric(ageg5yr),\r\n                             \r\n         income3 = as.factor(case_when(income3 == 77 ~ NA_character_,\r\n                                       income3 == 99 ~ NA_character_,\r\n                                       .default = as.factor(income3)\r\n                                       )\r\n                             ),\r\n         employ1 = as.factor(case_when(employ1 == 9 ~ NA_character_,\r\n                                       .default = as.factor(employ1)\r\n                                       )\r\n                             ),\r\n         sexvar = as.factor(sexvar),\r\n         marital = as.factor(case_when(marital == 9 ~ NA_character_,\r\n                                       .default = as.factor(marital)\r\n                                       )),\r\n         educag = as.factor(case_when(educag == 9 ~ NA_character_,\r\n                                      .default = as.factor(educag)\r\n                                      )\r\n                            ),\r\n         renthom1 = as.factor(case_when(renthom1 == 7 ~ NA_character_,\r\n                                        renthom1 == 9 ~ NA_character_,\r\n                                        .default = as.factor(renthom1)\r\n                                      )\r\n                            ),\r\n         chldcnt = as.factor(case_when(chldcnt == 1 ~ \"0\",\r\n                                       chldcnt == 2 ~ \"1\",\r\n                                       chldcnt == 3 ~ \"2\",\r\n                                       chldcnt == 4 ~ \"3\",\r\n                                       chldcnt == 5 ~ \"4\",\r\n                                       chldcnt == 6 ~ \"5 or more\",\r\n                                       chldcnt == 9 ~ NA_character_)\r\n                            ),\r\n         age80 = as.numeric(age80),\r\n         racepr1 = as.factor(racepr1),\r\n         genhlth = as.factor(case_when(genhlth == 9 ~ NA_character_,\r\n                                       .default = as.factor(genhlth)\r\n                                       )\r\n                             ),\r\n         priminsr = as.factor(case_when(priminsr == 88 ~ \"11\", # no coverage\r\n                                        priminsr == 77 ~ NA_character_,\r\n                                        priminsr == 99 ~ NA_character_,\r\n                                        .default = as.factor(priminsr)\r\n                                        )\r\n                              ),\r\n         menthlth = as.numeric(ifelse(menthlth == 88, 0, menthlth)), #filter out 77 and 99 later\r\n         blind = as.factor(case_when(blind == 7 ~ NA_character_,\r\n                                     blind == 9 ~ NA_character_,\r\n                                     .default = as.factor(blind)\r\n                                     )\r\n                           ),\r\n         decide = as.factor(case_when(decide == 7 ~ NA_character_,\r\n                                     decide == 9 ~ NA_character_,\r\n                                     .default = as.factor(decide)\r\n                                     )\r\n                            ),\r\n         hlthpln = as.factor(case_when(hlthpln == 9 ~ NA_character_,\r\n                                       .default = as.factor(hlthpln)\r\n                                       )\r\n                             ),\r\n         wtkg3 = as.numeric(wtkg3 / 100),\r\n         htm4 = as.numeric(htm4 / 100),\r\n         diffwalk = as.factor(case_when(diffwalk == 7 ~ NA_character_,\r\n                                     diffwalk == 9 ~ NA_character_,\r\n                                     .default = as.factor(diffwalk)\r\n                                     )\r\n                            ),\r\n         diffdres = as.factor(case_when(diffdres == 7 ~ NA_character_,\r\n                                     diffdres == 9 ~ NA_character_,\r\n                                     .default = as.factor(diffdres)\r\n                                     )\r\n                            ),\r\n         deaf = as.factor(case_when(deaf == 7 ~ NA_character_,\r\n                                     deaf == 9 ~ NA_character_,\r\n                                     .default = as.factor(deaf)\r\n                                     )\r\n                            ),\r\n         physhlth = as.numeric(ifelse(physhlth == 88, 0, physhlth)), #filter out 77 and 99 later\r\n\r\n         sleptim1 = as.numeric(sleptim1), # filter out 77 and 99\r\n         totinda = as.factor(case_when(totinda == 9 ~ NA_character_,\r\n                                       .default = as.factor(totinda)\r\n                                       )\r\n                             ),\r\n         exerany2 = as.factor(case_when(exerany2 == 9 ~ NA_character_,\r\n                                       .default = as.factor(exerany2)\r\n                                       )\r\n                             ),\r\n         drnkwk2 = as.numeric(ifelse(drnkwk2 == 99900, NA_character_, drnkwk2)\r\n                              ),\r\n         drnkany6 = as.factor(case_when(drnkany6 == 7 ~ NA_character_,\r\n                                        drnkany6 == 9 ~ NA_character_,\r\n                                        .default = as.factor(drnkany6)\r\n                                     )\r\n                            ),\r\n         checkup1 = as.factor(case_when(checkup1 == 7 ~ NA_character_,\r\n                                        checkup1 == 8 ~ NA_character_,\r\n                                        checkup1 == 9 ~ NA_character_,\r\n                                       .default = as.factor(checkup1)\r\n                                       )\r\n                             ),\r\n         flushot7 = as.factor(case_when(flushot7 == 7 ~ NA_character_,\r\n                                        flushot7 == 9 ~ NA_character_,\r\n                                        .default = as.factor(flushot7)\r\n                                     )\r\n                            ),\r\n         chckdny2 = as.factor(case_when(chckdny2 == 7 ~ NA_character_,\r\n                                        chckdny2 == 9 ~ NA_character_,\r\n                                        .default = as.factor(chckdny2)\r\n                                     )\r\n                            ),\r\n         addepev3 = as.factor(case_when(addepev3 == 7 ~ NA_character_,\r\n                                        addepev3 == 9 ~ NA_character_,\r\n                                        .default = as.factor(addepev3)\r\n                                     )\r\n                            ),\r\n         drdxar2 = as.factor(drdxar2),\r\n         asthma3 = as.factor(case_when(asthma3 == 7 ~ NA_character_,\r\n                                        asthma3 == 9 ~ NA_character_,\r\n                                        .default = as.factor(asthma3)\r\n                                     )\r\n                            ),\r\n         denvst3 = as.factor(case_when(denvst3 == 9 ~ NA_character_,\r\n                                       .default = as.factor(denvst3)\r\n                                     )\r\n                            )\r\n         )\r\ndata <-\r\n  data %>% \r\n  filter (ageg5yr > 2 & age80 >=30 & menthlth < 77 & physhlth < 77 & sleptim1 < 77) %>%  # filter for age >-30 years definition of type 2 diabetes\r\n  mutate(ageg5yr = as.factor(ageg5yr)\r\n         ) %>% \r\n  na.omit()\r\n\r\nskim(data)\r\n\r\n#write_csv(data, \"diabetes_cleaned_data.csv\")\r\n\r\n\r\n\r\n\r\n# check correlation between numeric\r\ndata <- read_csv(\"diabetes_cleaned_data.csv\")\r\ndata %>% \r\n  select_if(is.numeric) %>% \r\n  as.matrix(.) %>% \r\n  rcorr() %>% \r\n  tidy() %>% \r\n  arrange(desc(abs(estimate)))\r\n\r\n  ┌───────────────────────────────────────────────────────┐\r\n  │ column1    column2     estimate        n      p.value │\r\n  ├───────────────────────────────────────────────────────┤\r\n  │ exerany2   totinda     1          243049     0        │\r\n  │ age80      ageg5yr     0.995      243049     0        │\r\n  │ wtkg3      bmi5        0.859      243049     0        │\r\n  │ bmi5       bmi5cat     0.826      243049     0        │\r\n  │ wtkg3      bmi5cat     0.738      243049     0        │\r\n  │ htm4       sexvar     -0.698      243049     0        │\r\n  │ age80      employ1     0.611      243049     0        │\r\n  │ employ1    ageg5yr     0.61       243049     0        │\r\n  │ hlthpln    priminsr    0.601      243049     0        │\r\n  │ physhlth   genhlth     0.499      243049     0        │\r\n  │ htm4       wtkg3       0.48       243049     0        │\r\n  │ physhlth   diffwalk   -0.44       243049     0        │\r\n  │ educag     income3     0.433      243049     0        │\r\n  │ addepev3   menthlth   -0.42       243049     0        │\r\n  │ diffwalk   genhlth    -0.418      243049     0        │\r\n  │ diffdres   diffwalk    0.388      243049     0        │\r\n  │ decide     menthlth   -0.379      243049     0        │\r\n  │ employ1    income3    -0.373      243049     0        │\r\n  │ wtkg3      sexvar     -0.355      243049     0        │\r\n  │ priminsr   income3    -0.35       243049     0        │\r\n  │ genhlth    income3    -0.344      243049     0        │\r\n  │ drdxar2    age80      -0.34       243049     0        │\r\n  │ drdxar2    ageg5yr    -0.338      243049     0        │\r\n  │ drnkany6   drnkwk2    -0.332      243049     0        │\r\n  │ physhlth   diffdres   -0.331      243049     0        │\r\n  │ addepev3   decide      0.33       243049     0        │\r\n  │ physhlth   menthlth    0.323      243049     0        │\r\n  │ renthom1   income3    -0.323      243049     0        │\r\n  │ diffwalk   employ1    -0.318      243049     0        │\r\n  │ marital    income3    -0.315      243049     0        │\r\n  │ drdxar2    diffwalk    0.311      243049     0        │\r\n  │ drdxar2    employ1    -0.306      243049     0        │\r\n  │ diffwalk   income3     0.305      243049     0        │\r\n  │ renthom1   marital     0.298      243049     0        │\r\n  │ totinda    genhlth     0.289      243049     0        │\r\n  │ exerany2   genhlth     0.289      243049     0        │\r\n  │ totinda    diffwalk   -0.287      243049     0        │\r\n  │ exerany2   diffwalk   -0.287      243049     0        │\r\n  │ drnkany6   income3    -0.281      243049     0        │\r\n  │ menthlth   genhlth     0.281      243049     0        │\r\n  │ denvst3    income3    -0.271      243049     0        │\r\n  │ diffdres   genhlth    -0.264      243049     0        │\r\n  │ drdxar2    genhlth    -0.264      243049     0        │\r\n  │ flushot7   age80      -0.264      243049     0        │\r\n  │ flushot7   ageg5yr    -0.263      243049     0        │\r\n  │ decide     genhlth    -0.261      243049     0        │\r\n  │ physhlth   decide     -0.257      243049     0        │\r\n  │ totinda    physhlth    0.253      243049     0        │\r\n  │ exerany2   physhlth    0.253      243049     0        │\r\n  │ checkup1   hlthpln     0.247      243049     0        │\r\n  │ genhlth    employ1     0.246      243049     0        │\r\n  │ genhlth    bmi5        0.244      243049     0        │\r\n  │ totinda    income3    -0.242      243049     0        │\r\n  │ exerany2   income3    -0.242      243049     0        │\r\n  │ physhlth   income3    -0.239      243049     0        │\r\n  │ diffwalk   decide      0.238      243049     0        │\r\n  │ drdxar2    physhlth   -0.235      243049     0        │\r\n  │ genhlth    educag     -0.235      243049     0        │\r\n  │ denvst3    educag     -0.232      243049     0        │\r\n  │ decide     income3     0.227      243049     0        │\r\n  │ checkup1   age80      -0.225      243049     0        │\r\n  │ checkup1   ageg5yr    -0.223      243049     0        │\r\n  │ physhlth   employ1     0.219      243049     0        │\r\n  │ diffwalk   ageg5yr    -0.219      243049     0        │\r\n  │ diffwalk   age80      -0.218      243049     0        │\r\n  │ deaf       ageg5yr    -0.216      243049     0        │\r\n  │ deaf       age80      -0.214      243049     0        │\r\n  │ addepev3   genhlth    -0.214      243049     0        │\r\n  │ flushot7   checkup1    0.214      243049     0        │\r\n  │ totinda    educag     -0.213      243049     0        │\r\n  │ exerany2   educag     -0.213      243049     0        │\r\n  │ diffdres   decide      0.212      243049     0        │\r\n  │ priminsr   educag     -0.211      243049     0        │\r\n  │ genhlth    cvdcrhd4   -0.207      243049     0        │\r\n  │ addepev3   physhlth   -0.207      243049     0        │\r\n  │ genhlth    bmi5cat     0.204      243049     0        │\r\n  │ ageg5yr    cvdcrhd4   -0.2        243049     0        │\r\n  │ age80      cvdcrhd4   -0.199      243049     0        │\r\n  │ age80      renthom1   -0.194      243049     0        │\r\n  │ drnkany6   genhlth     0.192      243049     0        │\r\n  │ diffwalk   blind       0.191      243049     0        │\r\n  │ renthom1   ageg5yr    -0.191      243049     0        │\r\n  │ htm4       income3     0.191      243049     0        │\r\n  │ drnkany6   educag     -0.187      243049     0        │\r\n  │ wtkg3      genhlth     0.185      243049     0        │\r\n  │ priminsr   employ1     0.185      243049     0        │\r\n  │ diffwalk   bmi5       -0.185      243049     0        │\r\n  │ employ1    cvdcrhd4   -0.184      243049     0        │\r\n  │ priminsr   renthom1    0.184      243049     0        │\r\n  │ diffdres   income3     0.183      243049     0        │\r\n  │ chckdny2   genhlth    -0.182      243049     0        │\r\n  │ diffwalk   menthlth   -0.181      243049     0        │\r\n  │ denvst3    genhlth     0.18       243049     0        │\r\n  │ deaf       employ1    -0.18       243049     0        │\r\n  │ racepr1    age80      -0.179      243049     0        │\r\n  │ drdxar2    income3     0.179      243049     0        │\r\n  │ drnkany6   employ1     0.178      243049     0        │\r\n  │ blind      income3     0.178      243049     0        │\r\n  │ racepr1    ageg5yr    -0.178      243049     0        │\r\n  │ income3    ageg5yr    -0.177      243049     0        │\r\n  │ flushot7   employ1    -0.177      243049     0        │\r\n  │ denvst3    renthom1    0.177      243049     0        │\r\n  │ drnkany6   diffwalk   -0.176      243049     0        │\r\n  │ age80      income3    -0.176      243049     0        │\r\n  │ totinda    diffdres   -0.174      243049     0        │\r\n  │ exerany2   diffdres   -0.174      243049     0        │\r\n  │ diffdres   employ1    -0.174      243049     0        │\r\n  │ denvst3    priminsr    0.173      243049     0        │\r\n  │ diffdres   menthlth   -0.173      243049     0        │\r\n  │ renthom1   educag     -0.173      243049     0        │\r\n  │ menthlth   income3    -0.172      243049     0        │\r\n  │ diffwalk   educag      0.172      243049     0        │\r\n  │ blind      genhlth    -0.171      243049     0        │\r\n  │ checkup1   employ1    -0.169      243049     0        │\r\n  │ racepr1    renthom1    0.168      243049     0        │\r\n  │ diffwalk   cvdcrhd4    0.168      243049     0        │\r\n  │ denvst3    checkup1    0.167      243049     0        │\r\n  │ decide     blind       0.165      243049     0        │\r\n  │ diffwalk   cvdstrk3    0.164      243049     0        │\r\n  │ totinda    bmi5        0.164      243049     0        │\r\n  │ exerany2   bmi5        0.164      243049     0        │\r\n  │ drnkany6   totinda     0.161      243049     0        │\r\n  │ drnkany6   exerany2    0.161      243049     0        │\r\n  │ genhlth    cvdstrk3   -0.161      243049     0        │\r\n  │ addepev3   diffwalk    0.16       243049     0        │\r\n  │ racepr1    income3    -0.159      243049     0        │\r\n  │ menthlth   age80      -0.158      243049     0        │\r\n  │ menthlth   ageg5yr    -0.157      243049     0        │\r\n  │ chckdny2   diffwalk    0.156      243049     0        │\r\n  │ deaf       diffwalk    0.156      243049     0        │\r\n  │ sleptim1   ageg5yr     0.156      243049     0        │\r\n  │ sleptim1   age80       0.155      243049     0        │\r\n  │ age80      cureci2    -0.154      243049     0        │\r\n  │ denvst3    flushot7    0.153      243049     0        │\r\n  │ menthlth   renthom1    0.153      243049     0        │\r\n  │ ageg5yr    cureci2    -0.153      243049     0        │\r\n  │ physhlth   cvdcrhd4   -0.152      243049     0        │\r\n  │ cvdcrhd4   cvdstrk3    0.151      243049     0        │\r\n  │ flushot7   educag     -0.151      243049     0        │\r\n  │ physhlth   blind      -0.15       243049     0        │\r\n  │ employ1    cvdstrk3   -0.15       243049     0        │\r\n  │ hlthpln    age80      -0.149      243049     0        │\r\n  │ hlthpln    ageg5yr    -0.149      243049     0        │\r\n  │ asthma3    addepev3    0.148      243049     0        │\r\n  │ drdxar2    checkup1    0.148      243049     0        │\r\n  │ addepev3   sexvar     -0.147      243049     0        │\r\n  │ flushot7   hlthpln     0.147      243049     0        │\r\n  │ chckdny2   employ1    -0.147      243049     0        │\r\n  │ denvst3    hlthpln     0.146      243049     0        │\r\n  │ priminsr   marital     0.145      243049     0        │\r\n  │ chckdny2   cvdcrhd4    0.145      243049     0        │\r\n  │ denvst3    totinda     0.145      243049     0        │\r\n  │ denvst3    exerany2    0.145      243049     0        │\r\n  │ genhlth    renthom1    0.145      243049     0        │\r\n  │ drdxar2    diffdres    0.144      243049     0        │\r\n  │ diffdres   blind       0.144      243049     0        │\r\n  │ asthma3    genhlth    -0.143      243049     0        │\r\n  │ chckdny2   physhlth   -0.143      243049     0        │\r\n  │ drnkany6   physhlth    0.143      243049     0        │\r\n  │ decide     renthom1   -0.142      243049     0        │\r\n  │ addepev3   income3     0.14       243049     0        │\r\n  │ hlthpln    educag     -0.139      243049     0        │\r\n  │ educag     employ1    -0.139      243049     0        │\r\n  │ drnkwk2    sexvar     -0.138      243049     0        │\r\n  │ drnkany6   ageg5yr     0.137      243049     0        │\r\n  │ drdxar2    cvdcrhd4    0.137      243049     0        │\r\n  │ drnkany6   age80       0.137      243049     0        │\r\n  │ drdxar2    deaf        0.136      243049     0        │\r\n  │ totinda    employ1     0.135      243049     0        │\r\n  │ exerany2   employ1     0.135      243049     0        │\r\n  │ drdxar2    addepev3    0.133      243049     0        │\r\n  │ htm4       racepr1    -0.133      243049     0        │\r\n  │ addepev3   diffdres    0.133      243049     0        │\r\n  │ hlthpln    racepr1     0.133      243049     0        │\r\n  │ priminsr   genhlth     0.132      243049     0        │\r\n  │ sleptim1   menthlth   -0.131      243049     0        │\r\n  │ denvst3    diffwalk   -0.131      243049     0        │\r\n  │ chckdny2   ageg5yr    -0.131      243049     0        │\r\n  │ chckdny2   age80      -0.131      243049     0        │\r\n  │ diffwalk   bmi5cat    -0.13       243049     0        │\r\n  │ decide     educag      0.13       243049     0        │\r\n  │ hlthpln    renthom1    0.129      243049     0        │\r\n  │ denvst3    marital     0.129      243049     0        │\r\n  │ totinda    menthlth    0.128      243049     0        │\r\n  │ exerany2   menthlth    0.128      243049     0        │\r\n  │ physhlth   cvdstrk3   -0.128      243049     0        │\r\n  │ asthma3    physhlth   -0.128      243049     0        │\r\n  │ deaf       genhlth    -0.128      243049     0        │\r\n  │ totinda    bmi5cat     0.127      243049     0        │\r\n  │ exerany2   bmi5cat     0.127      243049     0        │\r\n  │ ageg5yr    cvdstrk3   -0.127      243049     0        │\r\n  │ drdxar2    decide      0.127      243049     0        │\r\n  │ age80      cvdstrk3   -0.126      243049     0        │\r\n  │ physhlth   educag     -0.126      243049     0        │\r\n  │ hlthpln    income3    -0.126      243049     0        │\r\n  │ racepr1    educag     -0.126      243049     0        │\r\n  │ drdxar2    totinda    -0.125      243049     0        │\r\n  │ drdxar2    exerany2   -0.125      243049     0        │\r\n  │ wtkg3      ageg5yr    -0.125      243049     0        │\r\n  │ genhlth    age80       0.124      243049     0        │\r\n  │ genhlth    ageg5yr     0.124      243049     0        │\r\n  │ wtkg3      age80      -0.123      243049     0        │\r\n  │ addepev3   renthom1   -0.122      243049     0        │\r\n  │ htm4       employ1    -0.122      243049     0        │\r\n  │ blind      employ1    -0.122      243049     0        │\r\n  │ totinda    decide     -0.122      243049     0        │\r\n  │ exerany2   decide     -0.122      243049     0        │\r\n  │ priminsr   racepr1     0.122      243049     0        │\r\n  │ drnkwk2    htm4        0.121      243049     0        │\r\n  │ drnkany6   htm4       -0.121      243049     0        │\r\n  │ drdxar2    chckdny2    0.121      243049     0        │\r\n  │ deaf       blind       0.121      243049     0        │\r\n  │ drdxar2    flushot7    0.121      243049     0        │\r\n  │ decide     employ1    -0.121      243049     0        │\r\n  │ asthma3    menthlth   -0.121      243049     0        │\r\n  │ income3    cvdstrk3    0.121      243049     0        │\r\n  │ drdxar2    bmi5       -0.119      243049     0        │\r\n  │ diffwalk   wtkg3      -0.119      243049     0        │\r\n  │ sexvar     income3    -0.118      243049     0        │\r\n  │ physhlth   bmi5        0.118      243049     0        │\r\n  │ decide     priminsr   -0.118      243049     0        │\r\n  │ denvst3    menthlth    0.115      243049     0        │\r\n  │ menthlth   marital     0.115      243049     0        │\r\n  │ denvst3    physhlth    0.114      243049     0        │\r\n  │ sleptim1   employ1     0.114      243049     0        │\r\n  │ checkup1   priminsr    0.114      243049     0        │\r\n  │ age80      marital    -0.114      243049     0        │\r\n  │ asthma3    diffwalk    0.112      243049     0        │\r\n  │ addepev3   bmi5       -0.112      243049     0        │\r\n  │ marital    ageg5yr    -0.11       243049     0        │\r\n  │ asthma3    bmi5       -0.109      243049     0        │\r\n  │ denvst3    decide     -0.109      243049     0        │\r\n  │ deaf       cvdcrhd4    0.108      243049     0        │\r\n  │ addepev3   ageg5yr     0.107      243049     0        │\r\n  │ blind      educag      0.107      243049     0        │\r\n  │ deaf       decide      0.107      243049     0        │\r\n  │ menthlth   cureci2     0.106      243049     0        │\r\n  │ addepev3   age80       0.106      243049     0        │\r\n  │ blind      menthlth   -0.106      243049     0        │\r\n  │ flushot7   renthom1    0.106      243049     0        │\r\n  │ denvst3    drnkany6    0.106      243049     0        │\r\n  │ asthma3    decide      0.105      243049     0        │\r\n  │ drdxar2    bmi5cat    -0.105      243049     0        │\r\n  │ physhlth   renthom1    0.105      243049     0        │\r\n  │ diffwalk   priminsr   -0.105      243049     0        │\r\n  │ diffdres   cvdstrk3    0.104      243049     0        │\r\n  │ asthma3    drdxar2     0.104      243049     0        │\r\n  │ drnkany6   priminsr    0.104      243049     0        │\r\n  │ diffwalk   renthom1   -0.103      243049     0        │\r\n  │ deaf       income3     0.102      243049     0        │\r\n  │ totinda    ageg5yr     0.102      243049     0        │\r\n  │ exerany2   ageg5yr     0.102      243049     0        │\r\n  │ genhlth    marital     0.101      243049     0        │\r\n  │ totinda    age80       0.101      243049     0        │\r\n  │ exerany2   age80       0.101      243049     0        │\r\n  │ totinda    wtkg3       0.0998     243049     0        │\r\n  │ exerany2   wtkg3       0.0998     243049     0        │\r\n  │ hlthpln    marital     0.0995     243049     0        │\r\n  │ physhlth   deaf       -0.0994     243049     0        │\r\n  │ educag     marital    -0.0992     243049     0        │\r\n  │ menthlth   sexvar      0.0985     243049     0        │\r\n  │ drdxar2    drnkany6   -0.0982     243049     0        │\r\n  │ physhlth   priminsr    0.0982     243049     0        │\r\n  │ htm4       ageg5yr    -0.0969     243049     0        │\r\n  │ racepr1    marital     0.0968     243049     0        │\r\n  │ flushot7   priminsr    0.0964     243049     0        │\r\n  │ htm4       age80      -0.0962     243049     0        │\r\n  │ totinda    renthom1    0.096      243049     0        │\r\n  │ exerany2   renthom1    0.096      243049     0        │\r\n  │ diffdres   bmi5       -0.0957     243049     0        │\r\n  │ renthom1   cureci2     0.0953     243049     0        │\r\n  │ totinda    priminsr    0.0952     243049     0        │\r\n  │ exerany2   priminsr    0.0952     243049     0        │\r\n  │ addepev3   totinda    -0.095      243049     0        │\r\n  │ addepev3   exerany2   -0.095      243049     0        │\r\n  │ addepev3   htm4        0.0946     243049     0        │\r\n  │ addepev3   cureci2    -0.0939     243049     0        │\r\n  │ drnkany6   sexvar      0.0939     243049     0        │\r\n  │ blind      cvdstrk3    0.0938     243049     0        │\r\n  │ decide     marital    -0.0934     243049     0        │\r\n  │ menthlth   bmi5        0.0933     243049     0        │\r\n  │ educag     bmi5       -0.0933     243049     0        │\r\n  │ flushot7   racepr1     0.0932     243049     0        │\r\n  │ chckdny2   income3     0.0929     243049     0        │\r\n  │ income3    cvdcrhd4    0.0917     243049     0        │\r\n  │ totinda    blind      -0.0917     243049     0        │\r\n  │ exerany2   blind      -0.0917     243049     0        │\r\n  │ diffdres   educag      0.0916     243049     0        │\r\n  │ drdxar2    cvdstrk3    0.0909     243049     0        │\r\n  │ decide     cvdstrk3    0.0909     243049     0        │\r\n  │ drdxar2    blind       0.0908     243049     0        │\r\n  │ flushot7   income3    -0.0906     243049     0        │\r\n  │ drdxar2    menthlth   -0.0905     243049     0        │\r\n  │ drnkany6   diffdres   -0.09       243049     0        │\r\n  │ ageg5yr    bmi5       -0.09       243049     0        │\r\n  │ addepev3   marital    -0.0899     243049     0        │\r\n  │ asthma3    sexvar     -0.0893     243049     0        │\r\n  │ chckdny2   cvdstrk3    0.0892     243049     0        │\r\n  │ menthlth   priminsr    0.0888     243049     0        │\r\n  │ drdxar2    educag      0.0887     243049     0        │\r\n  │ diffdres   cvdcrhd4    0.0884     243049     0        │\r\n  │ age80      bmi5       -0.0877     243049     0        │\r\n  │ wtkg3      employ1    -0.087      243049     0        │\r\n  │ chckdny2   totinda    -0.0869     243049     0        │\r\n  │ chckdny2   exerany2   -0.0869     243049     0        │\r\n  │ checkup1   sexvar     -0.0869     243049     0        │\r\n  │ denvst3    diffdres   -0.0868     243049     0        │\r\n  │ checkup1   diffwalk    0.0867     243049     0        │\r\n  │ chckdny2   diffdres    0.0865     243049     0        │\r\n  │ educag     bmi5cat    -0.0865     243049     0        │\r\n  │ drdxar2    htm4        0.0859     243049     0        │\r\n  │ chckdny2   drnkany6   -0.0857     243049     0        │\r\n  │ drnkany6   renthom1    0.0851     243049     0        │\r\n  │ racepr1    employ1    -0.0848     243049     0        │\r\n  │ decide     cureci2    -0.0848     243049     0        │\r\n  │ diffdres   renthom1   -0.0845     243049     0        │\r\n  │ totinda    htm4       -0.0845     243049     0        │\r\n  │ exerany2   htm4       -0.0845     243049     0        │\r\n  │ denvst3    blind      -0.0843     243049     0        │\r\n  │ drdxar2    sexvar     -0.0839     243049     0        │\r\n  │ addepev3   blind       0.083      243049     0        │\r\n  │ denvst3    bmi5        0.0826     243049     0        │\r\n  │ deaf       sexvar      0.0822     243049     0        │\r\n  │ income3    bmi5       -0.082      243049     0        │\r\n  │ deaf       diffdres    0.0819     243049     0        │\r\n  │ diffwalk   htm4        0.0816     243049     0        │\r\n  │ totinda    cvdcrhd4   -0.0816     243049     0        │\r\n  │ exerany2   cvdcrhd4   -0.0816     243049     0        │\r\n  │ totinda    cvdstrk3   -0.0803     243049     0        │\r\n  │ exerany2   cvdstrk3   -0.0803     243049     0        │\r\n  │ blind      renthom1   -0.0802     243049     0        │\r\n  │ checkup1   genhlth    -0.0801     243049     0        │\r\n  │ drnkany6   decide     -0.0801     243049     0        │\r\n  │ asthma3    bmi5cat    -0.0796     243049     0        │\r\n  │ drdxar2    racepr1     0.0788     243049     0        │\r\n  │ physhlth   age80       0.078      243049     0        │\r\n  │ physhlth   wtkg3       0.0774     243049     0        │\r\n  │ physhlth   ageg5yr     0.0773     243049     0        │\r\n  │ menthlth   educag     -0.0771     243049     0        │\r\n  │ physhlth   bmi5cat     0.077      243049     0        │\r\n  │ drnkany6   bmi5        0.0769     243049     0        │\r\n  │ asthma3    diffdres    0.0768     243049     0        │\r\n  │ checkup1   marital     0.0768     243049     0        │\r\n  │ addepev3   bmi5cat    -0.0766     243049     0        │\r\n  │ checkup1   renthom1    0.0763     243049     0        │\r\n  │ hlthpln    employ1    -0.0763     243049     0        │\r\n  │ denvst3    wtkg3       0.0763     243049     0        │\r\n  │ checkup1   htm4        0.0762     243049     0        │\r\n  │ checkup1   cvdcrhd4    0.076      243049     0        │\r\n  │ drnkany6   blind      -0.0749     243049     0        │\r\n  │ checkup1   drnkwk2     0.0748     243049     0        │\r\n  │ htm4       educag      0.0748     243049     0        │\r\n  │ deaf       educag      0.0742     243049     0        │\r\n  │ flushot7   cureci2     0.0741     243049     0        │\r\n  │ chckdny2   blind       0.074      243049     0        │\r\n  │ sexvar     cvdcrhd4    0.074      243049     0        │\r\n  │ drnkany6   cvdstrk3   -0.0736     243049     0        │\r\n  │ flushot7   marital     0.0735     243049     0        │\r\n  │ drnkwk2    income3     0.0734     243049     0        │\r\n  │ asthma3    htm4        0.0732     243049     0        │\r\n  │ blind      priminsr   -0.0729     243049     0        │\r\n  │ blind      cvdcrhd4    0.0727     243049     0        │\r\n  │ drnkany6   cvdcrhd4   -0.0716     243049     0        │\r\n  │ asthma3    income3     0.0716     243049     0        │\r\n  │ deaf       cvdstrk3    0.0711     243049     0        │\r\n  │ diffdres   wtkg3      -0.071      243049     0        │\r\n  │ totinda    deaf       -0.0709     243049     0        │\r\n  │ exerany2   deaf       -0.0709     243049     0        │\r\n  │ flushot7   cvdcrhd4    0.0708     243049     0        │\r\n  │ drdxar2    hlthpln     0.0708     243049     0        │\r\n  │ flushot7   sleptim1   -0.0705     243049     0        │\r\n  │ marital    cureci2     0.0702     243049     0        │\r\n  │ diffdres   priminsr   -0.0699     243049     0        │\r\n  │ denvst3    racepr1     0.0695     243049     0        │\r\n  │ diffwalk   marital    -0.0695     243049     0        │\r\n  │ chckdny2   deaf        0.0693     243049     0        │\r\n  │ genhlth    racepr1     0.069      243049     0        │\r\n  │ denvst3    addepev3   -0.0683     243049     0        │\r\n  │ sleptim1   genhlth    -0.0682     243049     0        │\r\n  │ renthom1   bmi5        0.0672     243049     0        │\r\n  │ sexvar     employ1     0.0668     243049     0        │\r\n  │ denvst3    cureci2     0.0665     243049     0        │\r\n  │ blind      ageg5yr    -0.0665     243049     0        │\r\n  │ drnkwk2    ageg5yr    -0.0663     243049     0        │\r\n  │ sleptim1   decide      0.0661     243049     0        │\r\n  │ drnkwk2    age80      -0.0657     243049     0        │\r\n  │ blind      age80      -0.0655     243049     0        │\r\n  │ htm4       menthlth   -0.0654     243049     0        │\r\n  │ educag     cvdstrk3    0.0653     243049     0        │\r\n  │ drnkwk2    employ1    -0.0652     243049     0        │\r\n  │ drnkany6   racepr1     0.065      243049     0        │\r\n  │ totinda    marital     0.0635     243049     0        │\r\n  │ exerany2   marital     0.0635     243049     0        │\r\n  │ sleptim1   racepr1    -0.0634     243049     0        │\r\n  │ ageg5yr    bmi5cat    -0.0633     243049     0        │\r\n  │ employ1    cureci2    -0.0631     243049     0        │\r\n  │ decide     bmi5       -0.0629     243049     0        │\r\n  │ denvst3    bmi5cat     0.0628     243049     0        │\r\n  │ blind      racepr1    -0.0626     243049     0        │\r\n  │ sexvar     bmi5cat    -0.0625     243049     0        │\r\n  │ drdxar2    wtkg3      -0.0623     243049     0        │\r\n  │ diffwalk   sexvar     -0.0623     243049     0        │\r\n  │ asthma3    renthom1   -0.0618     243049     0        │\r\n  │ chckdny2   decide      0.0618     243049     0        │\r\n  │ checkup1   physhlth   -0.0618     243049     0        │\r\n  │ chckdny2   flushot7    0.0611     243049     0        │\r\n  │ decide     cvdcrhd4    0.0607     243049     0        │\r\n  │ age80      bmi5cat    -0.0607     243049     0        │\r\n  │ checkup1   cureci2     0.0606     243049     0        │\r\n  │ chckdny2   checkup1    0.0606     243049     0        │\r\n  │ denvst3    age80      -0.06       243049     0        │\r\n  │ drnkany6   marital     0.0599     243049     0        │\r\n  │ educag     cureci2    -0.0598     243049     0        │\r\n  │ htm4       genhlth    -0.0589     243049     0        │\r\n  │ sleptim1   renthom1   -0.0588     243049     0        │\r\n  │ denvst3    ageg5yr    -0.0587     243049     0        │\r\n  │ addepev3   priminsr   -0.0585     243049     0        │\r\n  │ htm4       marital    -0.0577     243049     0        │\r\n  │ sleptim1   bmi5       -0.0571     243049     0        │\r\n  │ physhlth   marital     0.0568     243049     0        │\r\n  │ addepev3   chckdny2    0.0562     243049     0        │\r\n  │ sleptim1   physhlth   -0.0561     243049     0        │\r\n  │ checkup1   sleptim1   -0.0561     243049     0        │\r\n  │ asthma3    wtkg3      -0.0561     243049     0        │\r\n  │ denvst3    sleptim1   -0.0559     243049     0        │\r\n  │ denvst3    sexvar     -0.0559     243049     0        │\r\n  │ sleptim1   wtkg3      -0.0557     243049     0        │\r\n  │ totinda    sexvar      0.0555     243049     0        │\r\n  │ exerany2   sexvar      0.0555     243049     0        │\r\n  │ menthlth   bmi5cat     0.0553     243049     0        │\r\n  │ totinda    racepr1     0.0548     243049     0        │\r\n  │ exerany2   racepr1     0.0548     243049     0        │\r\n  │ deaf       priminsr   -0.0544     243049     0        │\r\n  │ denvst3    cvdstrk3   -0.054      243049     0        │\r\n  │ diffdres   ageg5yr    -0.0537     243049     0        │\r\n  │ diffdres   age80      -0.0535     243049     0        │\r\n  │ blind      marital    -0.0535     243049     0        │\r\n  │ drnkwk2    cureci2     0.0535     243049     0        │\r\n  │ drnkany6   bmi5cat     0.0534     243049     0        │\r\n  │ checkup1   bmi5       -0.0533     243049     0        │\r\n  │ diffdres   bmi5cat    -0.0532     243049     0        │\r\n  │ checkup1   bmi5cat    -0.0528     243049     0        │\r\n  │ educag     ageg5yr    -0.0524     243049     0        │\r\n  │ drnkany6   deaf       -0.0522     243049     0        │\r\n  │ flushot7   menthlth    0.0522     243049     0        │\r\n  │ flushot7   sexvar     -0.052      243049     0        │\r\n  │ htm4       renthom1   -0.0517     243049     0        │\r\n  │ checkup1   cvdstrk3    0.0516     243049     0        │\r\n  │ age80      educag     -0.0515     243049     0        │\r\n  │ chckdny2   bmi5       -0.0513     243049     0        │\r\n  │ htm4       decide      0.0512     243049     0        │\r\n  │ asthma3    totinda    -0.0511     243049     0        │\r\n  │ asthma3    exerany2   -0.0511     243049     0        │\r\n  │ drdxar2    priminsr   -0.0508     243049     0        │\r\n  │ sleptim1   cureci2    -0.0506     243049     0        │\r\n  │ sleptim1   bmi5cat    -0.0504     243049     0        │\r\n  │ physhlth   htm4       -0.0503     243049     0        │\r\n  │ menthlth   cvdstrk3   -0.0499     243049     0        │\r\n  │ decide     age80       0.0496     243049     0        │\r\n  │ flushot7   deaf        0.0495     243049     0        │\r\n  │ asthma3    blind       0.049      243049     0        │\r\n  │ marital    sexvar      0.0488     243049     0        │\r\n  │ decide     ageg5yr     0.0487     243049     0        │\r\n  │ checkup1   deaf        0.0484     243049     0        │\r\n  │ addepev3   cvdstrk3    0.0482     243049     0        │\r\n  │ flushot7   htm4        0.0482     243049     0        │\r\n  │ addepev3   employ1    -0.048      243049     0        │\r\n  │ addepev3   wtkg3      -0.048      243049     0        │\r\n  │ wtkg3      menthlth    0.0477     243049     0        │\r\n  │ asthma3    ageg5yr     0.0476     243049     0        │\r\n  │ income3    bmi5cat    -0.0476     243049     0        │\r\n  │ asthma3    age80       0.0475     243049     0        │\r\n  │ educag     cvdcrhd4    0.0473     243049     0        │\r\n  │ asthma3    sleptim1    0.0472     243049     0        │\r\n  │ htm4       blind       0.0472     243049     0        │\r\n  │ diffdres   marital    -0.0471     243049     0        │\r\n  │ drnkwk2    diffwalk    0.0471     243049     0        │\r\n  │ addepev3   sleptim1    0.0467     243049     0        │\r\n  │ addepev3   drnkany6   -0.0467     243049     0        │\r\n  │ hlthpln    cureci2     0.0466     243049     0        │\r\n  │ priminsr   cureci2     0.0464     243049     0        │\r\n  │ hlthpln    menthlth    0.0462     243049     0        │\r\n  │ decide     sexvar     -0.0456     243049     0        │\r\n  │ drnkany6   menthlth    0.0456     243049     0        │\r\n  │ decide     racepr1    -0.0449     243049     0        │\r\n  │ denvst3    cvdcrhd4   -0.0447     243049     0        │\r\n  │ wtkg3      educag     -0.0443     243049     0        │\r\n  │ chckdny2   menthlth   -0.0442     243049     0        │\r\n  │ checkup1   educag     -0.0441     243049     0        │\r\n  │ flushot7   diffwalk    0.0441     243049     0        │\r\n  │ wtkg3      cvdcrhd4   -0.0438     243049     0        │\r\n  │ flushot7   drnkwk2     0.0431     243049     0        │\r\n  │ chckdny2   bmi5cat    -0.043      243049     0        │\r\n  │ physhlth   sexvar      0.0429     243049     0        │\r\n  │ priminsr   cvdstrk3   -0.0426     243049     0        │\r\n  │ drnkwk2    bmi5       -0.0412     243049     0        │\r\n  │ checkup1   drnkany6   -0.0406     243049     0        │\r\n  │ asthma3    chckdny2    0.0404     243049     0        │\r\n  │ income3    cureci2    -0.0404     243049     0        │\r\n  │ asthma3    cvdstrk3    0.04       243049     0        │\r\n  │ asthma3    drnkany6   -0.0397     243049     0        │\r\n  │ sleptim1   marital    -0.0388     243049     0        │\r\n  │ wtkg3      racepr1    -0.0388     243049     0        │\r\n  │ asthma3    marital    -0.0386     243049     0        │\r\n  │ cvdcrhd4   bmi5cat    -0.0384     243049     0        │\r\n  │ chckdny2   drnkwk2     0.0382     243049     0        │\r\n  │ asthma3    cvdcrhd4    0.0382     243049     0        │\r\n  │ decide     bmi5cat    -0.0382     243049     0        │\r\n  │ chckdny2   educag      0.038      243049     0        │\r\n  │ addepev3   checkup1    0.0375     243049     0        │\r\n  │ drnkwk2    genhlth    -0.0372     243049     0        │\r\n  │ flushot7   wtkg3       0.037      243049     0        │\r\n  │ employ1    bmi5cat    -0.0369     243049     0        │\r\n  │ renthom1   cvdstrk3   -0.0369     243049     0        │\r\n  │ menthlth   cvdcrhd4   -0.0367     243049     0        │\r\n  │ cvdcrhd4   bmi5       -0.0359     243049     0        │\r\n  │ checkup1   menthlth    0.0358     243049     0        │\r\n  │ flushot7   decide     -0.0358     243049     0        │\r\n  │ addepev3   cvdcrhd4    0.0355     243049     0        │\r\n  │ marital    bmi5        0.0353     243049     0        │\r\n  │ drnkwk2    hlthpln     0.0353     243049     0        │\r\n  │ addepev3   deaf        0.0348     243049     0        │\r\n  │ racepr1    cvdcrhd4    0.0346     243049     0        │\r\n  │ genhlth    cureci2     0.0345     243049     0        │\r\n  │ hlthpln    sexvar     -0.0342     243049     0        │\r\n  │ denvst3    chckdny2   -0.0339     243049     0        │\r\n  │ checkup1   diffdres    0.0335     243049     0        │\r\n  │ totinda    hlthpln     0.0333     243049     0        │\r\n  │ exerany2   hlthpln     0.0333     243049     0        │\r\n  │ checkup1   racepr1     0.0331     243049     0        │\r\n  │ htm4       priminsr   -0.0331     243049     0        │\r\n  │ deaf       racepr1     0.0329     243049     0        │\r\n  │ priminsr   cvdcrhd4   -0.0329     243049     0        │\r\n  │ denvst3    deaf       -0.0326     243049     0        │\r\n  │ renthom1   bmi5cat     0.0326     243049     0        │\r\n  │ priminsr   age80       0.0326     243049     0        │\r\n  │ hlthpln    cvdcrhd4    0.0325     243049     0        │\r\n  │ priminsr   ageg5yr     0.0325     243049     0        │\r\n  │ sleptim1   diffdres    0.0324     243049     0        │\r\n  │ denvst3    asthma3    -0.032      243049     0        │\r\n  │ drdxar2    drnkwk2     0.0315     243049     0        │\r\n  │ racepr1    bmi5        0.0315     243049     0        │\r\n  │ flushot7   totinda     0.0313     243049     0        │\r\n  │ flushot7   exerany2    0.0313     243049     0        │\r\n  │ hlthpln    decide     -0.0313     243049     0        │\r\n  │ flushot7   cvdstrk3    0.0312     243049     0        │\r\n  │ drnkwk2    bmi5cat    -0.0311     243049     0        │\r\n  │ drnkwk2    physhlth   -0.031      243049     0        │\r\n  │ denvst3    employ1     0.0308     243049     0        │\r\n  │ employ1    bmi5       -0.0302     243049     0        │\r\n  │ drnkwk2    totinda    -0.0301     243049     0        │\r\n  │ drnkwk2    exerany2   -0.0301     243049     0        │\r\n  │ racepr1    bmi5cat     0.0301     243049     0        │\r\n  │ asthma3    checkup1    0.03       243049     0        │\r\n  │ chckdny2   htm4        0.03       243049     0        │\r\n  │ chckdny2   wtkg3      -0.0298     243049     0        │\r\n  │ wtkg3      renthom1    0.0298     243049     0        │\r\n  │ deaf       menthlth   -0.0297     243049     0        │\r\n  │ diffdres   racepr1    -0.0287     243049     0        │\r\n  │ sleptim1   blind       0.0286     243049     0        │\r\n  │ menthlth   racepr1     0.0279     243049     0        │\r\n  │ asthma3    deaf        0.0279     243049     0        │\r\n  │ physhlth   cureci2     0.0279     243049     0        │\r\n  │ wtkg3      decide     -0.0277     243049     0        │\r\n  │ drnkwk2    menthlth    0.0276     243049     0        │\r\n  │ chckdny2   priminsr   -0.0274     243049     0        │\r\n  │ htm4       bmi5cat     0.0272     243049     0        │\r\n  │ sleptim1   hlthpln    -0.0271     243049     0        │\r\n  │ sleptim1   educag      0.0267     243049     0        │\r\n  │ drnkwk2    racepr1    -0.0261     243049     0        │\r\n  │ renthom1   employ1    -0.026      243049     0        │\r\n  │ asthma3    employ1    -0.0258     243049     0        │\r\n  │ blind      bmi5       -0.0258     243049     0        │\r\n  │ wtkg3      income3     0.0256     243049     0        │\r\n  │ deaf       hlthpln     0.0252     243049     0        │\r\n  │ priminsr   sexvar     -0.025      243049     0        │\r\n  │ addepev3   flushot7    0.0243     243049     0        │\r\n  │ drnkwk2    cvdcrhd4    0.0242     243049     0        │\r\n  │ htm4       cvdcrhd4   -0.024      243049     0        │\r\n  │ diffdres   htm4        0.0237     243049     0        │\r\n  │ denvst3    drnkwk2     0.0237     243049     0        │\r\n  │ chckdny2   hlthpln     0.0236     243049     0        │\r\n  │ asthma3    flushot7    0.0236     243049     0        │\r\n  │ deaf       htm4       -0.0235     243049     0        │\r\n  │ educag     sexvar      0.0232     243049     0        │\r\n  │ checkup1   totinda    -0.0232     243049     0        │\r\n  │ checkup1   exerany2   -0.0232     243049     0        │\r\n  │ drnkwk2    wtkg3       0.023      243049     0        │\r\n  │ drnkany6   hlthpln     0.0229     243049     0        │\r\n  │ asthma3    cureci2    -0.0219     243049     0        │\r\n  │ diffwalk   hlthpln     0.0217     243049     0        │\r\n  │ flushot7   genhlth    -0.0215     243049     0        │\r\n  │ hlthpln    genhlth     0.0212     243049     0        │\r\n  │ menthlth   employ1     0.0212     243049     0        │\r\n  │ htm4       bmi5       -0.0209     243049     0        │\r\n  │ denvst3    drdxar2    -0.0209     243049     0        │\r\n  │ drnkany6   cureci2    -0.0206     243049     0        │\r\n  │ addepev3   educag      0.0203     243049     0        │\r\n  │ sleptim1   diffwalk    0.02       243049     0        │\r\n  │ renthom1   sexvar      0.0198     243049     0        │\r\n  │ htm4       cvdstrk3    0.0197     243049     0        │\r\n  │ chckdny2   renthom1   -0.0196     243049     0        │\r\n  │ htm4       cureci2     0.0196     243049     0        │\r\n  │ blind      sexvar     -0.0192     243049     0        │\r\n  │ diffwalk   racepr1    -0.0189     243049     0        │\r\n  │ asthma3    drnkwk2     0.0188     243049     0        │\r\n  │ drnkwk2    cvdstrk3    0.0186     243049     0        │\r\n  │ sleptim1   deaf       -0.0184     243049     0        │\r\n  │ physhlth   racepr1     0.0181     243049     0        │\r\n  │ totinda    cureci2     0.018      243049     0        │\r\n  │ exerany2   cureci2     0.018      243049     0        │\r\n  │ cureci2    cvdcrhd4    0.0178     243049     0        │\r\n  │ marital    cvdstrk3   -0.0174     243049     0        │\r\n  │ hlthpln    blind      -0.0169     243049     0        │\r\n  │ checkup1   blind       0.0165     243049     4.44e-16 │\r\n  │ hlthpln    cvdstrk3    0.0165     243049     4.44e-16 │\r\n  │ asthma3    racepr1    -0.0164     243049     6.66e-16 │\r\n  │ diffdres   cureci2    -0.0163     243049     8.88e-16 │\r\n  │ deaf       wtkg3      -0.0163     243049     8.88e-16 │\r\n  │ age80      sexvar      0.0162     243049     1.33e-15 │\r\n  │ drdxar2    renthom1    0.0162     243049     1.33e-15 │\r\n  │ sexvar     ageg5yr     0.0161     243049     2.44e-15 │\r\n  │ drdxar2    cureci2     0.0157     243049     1.15e-14 │\r\n  │ flushot7   bmi5        0.0154     243049     3.02e-14 │\r\n  │ blind      cureci2    -0.0152     243049     7.48e-14 │\r\n  │ addepev3   racepr1     0.015      243049     1.51e-13 │\r\n  │ blind      bmi5cat    -0.0145     243049     7.6e-13  │\r\n  │ asthma3    priminsr   -0.014      243049     5.39e-12 │\r\n  │ chckdny2   cureci2     0.0139     243049     6.99e-12 │\r\n  │ sleptim1   cvdcrhd4   -0.0136     243049     1.86e-11 │\r\n  │ cvdstrk3   bmi5       -0.0136     243049     2.08e-11 │\r\n  │ drnkwk2    diffdres    0.0134     243049     3.85e-11 │\r\n  │ chckdny2   marital    -0.0133     243049     5.55e-11 │\r\n  │ asthma3    educag      0.013      243049     1.36e-10 │\r\n  │ deaf       bmi5cat    -0.0126     243049     5.65e-10 │\r\n  │ flushot7   bmi5cat     0.0125     243049     6.21e-10 │\r\n  │ sleptim1   cvdstrk3   -0.0124     243049     1.08e-09 │\r\n  │ asthma3    hlthpln     0.0123     243049     1.16e-09 │\r\n  │ cvdstrk3   bmi5cat    -0.0121     243049     2.7e-09  │\r\n  │ deaf       renthom1    0.012      243049     3.46e-09 │\r\n  │ racepr1    sexvar      0.0114     243049     1.75e-08 │\r\n  │ deaf       cureci2     0.0114     243049     2.1e-08  │\r\n  │ checkup1   wtkg3      -0.0112     243049     2.98e-08 │\r\n  │ sleptim1   sexvar      0.0108     243049     8.95e-08 │\r\n  │ sexvar     cureci2    -0.0107     243049     1.47e-07 │\r\n  │ deaf       marital     0.0106     243049     1.53e-07 │\r\n  │ wtkg3      cureci2     0.0106     243049     1.95e-07 │\r\n  │ marital    cvdcrhd4    0.0104     243049     2.68e-07 │\r\n  │ denvst3    htm4        0.0103     243049     3.46e-07 │\r\n  │ priminsr   bmi5        0.0102     243049     4.84e-07 │\r\n  │ racepr1    cureci2     0.0102     243049   5e-07      │\r\n  │                                                       │\r\n  │ sexvar     bmi5       -0.00998    243049     8.71e-07 │\r\n  │ genhlth    sexvar      0.00989    243049     1.08e-06 │\r\n  │ flushot7   drnkany6    0.00988    243049     1.11e-06 │\r\n  │ wtkg3      priminsr   -0.00984    243049     1.24e-06 │\r\n  │ diffdres   sexvar     -0.00974    243049     1.56e-06 │\r\n  │ racepr1    cvdstrk3    0.00958    243049     2.31e-06 │\r\n  │ sleptim1   htm4       -0.00956    243049     2.43e-06 │\r\n  │ chckdny2   sexvar     -0.00928    243049     4.76e-06 │\r\n  │ totinda    sleptim1   -0.00884    243049     1.3e-05  │\r\n  │ exerany2   sleptim1   -0.00884    243049     1.3e-05  │\r\n  │ flushot7   blind      -0.00877    243049     1.55e-05 │\r\n  │ sleptim1   priminsr   -0.00876    243049     1.56e-05 │\r\n  │ drdxar2    sleptim1    0.00838    243049     3.59e-05 │\r\n  │ drnkwk2    marital     0.00827    243049     4.58e-05 │\r\n  │ drdxar2    marital     0.00786    243049     0.000107 │\r\n  │ flushot7   physhlth   -0.00778    243049     0.000125 │\r\n  │ chckdny2   racepr1     0.00753    243049     0.000204 │\r\n  │ chckdny2   sleptim1   -0.00697    243049     0.000594 │\r\n  │ sexvar     cvdstrk3    0.00694    243049     0.000623 │\r\n  │ drnkwk2    priminsr    0.00692    243049     0.000651 │\r\n  │ drnkany6   wtkg3       0.00666    243049     0.00102  │\r\n  │ sleptim1   income3     0.00662    243049     0.0011   │\r\n  │ drnkwk2    renthom1   -0.00644    243049     0.0015   │\r\n  │ drnkwk2    educag      0.00577    243049     0.00443  │\r\n  │ marital    bmi5cat     0.0057     243049     0.00497  │\r\n  │ diffdres   hlthpln     0.00558    243049     0.00598  │\r\n  │ cureci2    cvdstrk3    0.0052     243049     0.0103   │\r\n  │ drnkwk2    blind       0.00506    243049     0.0126   │\r\n  │ addepev3   drnkwk2     0.00486    243049     0.0165   │\r\n  │ drnkany6   sleptim1   -0.00404    243049     0.0466   │\r\n  │ deaf       bmi5       -0.00397    243049     0.05     │\r\n  │ cureci2    bmi5cat    -0.00376    243049     0.0639   │\r\n  │ drnkwk2    sleptim1   -0.00373    243049     0.0657   │\r\n  │ htm4       hlthpln    -0.00354    243049     0.0806   │\r\n  │ priminsr   bmi5cat     0.0032     243049     0.115    │\r\n  │ physhlth   hlthpln    -0.00249    243049     0.219    │\r\n  │ wtkg3      hlthpln    -0.00207    243049     0.308    │\r\n  │ wtkg3      cvdstrk3   -0.00206    243049     0.309    │\r\n  │ hlthpln    bmi5        0.002      243049     0.325    │\r\n  │ addepev3   hlthpln     0.00197    243049     0.332    │\r\n  │ checkup1   income3     0.00189    243049     0.351    │\r\n  │ wtkg3      blind       0.00189    243049     0.351    │\r\n  │ marital    employ1     0.00154    243049     0.448    │\r\n  │ drnkwk2    decide     -0.0015     243049     0.459    │\r\n  │ flushot7   diffdres   -0.00143    243049     0.48     │\r\n  │ cureci2    bmi5        0.00127    243049     0.532    │\r\n  │ hlthpln    bmi5cat    -0.0012     243049     0.554    │\r\n  │ renthom1   cvdcrhd4    0.000892   243049     0.66     │\r\n  │ checkup1   decide      0.000616   243049     0.761    │\r\n  │ drnkwk2    deaf        0.000379   243049     0.852    │\r\n  │ diffwalk   cureci2    -0.000232   243049     0.909    │\r\n  │ wtkg3      marital    -0.000111   243049     0.956    │\r\n  └───────────────────────────────────────────────────────┘\r\nColumn names: column1, column2, estimate, n, p.value\r\n\r\n\r\n\r\n# split data\r\nset.seed(2024021401)\r\ndata_split <-\r\n  data %>% \r\n  dplyr::sample_frac(size = 0.05, replace = FALSE) %>% #use 10% of data due to lack of computing power\r\n  initial_split(strata = diabete4) # strata by diabete4\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = diabete4)\r\n\r\n\r\n\r\n\r\n# split data\r\nset.seed(2024021401)\r\ndata_split_big <-\r\n  data %>% \r\n  initial_split(strata = diabete4) # strata by diabete4\r\ndata_train_big <-\r\n  data_split_big %>% \r\n  training()\r\ndata_test_big <-\r\n  data_split_big %>% \r\n  testing()\r\ndata_fold_big <-\r\n  data_train_big %>% \r\n  vfold_cv(v = 10, strata = diabete4)\r\n\r\n\r\n\r\n\r\nbase_rec <-\r\n  recipes::recipe(formula = diabete4 ~.,\r\n                  data = data_train) %>% \r\n  step_zv(all_predictors())\r\n\r\ndummy_rec <-\r\n  base_rec %>% \r\n  step_dummy(all_nominal_predictors())\r\n\r\nnormal_rec <-\r\n  dummy_rec %>% \r\n  step_normalize(all_predictors())\r\n\r\n\r\nlog_rec <-\r\n  base_rec %>% \r\n  step_log(all_numeric_predictors())\r\n\r\n\r\n\r\n\r\n# random forest\r\nrf_spec <-\r\n  rand_forest(trees = 1000L) %>% \r\n  set_engine(\"ranger\",\r\n             importance = \"permutation\") %>% \r\n  set_mode(\"classification\")\r\n\r\nrf_spec_for_tuning <-\r\n  rf_spec %>% \r\n  set_args(mtry = tune(),\r\n           min_n = tune())\r\n\r\n# Classification Tree Model\r\nct_spec <- \r\n  decision_tree() %>%\r\n  set_engine(engine = 'rpart') %>%\r\n  set_mode('classification') \r\n\r\nct_spec_for_tuning <-\r\n  ct_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(), \r\n           cost_complexity = tune())\r\n\r\n# knn\r\nknn_spec <-\r\n  nearest_neighbor() %>% \r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"classification\")\r\n\r\nknn_spec_for_tuning <-\r\n  knn_spec %>% \r\n  set_args(neighbors = tune(),\r\n           weight_func = tune(),\r\n           dist_power = tune())\r\n\r\n# xgboost\r\nxgb_spec <-\r\n  boost_tree(trees = 1000L) %>% \r\n  set_engine(\"xgboost\") %>% \r\n  set_mode(\"classification\")\r\n\r\nxgb_spec_for_tuning <-\r\n  xgb_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(),\r\n           loss_reduction = tune(),\r\n           sample_size = tune(),\r\n           mtry = tune(),\r\n           learn_rate = tune())\r\n\r\n# # naive bayes\r\n\r\nnaive_spec <-\r\n  naive_Bayes() %>%\r\n  set_engine(\"naivebayes\",\r\n             usepoisson = TRUE) %>%\r\n  set_mode(\"classification\")\r\n\r\nnaive_spec_for_tuning <-\r\n  naive_spec %>% \r\n  set_args(smoothness = tune(),\r\n           Laplace = tune())\r\n\r\n# Logistic Regression Model\r\nlogistic_spec <- \r\n  logistic_reg() %>%\r\n  set_engine(engine = 'glm') %>%\r\n  set_mode('classification') \r\n\r\n# Lasso Logistic Regression Model\r\n\r\nlogistic_lasso_spec <-\r\n  logistic_reg(mixture = 1, penalty = 1) %>% \r\n  set_engine(engine = 'glmnet') %>%\r\n  set_mode('classification') \r\n\r\n\r\nlogistic_lasso_spec_for_tuning <- \r\n  logistic_lasso_spec %>% \r\n  set_args(penalty = tune()) #we could let penalty = tune()\r\n\r\n\r\n\r\n\r\nbase_set <- #works\r\n  workflow_set (\r\n    list(base_rec, dummy_rec, log_rec), #preprocessor\r\n    list(rf_spec, ct_spec,\r\n         rf_spec_for_tuning, ct_spec_for_tuning), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\ndummy_set <- #works\r\n  workflow_set (\r\n    list(dummy_rec),\r\n    list(knn_spec, xgb_spec, logistic_spec,\r\n         knn_spec_for_tuning, xgb_spec_for_tuning),\r\n    cross = TRUE)\r\n\r\nnormal_set <-\r\n  workflow_set(\r\n    list(normal_rec),\r\n    list(logistic_lasso_spec,\r\n         logistic_lasso_spec_for_tuning),\r\n    cross = TRUE)\r\n\r\nnaive_set <- #works\r\n  workflow_set(\r\n    list(base_rec, log_rec),\r\n    list(naive_spec,\r\n         naive_spec_for_tuning),\r\n    cross = TRUE)\r\n\r\nmodel_set <-\r\n  bind_rows(base_set, dummy_set, normal_set, naive_set)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-02-15T11:04:25+08:00",
    "input_file": {}
  },
  {
    "path": "ml/2024-02-10-hotel-cancellation/",
    "title": "Hotel reservation cancellations",
    "description": "In this next exercise, we try and predict the probability that a hotel's reservations will eventually be cancelled, given information we have on hand, such as ADR, customer segment, and deposit type...",
    "author": [],
    "date": "2024-02-10",
    "categories": [
      "classification",
      "logistic regression",
      "LASSO regression",
      "random forest",
      "decision tree",
      "naive bayes",
      "knn",
      "xgboost"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nImporting the data\r\nData Wrangling\r\nEDA\r\nWhat to do if I have no ML?\r\nCorrelation check\r\nSplitting the Data\r\nCreate recipes\r\nBuild Models\r\nTuning with racing using workflow_map\r\nFinalize workflow and last fit\r\nAssessing the workflow\r\nMaterial added 28 February 2024\r\nExplore PCA using step_pca\r\nA recipe for interactions\r\nAdded tune_sim_anneal 6 March 2024\r\n\r\n\r\n\r\nIntroduction\r\nI came across this dataset while reading Julia Silge’s blog. It’s a real treasure trove of information, especially for someone starting out in Machine Learning like me.\r\nDetailed information on this dataset is available from this scientific paper. In her blog, Julia used the data to build a model that predicted which hotel stays included children, and which did not.\r\nI decided to explore something different. One of the challenges every hotel faces is cancellations. Cancellations have a big impact on revenue management strategies, because it determines how aggressively a hotel could overbook it’s inventory of rooms.\r\n\r\n\r\nrm(list = ls())\r\nsessionInfo()\r\n# Set packages and dependencies\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\", \"finetune\", \"themis\", \"embed\",# for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\",#load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", \"kknn\", \"earth\", \"klaR\", \"discrim\", \"naivebayes\", \"baguette\", \"kernlab\",#random forest\r\n               \"janitor\", \"lubridate\")\r\nload(\"hotel_cancellation_data.RData\")\r\n\r\n\r\nImporting the data\r\nWe start by importing the data, and do a quick summary of it using skim. Once again, I am also importing hotel_cancellation_data.RData and setting eval = FALSE for many of the code chunks to reduce the computational resources and time required to render this page.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\r\nskim(df)\r\n\r\n\r\nData Wrangling\r\nI’m still a bit confused as to whether to handle the data wrangling “here”, or at the recipe stage. Recall that there are numerous step_* functions to perform similar tasks as dplyr. I will make a note to ask Prof at our class next week.\r\nFor now, I’ll use mutate and dmy() from lubridate to get the arrival date, as well as mutate is_canceled and other character predictors into factors.\r\n\r\n\r\n#lets get variables into correct class\r\ndata <-\r\n  df %>%\r\n  janitor::clean_names() %>%\r\n  mutate(arrival_date = dmy(paste0(arrival_date_day_of_month, \"-\", arrival_date_month, \"-\", arrival_date_year)),\r\n         #day_of_week = factor(wday(arrival_date, label = TRUE)),\r\n         is_canceled = factor(ifelse(is_canceled == 0, \"stayed\", \"cancelled\")),\r\n         is_repeated_guest = factor(ifelse(is_repeated_guest == 0, \"first-time\", \"repeat\")),\r\n         children = children + babies) %>% \r\n  dplyr::select(-babies) %>% \r\n  mutate_if(is.character, as.factor)\r\n\r\n\r\nEDA\r\nLet’s perform some basic EDA. First, let’s see what proportion of reservations are cancelled.\r\n\r\n\r\ndata %>% \r\n  dplyr::select(hotel, is_canceled) %>% \r\n  group_by(hotel) %>% \r\n  count(is_canceled) %>% \r\n  mutate(percentage = n / sum(n) *100) %>% \r\n  ggplot(aes(x = hotel,\r\n             y = percentage,\r\n             fill = is_canceled)\r\n         ) +\r\n  geom_col(position = \"dodge\") +\r\n  labs(fill = \"cancel/stay\") +\r\n  theme(legend.position = \"bottom\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nThe City Hotel has a higher proportion of cancellations compared to the Resort Hotel, 41.7 percent vs 27.8 percent.\r\nDo cancellations vary by market segment? Let’s investigate that.\r\n\r\n\r\ndata %>% \r\n  dplyr::select(hotel, market_segment, is_canceled) %>% \r\n  group_by(hotel, market_segment) %>% \r\n  count(is_canceled) %>% \r\n  mutate(percentage = n / length(data$hotel) *100) %>% \r\n  ggplot(aes(x = market_segment,\r\n             y = percentage,\r\n             fill = is_canceled)\r\n         ) +\r\n  geom_col(position = \"dodge\") +\r\n  facet_grid(hotel~.)+\r\n  labs(fill = \"cancel/stay\") +\r\n  theme(legend.position = \"bottom\",\r\n        axis.text.x = element_text(size = rel(0.5))) +\r\n  theme_bw()\r\n\r\n\r\n\r\nFinally, let’s look at the proportion of reservations that are cancelled vs not cancelled to check for class imbalance.\r\n\r\n\r\ndata %>% \r\n  count(is_canceled)\r\n\r\n\r\nDo we need to address class imbalance? Let’s use step_upsample in one recipe to address class imbalance, and another recipe without to compare its performance.\r\nWhat to do if I have no ML?\r\nAssuming I am the City Hotel operator with no access to ML (or no knowledge of machine learning), I’d pay more attention to market segments with the highest cancellation rates, which are Groups and Online TA. I could proactively send out reminders to reconfirm reservations as the date of stay approaches. I could also “tighten-up” cancellation policies by imposing penalties if you cancel within 14 days of the intended date of stay.\r\nThese are very common cancellation policies which we currently see in practice. A riskier approach would be to allow particular segments with higher cancellation rates to overbook rooms, but this would come at the risk of “walking the guest” should cancellations not materialize.\r\nI wonder if my rudimentary Machine Learning algorithm can do a better job?\r\nCorrelation check\r\nNone of the numeric predictors appear to be highly correlated with each other.\r\n\r\n\r\ndata %>%\r\n  select_if(is.numeric) %>%\r\n  as.matrix(.) %>%\r\n  rcorr() %>%\r\n  tidy() %>%\r\n  mutate(absCorr = abs(estimate)) %>%\r\n  arrange(desc(absCorr)) %>% \r\n  dplyr::select(-estimate, -n, - p.value) %>% \r\n  DT::datatable() %>% \r\n  formatRound(\"absCorr\", digits = 3)\r\n\r\n\r\n\r\nSplitting the Data\r\nAs Julie Silge would call it, here is where we decide how to spend our “data budget”. A word of caution here for those with incompetent hardware. The dataset is considered “large” with respect to the laptop I have, which is a “hand me down” gaming rig I inherited from my son, originally purchased in 2017.\r\nThis machine “cannot” handle model fitting and tuning for a dataset of this size. It would either hang, or the task would be incomplete even after 48 hours. Previously, while I contemplated investing in an upgraded computer, I did the next best thing, which was to work on a smaller sample of the original data (10 percent to be precise). I used sample_frac() to randomly sample 10 percent of the data, which was a reasonable size to work on.\r\nI always suspected that the above wasn’t the best solution. Hence, as my learning progressed, I learnt about implementing “racing methods” of tuning using the finetune package. I came back and updated this post by implementing racing.\r\n\r\n\r\nset.seed(2024012901)\r\ndata_split <-\r\n  data %>% \r\n  #dplyr::sample_frac(size = 0.1, replace = FALSE) %>% #use 10% of data due to lack of computing power\r\n  dplyr::select(hotel, is_canceled, arrival_date, \r\n                market_segment, distribution_channel, reserved_room_type,\r\n                deposit_type, customer_type, lead_time, is_repeated_guest,\r\n                previous_cancellations, previous_bookings_not_canceled, booking_changes,\r\n                days_in_waiting_list, adr) %>% \r\n  filter(adr >=0, adr < 4000) %>%  #remove negative adr and outlier\r\n  initial_split(strata = is_canceled)\r\n\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\n\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = is_canceled)\r\n\r\n\r\nCreate recipes\r\nI created two recipes to accommodate the models which I explored.\r\n\r\n\r\nbase_rec <-\r\n  recipes::recipe(formula = is_canceled ~.,\r\n                  data = data_train) %>% \r\n  step_zv(all_predictors()) %>% # remove zero variance\r\n  # make new features with date\r\n  step_date(arrival_date, features = c(\"dow\", \"month\", \"year\"), role = \"predictors\") %>% \r\n  update_role(arrival_date, new_role = \"date\") %>% \r\n  step_YeoJohnson(all_numeric_predictors()) %>% \r\n  step_normalize(all_numeric_predictors()) %>% \r\n  step_dummy(all_nominal_predictors())\r\n\r\nupsample_rec <-\r\n    recipes::recipe(formula = is_canceled ~.,\r\n                  data = data_train) %>% \r\n  step_zv(all_predictors()) %>% # remove zero variance\r\n  step_upsample(is_canceled) %>% \r\n  # make new features with date\r\n  step_date(arrival_date, features = c(\"dow\", \"month\", \"year\"), role = \"predictors\") %>% \r\n  update_role(arrival_date, new_role = \"date\") %>% \r\n  step_YeoJohnson(all_numeric_predictors()) %>% \r\n  step_normalize(all_numeric_predictors()) %>% \r\n  step_dummy(all_nominal_predictors())\r\n\r\n\r\nBuild Models\r\nI explored 7 models for predicting whether a reservation would be canceled or not. This is a classification problem. The models were random forest (using ranger), classification decision tress (using rpart), k-nearest neighbor (using kknn), xgboost (using xgboost), naive bayes (using naivebayes), logistic regression (using glm) and logistic LASSO regression (using glmnet). I considered models using their preset default, as well as tuned hyper-parameters.\r\n** Revision on 26 Feb 2024, I decided to narrow down my models to only 4 models: random forest, xgboost, k-nearest neighbor, and logistic regression. I’ve also included a base null model to provide a baseline comparison.\r\n\r\n\r\n# random forest\r\nrf_spec <-\r\n  rand_forest() %>% \r\n  set_engine(\"ranger\",\r\n             importance = \"impurity\") %>% \r\n  set_mode(\"classification\")\r\n\r\nrf_spec_for_tuning <-\r\n  rf_spec %>% \r\n  set_args(trees = tune(),\r\n           mtry = tune(),\r\n           min_n = tune())\r\n\r\n# knn\r\nknn_spec <-\r\n  nearest_neighbor() %>% \r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"classification\")\r\n\r\nknn_spec_for_tuning <-\r\n  knn_spec %>% \r\n  set_args(neighbors = tune(),\r\n           weight_func = tune(),\r\n           dist_power = tune())\r\n\r\n# xgboost\r\nxgb_spec <-\r\n  boost_tree() %>% \r\n  set_engine(\"xgboost\") %>% \r\n  set_mode(\"classification\")\r\n\r\nxgb_spec_for_tuning <-\r\n  xgb_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(),\r\n           loss_reduction = tune(),\r\n           sample_size = tune(),\r\n           mtry = tune(),\r\n           learn_rate = tune())\r\n\r\n# Logistic Regression Model\r\nlogistic_spec <- \r\n  logistic_reg() %>%\r\n  set_engine(engine = 'glm') %>%\r\n  set_mode('classification') \r\n\r\nnull_spec <-\r\n  null_model() %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"parsnip\")\r\n\r\n\r\nI created 1 workflowset to accommodate the 2 recipes and 5 models. Its a good way to try-out multiple sets of recipe and model approaches together, and tune them at once. And, it works with tune_race_anova!\r\n\r\n\r\nbase_set <- \r\n  workflow_set (\r\n    list(basic = base_rec,\r\n         upsampling = upsample_rec), #preprocessor\r\n    list(rand_forest = rf_spec_for_tuning,\r\n         xgboost = xgb_spec_for_tuning,\r\n         knn = knn_spec_for_tuning,\r\n         logistic = logistic_spec,\r\n         null = null_spec), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\n\r\nTuning with racing using workflow_map\r\nThus far, working with workflow_set and workflow_map have made testing various recipe/workflow combinations quite easy. I fitted the workflow_set to the validation set data_fold and tuned hyper-parameters using tune_race_anova.\r\n\r\n\r\n# tune hyper parameters\r\nset.seed(2024020102)\r\ndoParallel::registerDoParallel(cl=15, cores = 30)\r\n\r\nracing_results <-\r\n  workflow_map(base_set,\r\n               fn = \"tune_race_anova\",\r\n               resamples = data_fold,\r\n               grid = c(3:10),\r\n               metrics = metric_set(roc_auc, f_meas, accuracy, mn_log_loss),\r\n               control = control_race(verbose = TRUE,\r\n                                      verbose_elim = TRUE,\r\n                                      allow_par = TRUE,\r\n                                      save_workflow = TRUE,\r\n                                      parallel_over = \"everything\"))\r\n\r\nsave(racing_results, file = \"racing_results.Rda\")\r\n\r\n\r\nIt took my laptop approximately 36 hours to complete the above tuning process - I had expected it to be faster. :(\r\nYou can visualize the results using an autoplot() as well as rank the best recipe/workflow combination using rank_results. We will use roc_auc as the metric to evaluate model performance.\r\n\r\n\r\nautoplot(racing_results) + theme_bw() + theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\n\r\n# rank results\r\nracing_results %>% \r\n  workflowsets::rank_results(rank_metric = \"roc_auc\") %>% \r\n  filter(.metric == \"roc_auc\") %>% \r\n  dplyr::select(wflow_id, mean, std_err, rank) %>% \r\n  datatable() %>% \r\n  formatRound(columns = c(\"mean\", \"std_err\"),\r\n              digits = 3)\r\n\r\n\r\n\r\nThe top performing model is the random forest model, using the basic recipe. Let’s extract the “winning” workflow and see what tuned parameters were selected. Upsampling did not provide better perfoemance.\r\n\r\n\r\nrf_tuned_parameters <-\r\n  racing_results %>% \r\n  extract_workflow_set_result(id = \"basic_rand_forest\") %>% \r\n  select_best(metric = \"roc_auc\")\r\n\r\n\r\nFinalize workflow and last fit\r\nWith the tuned parameters, we can “apply” it to be best performing workflow and fit it to data_split using finalize_workflow and last_fit respectively.\r\n\r\n\r\n# finalize workflow\r\nbase_rf_wflow <-\r\n  workflow() %>% \r\n  add_recipe(base_rec) %>% \r\n  add_model(rf_spec_for_tuning) %>% \r\n  finalize_workflow(rf_tuned_parameters)\r\n\r\n# last fit\r\nbase_rf_final_fit <-\r\n  base_rf_wflow %>% \r\n  last_fit(data_split)\r\n\r\n\r\nAssessing the workflow\r\nWith the final fitted workflow, we can obtain metrics for model performance using collect_metrics.\r\n\r\n\r\nresults_base_rf <-\r\n  base_rf_final_fit %>% \r\n  collect_metrics() %>% \r\n  mutate(algo = \"Base Recipe Random Forest\")\r\n\r\n\r\nThe results are quite promising, with an roc_auc of 0.884.\r\nWe can also obtain a confusion matrix, which compares predictions vs truth for the whole dataset. This is a 2 by 2 matrix that compares stayed/cancelled reservations for truth vs prediction.\r\n\r\n\r\nbase_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  conf_mat(is_canceled, .pred_class)\r\n\r\n           Truth\r\nPrediction  cancelled stayed\r\n  cancelled      6795   1373\r\n  stayed         4261  17419\r\n\r\nThis matrix can also be visualized as a plot:\r\n\r\n\r\nbase_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  conf_mat(is_canceled, .pred_class) %>% \r\n  autoplot() +\r\n  theme_bw()\r\n\r\n\r\n\r\nWe can also use autoplot to visualize the roc_curve.\r\n\r\n\r\nbase_rf_final_fit %>% \r\n  collect_predictions %>% \r\n  roc_curve(is_canceled, .pred_cancelled) %>%\r\n  autoplot()\r\n\r\n\r\n\r\nAnd finally, we can use VIP to visualize which were the most important features in the model.\r\n\r\n\r\nbase_rf_final_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  vip(geom = \"point\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nThe top 3 features determining cancellations are non-refundable deposit, lead-time, and adr.\r\n\r\n\r\n#save.image(\"hotel_cancellation_data.RData\")\r\n\r\n\r\nThank you for reading this post to the end.\r\nMaterial added 28 February 2024\r\nI learnt about Principal Component Analysis (PCA) over the weekend, and I would like to see if implementing it as a recipe step would help improve our results. Let’ go!\r\nExplore PCA using step_pca\r\nWe start by making a new recipe pca_rec.\r\n\r\n\r\n# make a recipe for pca\r\npca_rec <-\r\n  recipes::recipe(formula = is_canceled ~.,\r\n                  data = data_train) %>% \r\n  step_zv(all_predictors()) %>% # remove zero variance\r\n  # make new features with date\r\n  step_date(arrival_date, features = c(\"dow\", \"month\", \"year\"), role = \"predictors\") %>% \r\n  update_role(arrival_date, new_role = \"date\") %>% \r\n  step_YeoJohnson(all_numeric_predictors()) %>% \r\n  step_dummy(all_nominal_predictors()) %>% \r\n  step_normalize(all_numeric_predictors()) %>% \r\n  step_pca(all_predictors())\r\n\r\n\r\nSince we are only evaluating 1 model and 1 recipe, I will only need to create a workflow.\r\n\r\n\r\npca_rf_wflow <- \r\n  workflow() %>% \r\n  add_recipe(pca_rec) %>% \r\n  add_model(rf_spec_for_tuning)\r\n\r\n\r\nLet’s tune it with racing.\r\n\r\n\r\nset.seed(2024022801)\r\ndoParallel::registerDoParallel(cl=15, cores = 30)\r\npca_rf_results <-\r\n  tune_race_anova(\r\n    pca_rf_wflow,\r\n    data_fold,\r\n    grid = c(4:10),\r\n    metrics = metric_set(roc_auc, f_meas, accuracy, mn_log_loss),\r\n    control = control_race(verbose = TRUE, \r\n                           verbose_elim = TRUE,\r\n                           allow_par = TRUE,\r\n                           save_pred = TRUE,\r\n                           parallel_over = \"everything\")\r\n  )\r\n\r\nsave(pca_rf_results, file = \"pca_rf_results.Rda\")\r\n\r\n\r\nOnce tuning is done, lets finalize the tuned hyper-parameters to the workflow and fit to data_split.\r\n\r\n\r\npca_rf_tuned_wflow <-\r\n  pca_rf_wflow %>% \r\n  finalize_workflow(pca_rf_results %>% select_best(metric = \"roc_auc\"))\r\n\r\npca_rf_final_fit <-\r\n  pca_rf_tuned_wflow %>% \r\n  last_fit(data_split)\r\n\r\nresults_pca_rf <-\r\n  pca_rf_final_fit %>% \r\n  collect_metrics() %>% \r\n  mutate(algo = \"PCA Recipe Random Forest\")\r\n\r\n\r\nUnfortunately, roc_auc did not improve using PCA. Shall we explore if there are any interactions between numeric variables? Let’s do it.\r\nA recipe for interactions\r\n\r\n\r\n# make a recipe for interactions\r\n\r\nbase_inter_rec <-\r\n  recipes::recipe(formula = is_canceled ~.,\r\n                  data = data_train) %>% \r\n  step_zv(all_predictors()) %>% # remove zero variance\r\n  # make new features with date\r\n  step_date(arrival_date, features = c(\"dow\", \"month\", \"year\"), role = \"predictors\") %>% \r\n  update_role(arrival_date, new_role = \"date\") %>% \r\n  step_YeoJohnson(all_numeric_predictors()) %>% \r\n  step_normalize(all_numeric_predictors()) %>% \r\n  step_interact(terms = ~ all_numeric_predictors():all_numeric_predictors(),\r\n                role = \"predictor\") %>% \r\n  step_dummy(all_nominal_predictors())\r\n\r\n#base_inter_rec %>% prep() %>% juice() %>% skim()\r\n  \r\nbase_inter_rf_wflow <- \r\n  workflow() %>% \r\n  add_recipe(base_inter_rec) %>% \r\n  add_model(rf_spec_for_tuning)\r\n\r\nset.seed(2024022802)\r\ndoParallel::registerDoParallel(cl=15, cores = 30)\r\nbase_inter_rf_results <-\r\n  tune_race_anova(base_inter_rf_wflow,\r\n                  data_fold,\r\n                  grid = c(4:10),\r\n                  metrics = metric_set(roc_auc, f_meas, accuracy, mn_log_loss), \r\n                  control = control_race(verbose = TRUE, \r\n                                         verbose_elim = TRUE,\r\n                                         allow_par = TRUE, \r\n                                         save_pred = TRUE,\r\n                                         parallel_over = \"everything\")\r\n  )\r\n\r\nsave(base_inter_rf_results, file = \"base_inter_rf_results.Rda\")\r\n\r\nbase_inter_rf_tuned_wflow <-\r\n  base_inter_rf_wflow %>% \r\n  finalize_workflow(base_inter_rf_results %>% select_best(metric = \"roc_auc\"))\r\n\r\nbase_inter_rf_final_fit <-\r\n  base_inter_rf_tuned_wflow %>% \r\n  last_fit(data_split)\r\n\r\nresults_interact_rf <-\r\n  base_inter_rf_final_fit %>% \r\n  collect_metrics() %>% \r\n  mutate(algo = \"Base Interact Random Forest\")\r\n\r\n\r\nThe performance was better than using the PCA Random Forest combination, but did not exceed using the simpler Base Random Forest.\r\nHere is a summary of the results. As my knowledge of machine learning improves, I might come back and try out new feature engineering techniques to improve model performance. For now, I shall conclude here.\r\n\r\n\r\nresults <-\r\n  bind_rows(results_base_rf, results_pca_rf, results_interact_rf) %>% \r\n  dplyr::select(.metric, .estimate, algo) %>% \r\n  group_by(.metric) %>% \r\n  arrange(desc(.estimate)) %>% \r\n  datatable() %>% \r\n  formatRound(columns = \".estimate\",\r\n              digits = 3,\r\n              )\r\n\r\nresults\r\n\r\n\r\n\r\nAdded tune_sim_anneal 6 March 2024\r\nLearnt about iterative tuning, so I decided to try it out. I wonder if it will result in better model performance?\r\n\r\n\r\n# base recipe and rf workflow\r\nbase_rf_wflow2 <-\r\n  workflow() %>% \r\n  add_model(rf_spec_for_tuning) %>% \r\n  add_recipe(base_rec)\r\n\r\n# set up new search grid\r\nrf_grid <-\r\n  extract_parameter_set_dials(rf_spec_for_tuning) %>% \r\n  update(trees = trees(c(100L,1500L)),\r\n         mtry = mtry(c(5L,100L)),\r\n         min_n = min_n(c(5L,100L)\r\n                       )\r\n         )\r\n\r\nrf_metrics <-\r\n  metric_set(roc_auc, f_meas, accuracy, mn_log_loss)\r\n\r\n# tune_sim_anneal\r\n\r\nset.seed(2024040301)\r\ndoParallel::registerDoParallel(cl=15, cores = 30)\r\n\r\nbase_rf_sim_anneal_result <-\r\n  tune_sim_anneal(\r\n    object = base_rf_wflow2,\r\n    resamples = data_fold,\r\n    iter = 25,\r\n    metrics = rf_metrics,\r\n    param_info = rf_grid,\r\n    initial = 1,\r\n    control = control_sim_anneal(verbose = TRUE,\r\n                                 verbose_iter = TRUE,\r\n                                 allow_par = TRUE,\r\n                                 parallel_over = \"everything\")\r\n  )\r\n\r\nsave(base_rf_sim_anneal_result, file = \"base_rf_sim_anneal_result.Rda\")\r\n\r\n\r\nbase_rf_sim_anneal_tuned_wflow <-\r\n  base_rf_wflow2 %>% \r\n  finalize_workflow(base_rf_sim_anneal_result %>% select_best(metric = \"roc_auc\"))\r\n\r\nbase_rf_sim_anneal_final_fit <-\r\n  base_rf_sim_anneal_tuned_wflow %>% \r\n  last_fit(data_split)\r\n\r\nresults_base_rf_sim_anneal <-\r\n  base_rf_sim_anneal_final_fit %>% \r\n  collect_metrics() %>% \r\n  mutate(algo = \"Base RF Sim Anneal\")\r\n\r\nresults2 <-\r\n  bind_rows(results_base_rf, results_pca_rf, results_interact_rf, results_base_rf_sim_anneal) %>% \r\n  dplyr::select(.metric, .estimate, algo) %>% \r\n  group_by(.metric) %>% \r\n  arrange(desc(.estimate)) %>% \r\n  datatable() %>% \r\n  formatRound(columns = \".estimate\",\r\n              digits = 3,\r\n              )\r\n\r\n\r\n\r\n\r\nresults2\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-02-10-hotel-cancellation/hotel-cancellation_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2024-03-06T22:15:29+08:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "ml/2024-02-09-hdb-rental-prices/",
    "title": "HDB Rental Prices",
    "description": "Let's try and predict the mediaa rental price per sqft of HDA flats given information about their location, age, distance to MRT station....",
    "author": [
      {
        "name": "Mark Y",
        "url": {}
      }
    ],
    "date": "2024-02-09",
    "categories": [
      "regression",
      "linear regression",
      "random forest",
      "knn",
      "xgboost"
    ],
    "contents": "\r\n\r\nContents\r\nPredicting HDB Rental Prices\r\nSet dependenies and load necessary packages\r\nImport the data\r\nCorrelation check\r\nEDA\r\nSplit the data\r\nLet’s make some recipes\r\nMake some models\r\nFirst fit and tuning\r\nVisualize results\r\nFinalize workflow and last fit\r\nCollect predictions\r\nVisualize important features\r\n\r\n\r\n\r\nPredicting HDB Rental Prices\r\nI started Module 1 of SMU Academy’s Predictive Analytics and Machine Learning class in January 2024. This data set was given in class for homework. I shall use it as practice to document what was learnt in class.\r\nAt this stage of my learning, the primary objective of this exercise is to familiarize myself with the complete workflow from start to finish. Repeatedly fine-tuning hyper-parameters, or complex feature engineering in order to come up with the BEST predictive model, is not an objective at this stage.\r\nSet dependenies and load necessary packages\r\nAs some of the code takes quite a bit of time to run, I’ve loaded the RData file to speed things up.\r\n\r\n\r\nrm(list = ls())\r\nsessionInfo()\r\n\r\n# Set packages and dependencies\r\npacman::p_load(\"tidyverse\", #for tidy data science practice\r\n               \"tidymodels\", \"workflows\",# for tidy machine learning\r\n               \"pacman\", #package manager\r\n               \"devtools\", #developer tools\r\n               \"Hmisc\", \"skimr\", \"broom\", \"modelr\",#for EDA\r\n               \"jtools\", \"huxtable\", \"interactions\", # for EDA\r\n               \"ggthemes\", \"ggstatsplot\", \"GGally\",\r\n               \"scales\", \"gridExtra\", \"patchwork\", \"ggalt\", \"vip\",\r\n               \"ggstance\", \"ggfortify\", # for ggplot\r\n               \"DT\", \"plotly\", #interactive Data Viz\r\n               # Lets install some ML related packages that will help tidymodels::\r\n               \"usemodels\", \"poissonreg\", \"agua\", \"sparklyr\", \"dials\", #load computational engines\r\n               \"doParallel\", # for parallel processing (speedy computation)\r\n               \"ranger\", \"xgboost\", \"glmnet\", #random forest\r\n               \"janitor\")\r\nload(\"rental_data.RData\")\r\n\r\n\r\nImport the data\r\nRead in the dataset rental_price.csv, and use skim() to provide a quick summary and overview of it.\r\n\r\n\r\n# read in data\r\ndf <- read_csv(\"rental_price.csv\")\r\nskim(df)\r\n\r\n\r\nProperty name and region (which are currently characters) will need to be reclassified as factors. The same goes for ref_year and district. unit_id will be assigned a role of “an id” later in the recipe.\r\n\r\n\r\n# make factors\r\ndata <-\r\n  df %>% \r\n  mutate(across(c(district, region, ref_year, property), as.factor))\r\n\r\n\r\nCorrelation check\r\nLet’s check for correlation among numeric variables.\r\n\r\n\r\n# check correlation between numeric, exclude loc_x, loc_y\r\n\r\ndata %>% \r\n  select (-unit_id, -loc_x, -loc_y) %>% \r\n  select_if(is.numeric) %>% \r\n  as.matrix(.) %>% \r\n  rcorr() %>% \r\n  tidy() %>% \r\n  arrange(desc(abs(estimate)))\r\n\r\n   ┌─────────────────────────────────────────────────────┐\r\n   │ column1      column2      estimate      n   p.value │\r\n   ├─────────────────────────────────────────────────────┤\r\n   │ price_medi   age           -0.479    3128    0      │\r\n   │ an                                                  │\r\n   │ price_medi   dist_to_mr    -0.203    3128    0      │\r\n   │ an           t                                      │\r\n   │ dist_to_mr   age            0.0331   3128    0.0644 │\r\n   │ t                                                   │\r\n   └─────────────────────────────────────────────────────┘\r\nColumn names: column1, column2, estimate, n, p.value\r\n\r\nCorrelation among numeric variables appear to be fine. None seem to be highly correlated with one another.\r\nEDA\r\nBefore we proceed with splitting the data, let’s do some basic EDA.\r\n\r\n\r\n# price by region\r\n\r\ndata %>% \r\n  ggplot(aes(x = district,\r\n              y = price_median)\r\n         ) + \r\n  geom_boxplot(outlier.shape = NA) +\r\n  geom_point(alpha = 0.15, aes(color = region))+\r\n  theme_bw() +\r\n  labs(x = \"District\",\r\n       y = \"Price Median $ psf\")+\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\nHere we see the influence of location (as specified by district and region) on price.\r\n\r\n\r\ndata %>% \r\n  ggplot(aes(x = dist_to_mrt,\r\n             y = price_median,\r\n             color = region)\r\n         ) +\r\n  geom_point(alpha = 0.2) +\r\n  geom_smooth(aes(color = region),\r\n              method = \"lm\")+\r\n  theme_bw()+\r\n  labs(x = \"Distance to MRT Station (km)\",\r\n       y = \"Price Median $ psf\") +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\nDistance to MRT appears to have a stronger influence on price in region_CCR, and a more muted influence on price in region_OCR and region_RCR.\r\nSplit the data\r\nLet’s proceed with splitting the data into a training and testing set. At the same time, let’s create folds for tuning/cross validation purposes.\r\n\r\n\r\n# split data\r\nset.seed(2024020101)\r\ndata_split <-\r\n  data %>% \r\n  initial_split(strata = region) # strata by region\r\ndata_train <-\r\n  data_split %>% \r\n  training()\r\ndata_test <-\r\n  data_split %>% \r\n  testing()\r\ndata_fold <-\r\n  data_train %>% \r\n  vfold_cv(v = 10, strata = region)\r\n\r\n\r\nLet’s make some recipes\r\nLet’s make a few recipes for modeling.\r\n\r\n\r\nbase_rec <-\r\n  recipes::recipe(formula = price_median ~ .,\r\n                 data = data_train) %>% \r\n  update_role(unit_id, new_role = \"unit_id\") %>% \r\n  update_role(property, new_role = \"name_id\") %>% \r\n  step_rm(loc_x, loc_y) %>%  # remove geo locator, property name\r\n  step_dummy(all_nominal_predictors())\r\n\r\nlog_rec <-\r\n  base_rec %>%\r\n  step_log(price_median, age, dist_to_mrt) # age, dist_mrt\r\n\r\n\r\nMake some models\r\nLet’s make a few models to predict price_median, which is the rental price per square foot for each property. We shall set up 4 models: a simple linear regression model, random forest, xgboost, and k-nearest neighbor model. I will set up a basic model, as well as a model for tuning of hyper-parameters.\r\n\r\n\r\n# linear regression\r\nlm_spec <-\r\n  linear_reg()\r\n\r\n# random forest\r\nrf_spec <-\r\n  rand_forest(trees = 1000L) %>% \r\n  set_engine(\"ranger\") %>% \r\n  set_mode(\"regression\")\r\n\r\nrf_spec_for_tuning <-\r\n  rf_spec %>% \r\n  set_args(mtry = tune(),\r\n           min_n = tune())\r\n\r\n# knn\r\nknn_spec <-\r\n  nearest_neighbor() %>% \r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"regression\")\r\n\r\nknn_spec_for_tuning <-\r\n  knn_spec %>% \r\n  set_args(neighbors = tune(),\r\n           weight_func = \"optimal\",\r\n           dist_power = tune())\r\n\r\n# xgboost\r\nxgb_spec <-\r\n  boost_tree(trees = 1000L) %>% \r\n  set_engine(\"xgboost\") %>% \r\n  set_mode(\"regression\")\r\n\r\nxgb_spec_for_tuning <-\r\n  xgb_spec %>% \r\n  set_args(tree_depth = tune(),\r\n           min_n = tune(),\r\n           loss_reduction = tune(),                     \r\n           sample_size = tune(),\r\n           mtry = tune(),        \r\n           learn_rate = tune())        \r\n\r\n\r\nLet’s combine everything into a workflow set. xgb needs a separate workflow set as it only works with dummy variables. Combine both using bind_rows into 1 model_set.\r\n\r\n\r\nbase_set <-\r\n  workflow_set (\r\n    preproc = list(base_rec, log_rec), #preprocessor\r\n    models = list(rf_spec, knn_spec, xgb_spec,\r\n                  rf_spec_for_tuning, knn_spec_for_tuning, xgb_spec_for_tuning), #model\r\n    cross = TRUE) #default is cross = TRUE\r\n\r\n\r\nFirst fit and tuning\r\nLet’s use fit_resamples to do a first fit as well as tune the hyper-parameters. There are still lots of things I don’t know. For example, what does “grid = 11” mean? Why not 5? Why not 25?\r\n\r\n\r\n# first fit\r\nset.seed(2024020102)\r\ndoParallel::registerDoParallel()\r\nbase_results <-\r\n  workflow_map(base_set,\r\n               fn = \"tune_grid\",\r\n               resamples = data_fold,\r\n               grid = 11,\r\n               verbose = TRUE)\r\n\r\n\r\nOnce your machine is done tuning for hyper-parameters, you can collect the results, by metrics. Here we used “rmse”. My machine is rather incompetent, so this process is taking a while. Fortunately, the dataset is rather small.\r\n\r\n\r\nbase_results %>% \r\n  collect_metrics() %>% \r\n  filter(.metric == \"rmse\") %>% \r\n  arrange(mean) %>% \r\n  print(n = 5)\r\n\r\n# A tibble: 72 × 9\r\n  wflow_id       .config preproc model .metric .estimator   mean     n\r\n  <chr>          <chr>   <chr>   <chr> <chr>   <chr>       <dbl> <int>\r\n1 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0559    10\r\n2 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0565    10\r\n3 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0598    10\r\n4 recipe_2_rand… Prepro… recipe  rand… rmse    standard   0.0670    10\r\n5 recipe_2_boos… Prepro… recipe  boos… rmse    standard   0.0684    10\r\n# ℹ 67 more rows\r\n# ℹ 1 more variable: std_err <dbl>\r\n\r\nVisualize results\r\nWe can also visualize the results using a basic autoplot(), or rank the results using workflowsets::rank_results().\r\n\r\n\r\nautoplot(base_results) +\r\n  theme_bw() +\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\n\r\n\r\n# rank results\r\nbase_results %>% \r\n  workflowsets::rank_results(rank_metric = \"rmse\") %>% \r\n  filter(.metric == \"rmse\") %>% \r\n  print(n = 5)\r\n\r\n# A tibble: 72 × 9\r\n  wflow_id     .config .metric   mean std_err     n preprocessor model\r\n  <chr>        <chr>   <chr>    <dbl>   <dbl> <int> <chr>        <chr>\r\n1 recipe_2_bo… Prepro… rmse    0.0559 1.15e-3    10 recipe       boos…\r\n2 recipe_2_bo… Prepro… rmse    0.0565 9.01e-4    10 recipe       boos…\r\n3 recipe_2_bo… Prepro… rmse    0.0598 1.56e-3    10 recipe       boos…\r\n4 recipe_2_ra… Prepro… rmse    0.0670 1.36e-3    10 recipe       rand…\r\n5 recipe_2_bo… Prepro… rmse    0.0684 1.81e-3    10 recipe       boos…\r\n# ℹ 67 more rows\r\n# ℹ 1 more variable: rank <int>\r\n\r\nThe best recipe-model combination is recipe 2, which is log_rec and parameters from boost_tree_3. If you recall, boost_tree_3 is the basic xgboost model with no hyper-parameters for tuning. Let’s extract the hyper-parameters using select_best.\r\n\r\n\r\ntune_param <-\r\n  base_results %>% \r\n  extract_workflow_set_result(\"recipe_2_boost_tree_3\") %>% \r\n  select_best(metric = \"rmse\")\r\n\r\n\r\nFinalize workflow and last fit\r\nLet’s apply the parameters and finalize the workflow for the best recipe/model combination.\r\n\r\n\r\nlog_rec_xbg_boost_wflow <-\r\n  workflow() %>% \r\n  add_recipe(log_rec) %>% \r\n  add_model(xgb_spec) %>% \r\n  finalize_workflow(tune_param)\r\n\r\n\r\nFinally, we can fit the model by performing last_fit to the split data, data_split.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit <-\r\n  log_rec_xbg_boost_wflow %>% \r\n  last_fit(data_split)\r\n\r\n\r\nCollect predictions\r\nWe can collect metrics as well as predictions from the last fitted model.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  collect_metrics()\r\n\r\n    ┌───────────────────────────────────────────────────┐\r\n    │ .metric   .estimator   .estimate   .config        │\r\n    ├───────────────────────────────────────────────────┤\r\n    │ rmse      standard         0.057   Preprocessor1_ │\r\n    │                                    Model1         │\r\n    │ rsq       standard         0.959   Preprocessor1_ │\r\n    │                                    Model1         │\r\n    └───────────────────────────────────────────────────┘\r\nColumn names: .metric, .estimator, .estimate, .config\r\n\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  collect_predictions() %>% \r\n  ggplot(aes(x = price_median,\r\n             y = .pred)) +\r\n  geom_point(alpha = 0.2)+\r\n  geom_abline(lty = 2,\r\n              color = \"dodgerblue\") +\r\n  labs(x = \"Actual Median Price $psf\",\r\n       y = \"Predicted Median Price $psf\")+\r\n  theme_bw()\r\n\r\n\r\n\r\nVisualize important features\r\nWe can use the vip() to see which were the most important features.\r\n\r\n\r\nlog_rec_xbg_boost_final_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  vip(geom = \"point\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nAs we can see, location location location, or in this case region is the most important predictor of price, followed by age and distance to MRT station.\r\n\r\n\r\n\r\n",
    "preview": "ml/2024-02-09-hdb-rental-prices/hdb-rental-prices_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2024-02-15T21:04:59+08:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  }
]
