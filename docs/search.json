{
  "articles": [
    {
      "path": "capstone_Import_data.html",
      "title": "Capstone Project: Does home ownership lead to greater optimism for one's future",
      "author": [],
      "contents": "\r\n\r\nContents\r\nImporting the Data\r\n\r\nImporting the Data\r\nThe following code details how we imported, extracted relevant variables, recoded these variables to suit our analysis, and combined them into one dataset named capstone_data.csv.\r\nThis code was separated from the main document as it is a process that only needs to be performed once.\r\n\r\n\r\n# Load necessary packages\r\nlibrary(tidyverse)\r\nlibrary(haven) # Package for reading Stata (DTA) format.\r\nlibrary(skimr)\r\nrm(list = ls())\r\n\r\n## Import individual datasets\r\n\r\n# Import Demographics variable: marital status, gender, age from b3a_cov.dta\r\n# Common link: pidlink\r\nb3a_cov <- read_dta(\"b3a_cov.dta\")\r\nb3a_cov_reduced <-\r\n  b3a_cov %>%\r\n  select(1, 7:9, 32)\r\n\r\n# Import Home Ownership Status kr03\r\n\r\nb2_kr <- read_dta(\"b2_kr.dta\")\r\nb2_kr_reduced <-\r\n  b2_kr %>%\r\n  select(1:2)\r\n \r\n# Import Highest Education Level (dl06) and Ethinic group (dl01f)\r\n\r\nb3a_dl1 <- read_dta(\"b3a_dl1.dta\")\r\nb3a_dl1_reduced <-\r\n  b3a_dl1 %>%\r\n  select(1, 4, 17, 47)\r\n\r\n# Import life satisfaction, current economic outlook, and outlook for future\r\n# sw00, sw01, sw03\r\n\r\nb3a_sw <- read_dta(\"b3a_sw.dta\")\r\nb3a_sw_reduced <-\r\n  b3a_sw %>%\r\n  select(1, 3:4, 6,19) #joined\r\n\r\n# Import employment tk01a\r\n\r\nb3a_tk1 <- read_dta(\"b3a_tk1.dta\")\r\nb3a_tk1_reduced <-\r\n  b3a_tk1 %>%\r\n  select(1, 3, 41) #joined\r\n\r\n\r\n# Import Religion tr11\r\n\r\nb3a_tr <- read_dta(\"b3a_tr.dta\")\r\nb3a_tr_reduced <-\r\n  b3a_tr %>%\r\n  select(1, 16, 40)\r\n\r\n# Import Physical Health Status kk01\r\n\r\nb3b_kk1 <- read_dta(\"b3b_kk1.dta\")\r\nb3b_kk1_reduced <-\r\n  b3b_kk1 %>%\r\n  select(1, 3, 13)\r\n\r\n\r\nHere we combine the separate datasets into 1. This was done in a step-wise process, at each step checking for integrity.\r\n\r\n\r\n# Combining data\r\n# We will mutate and rename variables later\r\n\r\n# # Data combined: age, maritial status, sex, ethnicity, education\r\n# # Ethnicity added 20 Dec\r\ndata_combined <-\r\n  full_join(b3a_cov_reduced, b3a_dl1_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data combined_1: age, maritial status, sex, ethnicity, education\r\n# # Add: sw00, sw01, sw03\r\ndata_combined_1 <-\r\n  full_join(data_combined,b3a_sw_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_2: age, maritial status, sex, education, sw00, sw01, sw03\r\n# # Add: employment tk01a\r\n\r\ndata_combined_2 <-\r\n  full_join(data_combined_1, b3a_tk1_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_3: age, maritial status, sex, education, sw00, sw01, sw03, employment tk01a\r\n# # Add religion tr11\r\n\r\ndata_combined_3 <-\r\n  full_join (data_combined_2, b3a_tr_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_4: age, maritial status, sex, education, sw00, sw01, sw03, employment tk01a, religion tr11, \r\n# # Add health kk01\r\n\r\ndata_combined_4 <-\r\n  full_join (data_combined_3, b3b_kk1_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_5: age, maritial status, sex, education, sw00, sw01, sw03, employment tk01a, religion tr11, health kk01\r\n# # Add: home ownership kr03\r\n\r\ndata_combined_5 <-\r\n  full_join(data_combined_4, b2_kr_reduced, by = \"hhid14_9\") # note joined by hhid14_9 (household level) instead of pidlink (individual level)\r\n\r\n# data_combined_5 is the final data set comprising:\r\n# age, maritial status, sex, education, sw00, sw01, sw03,\r\n# employment tk01a, religion tr11, health kk01, home ownership kr03\r\n# \r\nskim(data_combined_5)\r\n\r\n(#tab:Combining Dataset)Data summary\r\nName\r\ndata_combined_5\r\nNumber of rows\r\n36581\r\nNumber of columns\r\n14\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n3\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nhhid14_9\r\n0\r\n1.00\r\n9\r\n9\r\n0\r\n15350\r\n0\r\npidlink\r\n190\r\n0.99\r\n8\r\n9\r\n0\r\n36391\r\n0\r\ndl01f\r\n2117\r\n0.94\r\n0\r\n5\r\n2804\r\n85\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n196\r\n0.99\r\n38.50\r\n18.92\r\n14\r\n26\r\n35\r\n48\r\n998\r\n▇▁▁▁▁\r\nmarstat\r\n196\r\n0.99\r\n2.05\r\n0.94\r\n1\r\n2\r\n2\r\n2\r\n6\r\n▇▁▁▁▁\r\nsex\r\n196\r\n0.99\r\n2.03\r\n1.00\r\n1\r\n1\r\n3\r\n3\r\n3\r\n▇▁▁▁▇\r\ndl06\r\n3904\r\n0.89\r\n17.07\r\n25.52\r\n2\r\n2\r\n5\r\n6\r\n99\r\n▇▁▁▂▁\r\nsw00\r\n4926\r\n0.87\r\n2.68\r\n0.81\r\n1\r\n2\r\n3\r\n3\r\n9\r\n▆▇▁▁▁\r\nsw01\r\n4926\r\n0.87\r\n3.05\r\n1.01\r\n1\r\n3\r\n3\r\n4\r\n9\r\n▃▇▁▁▁\r\nsw03\r\n4926\r\n0.87\r\n4.36\r\n1.49\r\n1\r\n3\r\n4\r\n5\r\n9\r\n▂▇▅▂▁\r\ntk01a\r\n2148\r\n0.94\r\n1.69\r\n0.95\r\n1\r\n1\r\n1\r\n3\r\n8\r\n▇▅▁▁▁\r\ntr11\r\n4987\r\n0.86\r\n2.10\r\n0.72\r\n1\r\n2\r\n2\r\n2\r\n7\r\n▇▂▁▁▁\r\nkk01\r\n2310\r\n0.94\r\n2.05\r\n0.68\r\n1\r\n2\r\n2\r\n2\r\n4\r\n▂▇▁▃▁\r\nkr03\r\n441\r\n0.99\r\n1.71\r\n4.33\r\n1\r\n1\r\n1\r\n1\r\n95\r\n▇▁▁▁▁\r\n\r\nIn this next step we recoded the variables to suit our analysis.\r\n\r\n\r\n# This next section will focusing recoding the data. Renaming each variable will be performed in the main script.\r\n\r\n# Recoding the data ----\r\nnames(data_combined_5)\r\n\r\n [1] \"hhid14_9\" \"age\"      \"marstat\"  \"sex\"      \"pidlink\"  \"dl01f\"   \r\n [7] \"dl06\"     \"sw00\"     \"sw01\"     \"sw03\"     \"tk01a\"    \"tr11\"    \r\n[13] \"kk01\"     \"kr03\"    \r\n\r\n# Please see below for a brief description of each variable.\r\ndata_recoded <-\r\n  data_combined_5 %>%\r\n  mutate(sw00 = as.numeric(case_when(sw00 == 5 ~ \"1\", # Recode sw00. 1: Not at all satisfied\r\n                          sw00 == 4 ~ \"2\",\r\n                          sw00 == 3 ~ \"3\",\r\n                          sw00 == 2 ~ \"4\",\r\n                          sw00 == 1 ~ \"5\", #5 will be \"completely satisfied\"\r\n                          sw00 == 9 ~ NA_character_)\r\n                          ),\r\n         sw01 = as.numeric(case_when(sw01 == 8 ~ NA_character_, # 8: (172) dont know, coded as NA to be removed later\r\n                                     sw01 == 9 ~ NA_character_, # 9: (2)maybe data entry error, found a couple of 9\r\n                                     .default = as.character(sw01)\r\n                                     )\r\n                           ),\r\n         sw03 = as.numeric(case_when(sw03 == 8 ~ NA_character_, # 8: (1882)dont know, coded as NA to be removed later\r\n                                     sw03 == 9 ~ NA_character_, # 9: (3) maybe data entry error, found a couple of 9\r\n                                     .default = as.character(sw03)\r\n                                     )\r\n                           ),\r\n         kr03 = as.factor(case_when(kr03 == 1 ~ \"1\", #own\r\n                                    kr03 == 2 ~ \"0\", # occupying\r\n                                    kr03 == 5 ~ \"0\", # rented\r\n                                    kr03 == 95 ~ \"0\" # others\r\n                                    )\r\n                          ),\r\n         tk01a = as.factor(case_when(tk01a == 1 ~ \"1\", # employed\r\n                                     tk01a == 3 ~ \"0\", # unemployed\r\n                                     tk01a == 8 ~ \"0\" # prefer not to say\r\n                                     )\r\n                           ),\r\n         # tr11: Respondents are asked to rate how religious they see themselves\r\n         tr11 = as.factor(case_when(tr11 == 7 ~ \"0\", # refused to say\r\n                                    tr11 == 4 ~ \"1\", # not religious\r\n                                    tr11 == 3 ~ \"2\", # somewhat religious\r\n                                    tr11 == 2 ~ \"3\", # religious\r\n                                    tr11 == 1 ~ \"4\" # very religious\r\n                                    )\r\n                          ),\r\n         kk01 = as.factor(case_when(kk01 == 4 ~ \"1\", # unhealthy\r\n                                    kk01 == 3 ~ \"2\", # somewhat unhealthy\r\n                                    kk01 == 2 ~ \"3\", # somewhat healthy\r\n                                    kk01 == 1 ~ \"4\" # healthy\r\n                                    )\r\n                          ), # Education recoded to 5 broad levels.\r\n         dl06 = as.factor(case_when(dl06 == 99 ~ \"0\", # dont know, others, missing, school for disabled\r\n                                    dl06 == 98 ~ \"0\",\r\n                                    dl06 == 95 ~ \"0\",\r\n                                    dl06 == 17 ~ \"0\",\r\n                                    dl06 == 90 ~ \"1\", # kindergarten, elementary, islamic elementary\r\n                                    dl06 == 2 ~ \"1\",\r\n                                    dl06 == 72 ~ \"1\",\r\n                                    dl06 == 3 ~ \"2\", # junior high, jh vocational, islamic jh\r\n                                    dl06 == 4 ~ \"2\",\r\n                                    dl06 == 73 ~ \"2\",\r\n                                    dl06 == 5 ~ \"3\", # senior high, sh vocational, islamic sh\r\n                                    dl06 == 6 ~ \"3\",\r\n                                    dl06 == 74 ~ \"3\",\r\n                                    dl06 == 60 ~ \"4\", # college, university (under, master, doctorate)\r\n                                    dl06 == 61 ~ \"4\",\r\n                                    dl06 == 62 ~ \"4\",\r\n                                    dl06 == 63 ~ \"4\",\r\n                                    dl06 == 11 ~ \"5\", # adult ed, open uni, islamic school\r\n                                    dl06 == 12 ~ \"5\",\r\n                                    dl06 == 13 ~ \"5\",\r\n                                    dl06 == 14 ~ \"5\",\r\n                                    dl06 == 15 ~ \"5\"\r\n                                    )\r\n                          ),\r\n         age = as.numeric(age), # age as numeric\r\n         marstat = as.factor(marstat), # marital status as factor\r\n         female = as.factor(case_when(sex == 1 ~ \"0\", # sex=1 means male, 3=female, so i made male the baseline\r\n                                   sex == 3 ~ \"1\"\r\n                                   )\r\n                         ),\r\n         fo01 = sw03 - sw01 # Outlook score\r\n         )\r\nglimpse(data_recoded)\r\n\r\nRows: 36,581\r\nColumns: 16\r\n$ hhid14_9 <chr> \"001060000\", \"001060004\", \"001060000\", \"001060000\",…\r\n$ age      <dbl> 59, 28, 39, 16, 30, 36, 26, 40, 55, 54, 34, 28, 24,…\r\n$ marstat  <fct> 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 5, …\r\n$ sex      <dbl+lbl> 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3,…\r\n$ pidlink  <chr> \"001060001\", \"001060004\", \"001060007\", \"001060008\",…\r\n$ dl01f    <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"…\r\n$ dl06     <fct> 1, 1, 1, 2, 2, 1, 1, NA, 1, 1, 4, 4, 4, 4, 1, 2, 1,…\r\n$ sw00     <dbl> 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 1, 2, 2, 2, 2, 4, 3, …\r\n$ sw01     <dbl> 3, 2, 3, 3, 2, 2, 2, 2, 4, 3, 2, 2, 1, 3, 2, 3, 3, …\r\n$ sw03     <dbl> 4, 2, 3, 3, 2, 2, 3, 2, NA, 2, 2, 3, 5, 5, 3, 5, NA…\r\n$ tk01a    <fct> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, …\r\n$ tr11     <fct> 3, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, …\r\n$ kk01     <fct> 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, …\r\n$ kr03     <fct> 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, …\r\n$ female   <fct> 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, …\r\n$ fo01     <dbl> 1, 0, 0, 0, 0, 0, 1, 0, NA, -1, 0, 1, 4, 2, 1, 2, N…\r\n\r\n# DATA WRANGLING COMPLETE \r\n# saved as csv file. \r\n\r\n#write_csv(data_recoded, \"capstone_data.csv\")\r\n\r\n\r\nAgain, the above processes need only be done once. Thereafter, the dataset should be imported for analysis using a simple read_csv(capstone_data.csv) command. I’m sure you’d like to return to the main document now and read all about our analysis. You may do so by clicking on this link\r\n\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:31:30+08:00"
    },
    {
      "path": "README.html",
      "author": [],
      "contents": "\r\n\r\nContents\r\nblog\r\n\r\nblog\r\nlets make a change and see if it updates\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:31:30+08:00"
    },
    {
      "path": "README2.html",
      "author": [],
      "contents": "\r\nlets see if this is uploaded to github\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:07:15+08:00"
    },
    {
      "path": "snowman2023.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\nI made this snowman in Dec 2023 to celebrate Christmas and to learn gganimate. It may not look like much, but it did take a number of hours and about 100 lines of code. Enjoy!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:07:29+08:00"
    },
    {
      "path": "spiral.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\nI made this spiral animation as part of my first exploration into digital art using R.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:07:41+08:00"
    },
    {
      "path": "temperature_spiral.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\nDo you think its been getting hotter and hotter in recent years? What about rain? Have we been getting more erratic rainfall in recent years? Here’s a cool way to visualize both.\r\nI stumbled upon Pat Schloss’ youtube channel the other day, while searching for R tutorials online. I watched his video on creating an animated climate spiral, and was inspired to try and do the same, using data for Singapore.\r\nThis is the link to Pat’s blog. I “borrowed” his idea of using “next_jan” so that the lines for December and January would touch and appear seamless. Thanks Pat! This is the climate spiral created by the smart folks at NASA’s Scientific Visualization Studio.\r\nHere is my attempt at creating a temperature spiral for Singapore. It is slightly different from Pat’s. My temperature lines are colored by temperature difference, rather than by year.\r\n\r\nThis is a different way of visualizing the same data. Which is more effective? Which do you prefer?\r\n\r\n\r\nI didnt stop there. Since there was rainfall data available, I created something similar for rainfall in Singapore.\r\n\r\nThe code for generating the temperature spiral. WARNING! It takes while to render, which is why I’ve included them as animated gifs in this page, rather than having the code render them “on the fly”. Data was obtained from data.gov.sg. Wouldn’t it be great to have data going back to the early 1900s?\r\nYou may use or modify the code for your own animation, but I would greatly appreciate a link back, please.\r\n\r\n\r\n# Remove all objects in workspace ----\r\nrm(list = ls())\r\n\r\npacman::p_load(tidyverse, lubridate, glue, ggthemes, readr, gganimate, gifski, magick)\r\n\r\ndf <- read_csv(\"SurfaceAirTemperatureMonthlyMean.csv\") #obtain data from data.gov.sg\r\n\r\ndata <-\r\n  df %>% \r\n  separate_wider_delim(month,\r\n                       delim = \"-\",\r\n                       names = c(\"year\", \"mth\")\r\n                       ) %>% \r\n  as.tibble() %>% \r\n  mutate(year = as.numeric(year),\r\n         mth = as.numeric(mth),\r\n         month = month.abb[mth]\r\n         )\r\n\r\ndf_8289 <- \r\n  data %>% \r\n  filter(year >= 1982 & year <= 1989) %>% \r\n  group_by(mth) %>% \r\n  summarise(m_temp = mean(mean_temp))\r\n\r\ndata_joined <-\r\n  left_join(data, df_8289, by = \"mth\") %>% \r\n  mutate(temp_dev = round((mean_temp - m_temp), digits = 2))\r\n  \r\nnext_jan <-\r\n  data_joined %>% \r\n  filter(month == \"Jan\") %>% \r\n  mutate(year = year -1,\r\n         month = \"next_jan\")\r\n\r\ntemp_data <-\r\n  bind_rows(data_joined, next_jan) %>% \r\n  mutate(month = factor(month,\r\n                        levels = c(month.abb, \"next_jan\")\r\n                        ),\r\n         mth = as.numeric(month)\r\n         ) \r\n\r\np <- \r\n  ggplot(data = temp_data,\r\n       aes(x = mth,\r\n           y = temp_dev,\r\n           group = year)\r\n       ) +\r\n  geom_line(aes(color = temp_dev) #plot lines for each year\r\n            )+\r\n  geom_hline(yintercept = c(-1:2), # concentric lines indicating scale\r\n             color = \"grey\")+\r\n  geom_label(aes(x = 12, # year in the middle\r\n                y = -2.8,\r\n                label = year), \r\n            fill = \"black\",\r\n            color = \"white\",\r\n            size = 9,\r\n            label.size = 0) +\r\n  geom_label(aes(x = 12, #labels for each concentric line\r\n                 y = -1),\r\n             label = \"-1 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"blue\",\r\n             label.size = 0,\r\n             size = 5) +\r\n  \r\n  geom_label(aes(x = 12,\r\n                 y = 0),\r\n             label = \"+0 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"yellow\",\r\n             label.size = 0,\r\n             size = 5) +\r\n  \r\n  geom_label(aes(x = 12,\r\n                 y = 1),\r\n             label = \"+1 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"orange\",\r\n             label.size = 0,\r\n             size = 5) +\r\n  geom_label(aes(x = 12,\r\n                 y = 2),\r\n             label = \"+2 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"red\",\r\n             label.size = 0,\r\n             size = 5) +\r\n\r\n  coord_polar(start = 2*pi/12)+\r\n  scale_x_continuous(breaks = 1:12,\r\n                     labels = month.abb\r\n                     ) +\r\n  scale_y_continuous(breaks = seq(-1.5, 2.5, 0.2),\r\n                     limits = c(-3, 2.5)\r\n                     )+\r\n  scale_color_gradient(low = \"blue\",\r\n                       high = \"red\")+\r\n  labs(title = \"Temperature change in Singapore (1982-2023)\",\r\n       subtitle = \"Monthly Mean Surface Air Temperature.\",\r\n       caption = \"Data from https//data.gov.sg\\n\r\n       Baseline: Average Monthly Temperature from 1982-1989\")+\r\n  theme(legend.position = \"none\",\r\n        plot.title = element_text(color = \"white\", \r\n                                  size = rel(1.2)),\r\n        plot.subtitle = element_text(color = \"white\",\r\n                                    size = rel(1)),\r\n        plot.caption = element_text(color = \"white\",\r\n                                    size = rel(0.8)),\r\n        axis.text.x = element_text(color = \"white\",\r\n                                   size = rel(2)),\r\n        axis.text.y = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        panel.background = element_rect(fill = \"black\"),\r\n        panel.grid = element_blank(),\r\n        plot.background = element_rect(fill = \"black\")\r\n        ) +\r\n  transition_time(year)+\r\n  shadow_mark(past = TRUE, alpha = 0.9)\r\nanimate(p, fps = 10, nframes = 200, end_pause = 60)\r\nanim_save(\"name_of_your_choice\", p)\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:07:45+08:00"
    },
    {
      "path": "thankyouprofroh.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\nI’ve finally graduated from SMU’s Certified Data Analytics (R) Specialist course. It was an amazing 5 months, full of ups and downs. I kept my fingers crossed over the first three modules of the course, praying not to fail at each assessment. My code was plagued with errors which took many many hours to resolve.\r\nIt wasn’t till November 2023 that I was able to follow along in class, and complete each lesson without any coding errors. I made this animation as a graduation present for Professor Roh. He is an amazing person.\r\n\r\nProf Roh: Thank you for inspiring me in this learning journey. Your energy and enthusiasm kept me going. Here’s my graduation present to you.\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe code for generating this animation can be found below. The most difficult part I experienced was writing the code for the fireworks display. Yes, the colorful dots at the top of the animation are about 10 clusters of fireworks. I used a kmeans algorithm to “partition” a selection of “sparkles” into clusters about a center. Subsequently, I had to figure out how to sequence the appearance of each sparkle. The desired effect I wanted to achieve was for each sparkle closest to their respective center to appear.\r\nYou may use or modify the code for your own animation, but I would greatly appreciate a link back, please.\r\n\r\n\r\n# Clear global environment ----\r\nrm(list = ls())\r\n# Load packages ----\r\nlibrary(tidyverse)\r\nlibrary(imager) #package for image processing\r\nlibrary(gganimate)\r\nlibrary(randomcoloR) #generate random colors\r\n\r\n#set.seed(20240114)\r\n\r\n# Read in coordinates of photo ----\r\ndf <- read_csv(\"prof_rohV2_coord.csv\")\r\ndf_length <- nrow(df)\r\ndf <-\r\n  df%>% \r\n  rename(order = n) %>% \r\n  mutate(order = rep(1:72, length.out = df_length))\r\n# 72 represents what WAS the max sparkles per cluster. In the next revision \r\n# this will be the first item to fix. Unfortunately, this project was assembled\r\n# 3 parts (photo, fireworks, thank you) in a very short time\r\n\r\n# Find min max x,y of photo to determine subsequent position of thank you\r\n# and fireworks elements.\r\nxmin <- min(df$x)\r\nxmax <- max(df$x)\r\nymin <- min(-df$y)\r\nymax <- max(-df$y)\r\n\r\n\r\n# Creating fireworks ----\r\n# Generate data for fireworks\r\n\r\ni <- 10 # number of clusters\r\ns <- 50 # number of sparkles per center\r\n\r\n# generate 10,000 random dots per cluster\r\ndots <-\r\n  tibble(x = runif(n = 10000*i, min = xmin, max = xmax),\r\n         y = runif(n = 10000*i, min = ymax, max = 100)) #adjust relative positioning\r\n# s * i gives number of dots per display\r\n\r\n\r\n# generate color table. c controls color, cluster_n is the cluster number\r\n# you could always specify your own colors\r\ncolor_table <-\r\n  tibble(cluster_n = 1:i,\r\n         c = randomColor(i)\r\n         )\r\n# sample s*i number of sparkles per cluster of fireworks\r\nsparkles <-\r\n  tibble(x = sample(dots$x, s*i),\r\n         y = sample(dots$y, s*i)\r\n         )\r\n# use kmeans algorithm to partition sparkles to each center\r\nkm <- kmeans(sparkles,\r\n             centers = i,\r\n             iter.max = 50,\r\n             nstart = 1)\r\n\r\n# Information on centers of each fireworks cluster\r\ncenter <- \r\n  tibble(c_x = km$centers[,1],\r\n         c_y = km$centers[,2]) %>% \r\n  mutate(cluster_n = 1:n())\r\n\r\n# Information on cluster allocation ie: tells you which sparkle is assigned to each cluster\r\ncluster <- \r\n  tibble(cluster_n = km$cluster)\r\n\r\n# generate the fireworks by assembling all plot info\r\nfireworks <-\r\n  bind_cols(sparkles, cluster) %>% # sparkles assigned to cluster\r\n  left_join(., color_table, by = \"cluster_n\") %>% # color assigned to cluster\r\n  left_join(., center, by = \"cluster_n\") %>% # center assigned to cluster\r\n  # next 4 lines of code have to do with sequencing of sparkles for animation\r\n  # i wanted sparkles closest to each center to start together in the animation\r\n  mutate(distance = sqrt((x - c_x)^2 + (y - c_y)^2)) %>% #calculate distance between sparkle and center\r\n  group_by(cluster_n) %>% \r\n  arrange(distance) %>% #arrange by ascending distance\r\n  mutate(order = 1:n()) %>% # ordering for animation sequence\r\n  ungroup()\r\n\r\n\r\n# Read in coordinates of thank you ----\r\ndf_t <- read_csv(\"thankyou.csv\")\r\ndf_t_length <- nrow(df_t)\r\ndf_t <-\r\n  df_t %>% \r\n  rename(order = n) %>% \r\n  mutate(order = rep(1:72, length.out = df_t_length))\r\n\r\n# Finally we plot ----\r\np <-\r\n  ggplot() +\r\n  # plot the face\r\n  geom_point(data = df,\r\n             aes(x = x,\r\n                 y = -y, # right-side the image. imagine reflecting off x-axis\r\n                 group = order),\r\n             size = 1.25,\r\n             color = \"white\") + # change back to white\r\n  # plot fireworks\r\n  geom_point(data = fireworks %>% filter(cluster_n == 1), #one geom per cluster\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 2),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 3),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 4),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 5),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 6),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 7),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 8),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 9),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 10),\r\n             aes(x = x, y = y, color = c), size = 2)+\r\n  # plot thank you\r\n  geom_point(data = df_t,\r\n             aes(x = x,\r\n                 y = -y+90, # positioning adjustment for relative positioning of items\r\n                 group = order),\r\n             size = 2,\r\n             color = \"white\") +\r\n  theme_set(theme_void()) + \r\n  theme(panel.background = element_rect(fill = 'black'),\r\n        legend.position = \"none\",\r\n        # rmarkdown refused to knit without gridlines or recognize theme_void() so I am removing them manually\r\n        axis.text.x = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        panel.border = element_blank(),\r\n        panel.grid = element_blank(),\r\n        axis.title.x = element_blank(),\r\n        axis.title.y = element_blank()\r\n        ) +\r\n  # set animation transition\r\n  transition_time(order)+\r\n  shadow_mark(alpha = 0.8, size = rel(0.8))\r\n\r\n# Render animation ----\r\nanimate(p, fps = 20, nframes = 200, end_pause = 60, height = 600, width = 600)\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:08:24+08:00"
    },
    {
      "path": "whatsapp.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntroduction\r\nProject\r\nExporting Whatsapp Chat\r\nImport and Data Wrangling\r\nData Visualization\r\nTokenize the words\r\nAFINN, BING and NRC\r\nLets do some very basic EDA\r\nHave fun with word cloud\r\nSentiment Analysis using AFINN Index\r\nCumulative Average Daily Sentiment\r\nSentiment Analysis: Marc vs Andrew\r\nTokenizing by n-grams\r\nSentiment Analysis using Bing\r\nNet Sentiment Using Bing\r\nSentiment Analysis using NRC\r\nFinal Thoughts\r\n\r\nIntroduction\r\nI’m attending the Certified Data Analytics (R) Specialist course taught by Professor Sungjong Roh at SMU Academy. In Module 3: Web scraping and Data Insights, we learnt the basics of Natural Language Processing (NLP) and Sentiment Analysis of text.\r\nI was particularly intrigued by the topic of Sentiment Analysis, and furthered my knowledge by reading the book “Text Mining with R: A Tidy Approach” by Julia Silge & David Robinson.\r\n\r\nYou may access read the book by following this link.\r\nProject\r\nTo further hone my skills in R, basic natural language processing (NLP) and sentiment analysis, I thought that it would be cool to do a sentiment analysis of my family’s Whatsapp group chat. This group chat was started in June 2019 and comprises 4 members, my wife (Angelina) , my two sons Marc (age 18) and Andrew (age 13) and me.\r\nThe chatgroup contains approximately 65,000 messages, and is used mainly to coordinate schedules, update whereabouts of each member, and chatting about anything under the sun. My end goal is to see if the tools I learnt in class could be used to provide an insight into the:\r\noverall sentiment of our chatgroup. Do we generally use positive language (I hope so) or is the group environment toxic (that would be a disappointment!).\r\nindividual sentiment of each member of the chatgroup. Are there members who are positive/negative and does that “gel” with my knowledge of their personality? Were there periods of positive or negative sentiment throughout the year(s), and does that tie-in with any significant events the family was experiencing?\r\nExporting Whatsapp Chat\r\nThis is fairly easy to do, and the chat exports as a single text file with date/timestamp. Unfortunately, stickers and emojis which are often used in our chatgroup cannot be exported.\r\nImport and Data Wrangling\r\nThe chat file can be imported into R using read_csv(). The chat.txt file imported from Whatsapp was relatively “clean”, and required minimal wrangling.\r\nThe date of message sent, name of sender, and text message were extracted using a combination of str_extract() and regex patterns. Date was subsequently formatted using dmy(). NA data was filtered using na.omit(). Finally, a “message_id” was assigned to each row using the function rowid_to_column()\r\nData Visualization\r\nLet’s find out who sent the most messages within the chatgroup.\r\n\r\nMirror mirror on the wall, who’s the most chatty of them all?\r\n\r\nAnyone want’s to make a guess?\r\nSurprise surpise! My wife sent the most messages within the group! Let’s take a look at the number of daily messages sent in the chatgroup since its inception.\r\n\r\n\r\n\r\nFigure 1: Daily Number of Messages Sent in chatgroup (2019 to date)\r\n\r\n\r\n\r\nInteresting! The number of daily number of messages within the chatgroup increased after 2022. Perhaps as life returned to a “post-covid normal”, messaging within the group increased as we were “out and about” much more, and required more coordination amidst our activities. Or maybe we were just more active on our phones?\r\nTokenize the words\r\nIn order to run sentiment analysis on the chat, each message needed to be parsed into individual words. This process of “tokenizing” the words is easily done within R using the function unnest_tokens(). This resulted in approximately 515,000 tokens.\r\nNext, I removed “stop-words” using tidytext::stop_words and anti_join(). These stop-words carry “no meaning” in sentiment analysis and are therefore removed prior to analysis. You are also able to remove names or other words you deem irrelevant by adding them to a custom lexicon. You will read more about that later…\r\nAFINN, BING and NRC\r\nHere’s a short introduction of the 3 sentiment lexicons I used to assist with sentiment analysis:\r\nAFINN is a lexicon of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. You can read more about AFINN here.\r\nBing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative. It was first published by Minqing Hu and Bing Liu in 2004. You can read more about Bing here.\r\nThe NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). You can read more about NRC here.\r\nI used these 3 “dictionaries” to perform sentiment analysis on the word tokens.\r\nLets do some very basic EDA\r\nUsing the AFINN indexed list of words (data_for_NLP_AFINN_indexed), here is the top 20 most frequently used words in our chatgroup:\r\n\r\n\r\nTable 1: Top 20 Most Frequently Used Words in Chatgroup\r\n\r\n\r\nWord\r\n\r\n\r\nCount\r\n\r\n\r\nlove\r\n\r\n\r\n1241\r\n\r\n\r\nleave\r\n\r\n\r\n794\r\n\r\n\r\nnice\r\n\r\n\r\n478\r\n\r\n\r\nwow\r\n\r\n\r\n332\r\n\r\n\r\ndrop\r\n\r\n\r\n272\r\n\r\n\r\nbad\r\n\r\n\r\n244\r\n\r\n\r\nfun\r\n\r\n\r\n225\r\n\r\n\r\nhaha\r\n\r\n\r\n214\r\n\r\n\r\nyeah\r\n\r\n\r\n212\r\n\r\n\r\nsafe\r\n\r\n\r\n201\r\n\r\n\r\njoin\r\n\r\n\r\n186\r\n\r\n\r\nfree\r\n\r\n\r\n184\r\n\r\n\r\nhope\r\n\r\n\r\n184\r\n\r\n\r\ncut\r\n\r\n\r\n179\r\n\r\n\r\nhappy\r\n\r\n\r\n156\r\n\r\n\r\nclean\r\n\r\n\r\n155\r\n\r\n\r\nshit\r\n\r\n\r\n142\r\n\r\n\r\nfine\r\n\r\n\r\n140\r\n\r\n\r\nluck\r\n\r\n\r\n128\r\n\r\n\r\nenjoy\r\n\r\n\r\n125\r\n\r\n\r\nNow here’s a slight problem. The word “leave”, which has a frequency of 794 and carries a sentiment score of “-1” in AFINN. As our family frequently uses the chatgroup to coordinate our schedules, the word “leave” is likely taken our of context here.\r\nFor example: “Let’s leave at 9am” or “We plan to leave at 7pm today.” As opposed to ” I have going to leave you!” which has a negative context.\r\nRemember I mentioned above about adding “custom” stopwords? To avoid potentially skewing the results, I decided removed the word “leave” by including it as a “custom lexicon” stop-word. Thereafter, I ran the analysis using AFINN again.\r\nSimilarly, the word “swim” is found within the NRC sentiments of anticipation, fear (really? why?) and joy. As both my sons are competitive swimmers and the word “swim” is used very frequently within our chat (760 times to be exact!), “swim” was removed as well.\r\nJust for fun, and probably to show off what I learnt in class, here is a word cloud after removing the word “leave”.\r\n\r\n\r\n\r\n\r\nFigure 2: WordCloud: Top 50 words after removing “leave”\r\n\r\n\r\n\r\nAs you can see, its a rather nice chatgroup to be a part of, with positive words such as “love”, “nice” and “happy”. Of course, there are also negative words being used, of which “shit” stands out. I’m cautiously optimistic that the overall AFINN index of our chat will be positive. 🤞\r\nHave fun with word cloud\r\nWord cloud is rather fun and it’s easy to get carried away. So here’s a couple more. This is a word cloud of our top 50 most frequently used words with a sentiment score <0.\r\n\r\n\r\n\r\n\r\nFigure 3: Top 50 Frequently used words with negative sentiment score\r\n\r\n\r\n\r\nThis is a word cloud of my top 50 most frequently used words. Hey, I use “love” quite a lot :)\r\n\r\n\r\n\r\n\r\nFigure 4: WordCloud: Mark’s Top 50 most frequently used words\r\n\r\n\r\n\r\nHere is Marc’s top 50 most frequently used words. Now we know who uses the word “shit” frequently. Still, it looks relatively positive.\r\n\r\n\r\n\r\n\r\nFigure 5: WordCloud: Marc’s Top 50 most frequently used words\r\n\r\n\r\n\r\nThis is Andrew’s top 50 most frequently used words. It would appear that he uses the word “love” quite frequently. 💖\r\n\r\n\r\n\r\n\r\nFigure 6: WordCloud: Andrew’s Top 50 most frequently used words\r\n\r\n\r\n\r\nLast but not least, here is a word cloud of my wife’s top 50 most frequently used words.\r\n\r\n\r\n\r\n\r\nFigure 7: WordCloud: Angelina’s Top 50 most frequently used words\r\n\r\n\r\n\r\nSentiment Analysis using AFINN Index\r\nLet’s shift gears and take a deeper look at what the AFINN indexed list of words reveal about the sentiment within our chatgroup.\r\nThe list of tokenized words were analyzed using the AFINN dictionary, where each word is given a sentiment score ranging from -5 (negative sentiment) to +5 (positive sentiment).\r\nI calculated an average sentiment score and standard error for each member of the family.\r\n\r\n\r\nTable 2: Average Sentiment of family members\r\n\r\n\r\nName\r\n\r\n\r\nAverage Sentiment\r\n\r\n\r\nStandard Error\r\n\r\n\r\nAndrew\r\n\r\n\r\n1.378\r\n\r\n\r\n0.074\r\n\r\n\r\nMark\r\n\r\n\r\n0.659\r\n\r\n\r\n0.032\r\n\r\n\r\nAaangelina\r\n\r\n\r\n0.639\r\n\r\n\r\n0.031\r\n\r\n\r\nMarc\r\n\r\n\r\n0.112\r\n\r\n\r\n0.058\r\n\r\n\r\nFamily\r\n\r\n\r\n0.627\r\n\r\n\r\n0.020\r\n\r\n\r\nMy wife and I had an average sentiment score similar to that of the overall “Family”. Andrew had the highest average sentiment score (perhaps he uses the word “love” too often?) while Marc had the lowest average sentiment score.\r\nHere is a boxplot summarizing the results,\r\n\r\n\r\n\r\nFigure 8: Sentiment Analysis of Yeo Family chatgroup (AFINN)\r\n\r\n\r\n\r\nThe difference in average sentiment score between Marc and Andrew stands out. Andrew’s AFINN sentiment score is much higher than Marc’s. Does this simply imply the use of more positive language within the chat, or does it make inference to a difference in their personalities? Can the difference be attributed to age where a younger child tends to be more openly expressive with his feelings, while an older child is more “reserved”?\r\nPersonally, I am surprised that the AFINN sentiment analysis of our chats was able to quite accutately make inference to the personality of my children. Marc tends to be more reserved and neutral, which may explain his use of words that are less “emotionally charged” and balanced. On the other hand, Andrew is more outgoing, bubbly and optimistic, which may explain his higher average sentiment score. Or maybe he simply loves the word “love” which carries a sentiment score of +3.\r\nCumulative Average Daily Sentiment\r\nNext, I plotted the cumulative average daily sentiment for each member of the family. I’m not sure what I was hoping to find. If you had a neutral average daily sentiment score, the line should be flat. If you were generally positive, I’d expect an upward sloping line. Conversely, periods of sustained negative sentiment would result in a downward trend.\r\n\r\n\r\n\r\nFigure 9: Cumulative Average Daily Sentiment Score (AFINN)\r\n\r\n\r\n\r\nA couple of interesting observations:\r\nAndrew had a significant increase in cumulative average daily sentiment score between 2022 and 2023. We discussed this, and brainstormed for possible reasons. I felt that possibly “covid reopening” and “life back to normal” was a contributing factor to his improved sentiment. Being more outgoing, perhaps covid-restrictions had a larger impact on his sentiment. Andrew thinks that his move to a different competitive swim club with a “less toxic” culture and people might have been a contributing factor in lifting his general mood.\r\nThe plot for Marc is almost flat throughout the year, reaching a peak score of only 75. This could imply days of positive average sentiment cancelling out days of negative sentiment, or simply the use of language that is “emotionally neutral”. Alarmingly, the plot showed a decline from mid-2022 to the start of 2023. We brainstormed for possible reasons to explain this, but none come to mind.\r\nHere is Marc’s plot.\r\n\r\n\r\n\r\nFigure 10: Marc’s Cumulative Average Daily Sentiment Score (AFINN)\r\n\r\n\r\n\r\nSentiment Analysis: Marc vs Andrew\r\nI am curious to explore the difference in sentiment scores between Marc and Andrew. To investigate further, I looked for words that they used in common, and analyzed to see if the frequency of usage of these words contributed to the difference in their sentiment scores.\r\nTo do this, I first created individual AFINN-indexed word lists for each child. Marc used 1783 AFINN-indexed words, while Andrew used 921 AFINN-indexed words. I then calculated their respective frequency of AFINN-indexed word usage.\r\nNext, I created a list of AFINN-indexed words that they used in common using inner_join(). This resulted in a list of 138 AFINN-indexed words that they used in common.\r\nI created a plot of these 138 words that they used in common, with the frequency of word usage by Andrew on the x-axis, and the frequency of word usage by Marc on the y-axis.\r\n\r\n\r\n\r\nFigure 11: Frequency of Usage (Words in Common) Marc vs Andrew\r\n\r\n\r\n\r\nIts quite apparent that the frequency of usage of the word “love” by Andrew is an outlier (32% vs 2% usage by Marc). In order to get a clearer picture of the other words in use, I removed the word “love” before generating the same plot again.\r\n\r\n\r\n\r\nFigure 12: Frequency of Word Usage (Words in Common) Marc vs Andrew. The word love has been removed.\r\n\r\n\r\n\r\nYou may notice (or if you actually counted), not all 138 (or 137 minus the word “love”) words have been plotted. That is because “check_overlap” in geom_text() has been set to TRUE to avoid the text from overlapping.\r\nWords that are close to or on the blue line (AB Line with slope = 1, intercept = 0) represent words that both children use with similar frequency. Examples of these words are “super”, “died”, “stupid”, “happy”, and “lol”. Marc uses the words “nice” and “shit” more frequently than Andrew, while Andrew uses the words “haha”, “yeah”, and “care” more frequently than Marc.\r\nIs it apparent now why Andrew might have a higher AFINN sentiment score in his chats? Maybe the next plot will give a clearer indication. I multiplied the frequency of each word used by its AFINN sentiment value.\r\n\r\n\r\n\r\nFigure 13: Frequency of word used x AFINN Sentiment Value\r\n\r\n\r\n\r\nAgain, we see the use or excessive use of the word “love” most likely skewing Andrew’s sentiment score vs Marc, which is not a bad thing :). Let’s look at the same plot with the word ‘love’ removed.\r\n\r\n\r\n\r\nFigure 14: Frequency of word used x AFINN Sentiment Value. The word ‘love’ has been removed.\r\n\r\n\r\n\r\nTokenizing by n-grams\r\nThus far, I’ve only considered words as individual units, and evaluated their relationships to sentiments. However, tokenizing by single-words has its drawbacks because relationships exist between words that follow one-another.\r\nThis is because words correlate with one another, and the choice of first word can significantly change the meaning of the second word. For example, putting “not” before “love” significantly changes the meaning and therefore sentiment.\r\nFor the purpose of this project, I shall limit my analysis to pairs of words, or “bigrams”. You can certainly analyze further by using “trigrams” (n=3). Hence more generally, a token comprising n words is called an “n-gram or ngram”. Tokenizing on bigrams would allow me to capture and examine the immediate context around each word.\r\nThe output is word-pairs in the column bigram. The bigrams are then separated into columns word1 and word2, before filtering for numbers and stopwords. You can choose to recombine word1 and word2 back into bigrams, but I chose to leave them separated.\r\nHere’s a table of the top 20 “paired words” in terms of frequency of usage:\r\n\r\n\r\nTable 3: Top 20 Bigrams\r\n\r\n\r\nword1\r\n\r\n\r\nword2\r\n\r\n\r\nfrequency\r\n\r\n\r\nlove\r\n\r\n\r\ny’all\r\n\r\n\r\n160\r\n\r\n\r\nsleep\r\n\r\n\r\ntight\r\n\r\n\r\n154\r\n\r\n\r\ny’all\r\n\r\n\r\ntmr\r\n\r\n\r\n144\r\n\r\n\r\nbed\r\n\r\n\r\nbugs\r\n\r\n\r\n106\r\n\r\n\r\nbugs\r\n\r\n\r\nbite\r\n\r\n\r\n106\r\n\r\n\r\ncoming\r\n\r\n\r\nhome\r\n\r\n\r\n105\r\n\r\n\r\ntight\r\n\r\n\r\ndon’t\r\n\r\n\r\n103\r\n\r\n\r\ntmr\r\n\r\n\r\nsleep\r\n\r\n\r\n103\r\n\r\n\r\nbye\r\n\r\n\r\nbye\r\n\r\n\r\n100\r\n\r\n\r\nkor\r\n\r\n\r\nkor\r\n\r\n\r\n79\r\n\r\n\r\nmom\r\n\r\n\r\nlove\r\n\r\n\r\n76\r\n\r\n\r\nwater\r\n\r\n\r\npolo\r\n\r\n\r\n68\r\n\r\n\r\notw\r\n\r\n\r\nhome\r\n\r\n\r\n66\r\n\r\n\r\nbowling\r\n\r\n\r\nalley\r\n\r\n\r\n63\r\n\r\n\r\ncar\r\n\r\n\r\npark\r\n\r\n\r\n63\r\n\r\n\r\npls\r\n\r\n\r\ndon’t\r\n\r\n\r\n61\r\n\r\n\r\nmarcy\r\n\r\n\r\npls\r\n\r\n\r\n58\r\n\r\n\r\ndon’t\r\n\r\n\r\nworry\r\n\r\n\r\n57\r\n\r\n\r\nsweet\r\n\r\n\r\ndreams\r\n\r\n\r\n55\r\n\r\n\r\nyeah\r\n\r\n\r\nyeah\r\n\r\n\r\n54\r\n\r\n\r\nNope! We don’t have bed bugs at home. Instead, the bigrams come from our favorite good-night phrase “sleep tight, don’t let the bed bugs bite”.\r\nGoing beyond counting, we can actually visualize the connections and correlations between words in the chatgroup thanks to igraph.\r\n\r\n\r\n\r\nFigure 15: igraph Plot of Top 50 bigrams\r\n\r\n\r\n\r\nAnd here’s the cool looking igraph, where arrows show the direction of correlation between words. The weights of the arrows shoe the number of times each bigram is used. The graph plots the top 50 bigrams used in our chat. That’s so cool!\r\nAs you can see, there aren’t any “negating words” such as “not love” or bigrams with “negative-positive” sentiment within the top 50 bigrams. As the bulk of this chat happened during the covid and post-covid period, you’ll see interesting bigrams like “wear mask”, “stay safe” and “sore throat”.\r\nSentiment Analysis using Bing\r\nRecall that Bing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\r\nHere is the top 10 most frequently used words in our chatgroup, indexed using Bing. Within this list, there are 9 positive sentiment words and 1 negative sentiment word.\r\n\r\n\r\nTable 4: Top 10 most frequently used words, indexed by Bing\r\n\r\n\r\nword1\r\n\r\n\r\nword2\r\n\r\n\r\nfrequency\r\n\r\n\r\npositive\r\n\r\n\r\nlove\r\n\r\n\r\n1241\r\n\r\n\r\npositive\r\n\r\n\r\nnice\r\n\r\n\r\n478\r\n\r\n\r\npositive\r\n\r\n\r\nready\r\n\r\n\r\n370\r\n\r\n\r\npositive\r\n\r\n\r\nwow\r\n\r\n\r\n332\r\n\r\n\r\nnegative\r\n\r\n\r\nbad\r\n\r\n\r\n244\r\n\r\n\r\npositive\r\n\r\n\r\nfun\r\n\r\n\r\n225\r\n\r\n\r\npositive\r\n\r\n\r\nsafe\r\n\r\n\r\n201\r\n\r\n\r\npositive\r\n\r\n\r\nfree\r\n\r\n\r\n184\r\n\r\n\r\npositive\r\n\r\n\r\nhappy\r\n\r\n\r\n156\r\n\r\n\r\npositive\r\n\r\n\r\nclean\r\n\r\n\r\n155\r\n\r\n\r\nHere is a plot comparing the top 10 negative and positive sentiment words used in our chat. I’m happy to see that the use of positive sentiment words far overwhelm the use of negative sentiment words.\r\n\r\n\r\n\r\nFigure 16: Top 10 Negative and Positive Sentiment Words, indexed by Bing\r\n\r\n\r\n\r\nHere’s a different way of looking at the same data, this time the usage of negative sentiment words is represented on the negative x-axis. Which way of visualization do you prefer?\r\n\r\n\r\n\r\nFigure 17: Top 10 Negative and Positive Sentiment Words, indexed by Bing, represented on negative-positive scale\r\n\r\n\r\n\r\nNet Sentiment Using Bing\r\nAlthough Bing classifies words in a binary manner, positive or negative, it is still possible to calculate a score that gives you an indication of the overall sentiment. Each occurrence of a positive word is given a sentiment score of “+1”, while a negative word is given a sentiment score of “-1”. This allows me to calculate a net sentiment score, and in this case, I did so for each member of the family.\r\nWhat do you think this analysis will reveal? Are you surprised by the result?\r\n\r\n\r\nTable 5: Net Sentiment Score, indexed by Bing\r\n\r\n\r\nFamily Member\r\n\r\n\r\nNet Sentiment\r\n\r\n\r\nAaangelina\r\n\r\n\r\n635\r\n\r\n\r\nAndrew\r\n\r\n\r\n184\r\n\r\n\r\nMarc\r\n\r\n\r\n-168\r\n\r\n\r\nMark\r\n\r\n\r\n734\r\n\r\n\r\nNext, I investigated the top 10 most frequently used positive and negative words by each member of the family. The chart was easily created using facet_wrap().\r\n\r\n\r\n\r\nFigure 18: Top 10 Frequently used Positive and Negative words, indexed by Bing, for each family member\r\n\r\n\r\n\r\nRecall from earlier, I am the top user of the word “love” within the chat group. Well, Bing picked that up too. Now, why isn’t my AFINN average sentiment index much higher? Hmmm, the only plausible explanation is that I must be using negative sentiment words quite frequently as well. 🤷‍♂️\r\nAs for Marc, his number of negative sentiment words used (1038) outnumbers the number of positive sentiment words used (870). But his top 10 negative and positive words used is quite balanced.\r\nSentiment Analysis using NRC\r\nThe NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). A word can be listed under multiple emotions. For example, the word “bad” is listed under the emotions anger, disgust, fear, and sadness, as well as the negative sentiment.\r\nA total of 35,688 words from the chatgroup were indexed by NRC, and here is a summary by the eight basic emotions:\r\n\r\n\r\nTable 6: Proportion of words in each NRC sentiment\r\n\r\n\r\nSentiment\r\n\r\n\r\nNumber of Words\r\n\r\n\r\nFrequency\r\n\r\n\r\nanticipation\r\n\r\n\r\n9556\r\n\r\n\r\n26.78\r\n\r\n\r\ntrust\r\n\r\n\r\n6775\r\n\r\n\r\n18.98\r\n\r\n\r\njoy\r\n\r\n\r\n5563\r\n\r\n\r\n15.59\r\n\r\n\r\nfear\r\n\r\n\r\n3549\r\n\r\n\r\n9.94\r\n\r\n\r\nsadness\r\n\r\n\r\n3127\r\n\r\n\r\n8.76\r\n\r\n\r\nanger\r\n\r\n\r\n2459\r\n\r\n\r\n6.89\r\n\r\n\r\nsurprise\r\n\r\n\r\n2404\r\n\r\n\r\n6.74\r\n\r\n\r\ndisgust\r\n\r\n\r\n2255\r\n\r\n\r\n6.32\r\n\r\n\r\nI always prefer to look at things visually, so here’s a plot of the Top 10 words for each of the eight emotions.\r\n\r\n\r\n\r\nFigure 19: Top 10 Words Categorized by NRC into Eight Basic Emotions\r\n\r\n\r\n\r\nIts comes as no surprise to me that about 27 percent of words were categorized under the “anticipation” emotion. After all, we do use the chat quite frequently to coordinate our schedules and transportation needs. Its interesting that NRC was able to pick this up.\r\nHowever, I noticed that NRC does have its quirks. Take a look at the top word in the “fear” emotion. Mum?? I know my wife is a scary person, but how did NRC know that? “Mum” to refer to my wife or my mother (or their mother, if you were my children). Perhaps in this case, NRC mistook it as “to keep quiet”? Perhaps we should spell it as “mom” from now on in our chats?\r\nDo you notice any other strange classifications? battery <-> Anger, mother <-> Sadness are a few more that may have been incorrectly classified.\r\nThe next plot shows the relative frequency of each family member’s word usage by the eight basic NRC sentiments.\r\n\r\n\r\n\r\nFigure 20: Relative Frequency of Family Members’ Word Usage by Sentiment, indexed by NRC\r\n\r\n\r\n\r\nWhat do you make of the results? Well, as parents, I suppose its normal that Angelina and I have the highest frequency of words in the “anticipation” category. Its also heartening to see that categories associated with negative sentiment (fear, anger, sadness and disgust) do not rank highly among our family members.\r\nFinal Thoughts\r\nI had fun working on this project. Not only did my skills in R impRove, it also revealed some interesting insights about my family members.\r\nWas the analysis ground-breaking? Definitely not, and admittedly, there was a lot more I could have done, eg: tf-idf, analysis by tri-grams, by topic, or even at the sentence level. Maybe I’ll do a more comprehensive analysis after I complete the next module on Machine Learning next year. Thanks for reading!\r\n\r\n\r\n\r\n",
      "last_modified": "2024-01-23T13:08:45+08:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
