{
  "articles": [
    {
      "path": "about.html",
      "title": "maRk's blog",
      "description": "Some additional details about the blog",
      "author": [],
      "contents": "\r\n\r\nContents\r\nWhy this blog?\r\nHow I got started with R?\r\nA little about me\r\nThank you\r\n\r\nWhy this blog?\r\nI always wanted to start one…if fact, I’ve started close to 10, but gave up on all of them. blogger, wordpress, tumblr… I’ve tried them all. Will I give up on this one? Only time will tell…\r\nBut in the short time that I’ve been learning R, I’ve grown to love the ecosystem, and creating a post within Distill is quite easy. When I first considered blogging within the R ecosystem, I gave blogdown/Hugo a try. But I found it difficult to set up.\r\nThis blog is hosted on Netlify (for FREE), and configured to deploy from Github. There were some teething pains, and it took me a while to figure out Github. Maybe one day, I shall write a dummy’s dummy-guide. But in the meantime, themockup and The Distillery provided sufficient resources to get me started. Finally, I modified the site theme from Infrequently Frequentist so thanks a bunch!\r\nHow I got started with R?\r\nI started learning R in September 2023, so as of writing this blog, my experience with R spans barely 3 months. With all the talk about “big data” and “machine learning”, I was curious about the field of data science. In addition to that, I had skillsfuture credits that were expiring. A bit of googling later, I signed up for the Certified Data Analytics (R) Specialist offered by SMU Academy.\r\nWhy R? Everyone seemed to be talking about python, so I decided to go “against the herd” and do something different. Why this particular class? Firstly, it was open enrollment, meaning it didn’t have much/any pre-requisites to meet. Secondly, I read the instructor’s bio and realized that we both graduated from the same University. It was a done deal!\r\nA little about me\r\n\r\n\r\nI’m approaching 50, so if I can learn R, it’s proof that you can teach an old dog new tricks! Wooof! I graduated from an Ivy-League university some 30 years ago, and have a bit of programming background in Java and HTML. My work experience has been in tourism, hotel development, and finance.\r\nI’m married with 2 grown-up children, love swimming and have supported Arsenal FC since George Graham was a manager. My favorite Arsenal player is the “non-flying dutchman”.\r\nThank you\r\nThank you to Prof Roh for being such an inspiration in this jouRney. Your enthusiasm and energy is infectious! Thank you to my wife and family for supporting me as I struggled with R.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-12-04T23:32:35+08:00"
    },
    {
      "path": "index.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\n\r\nWelcome! And thank you for stopping by. This blog was created using Distill for R Markdown and written entirely in R Studio. Hopefully, it will be a growing showcase of all projects related to R which I embark on.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-12-04T23:32:12+08:00"
    },
    {
      "path": "nikkei225.html",
      "title": "Nikkei225 Gamma Exposure Profile",
      "description": "Gamma Exposure Profile\n",
      "author": [],
      "contents": "\r\nIntroduction\r\nWhat happens in the options market strongly influences the behavior of the underlying asset. Market-making firms quote two-sided markets (ie: they provide liquidity on the bid and ask) and need to constantly delta-hedge their position to minimize any directional risks that they might have. (ie: maintain delta neutral portfolio) If all this sounds greek to you, well, that’s because it is. Here’s a primer on option greeks.\r\nIn options trading, delta hedging is defined as the process of buying or selling shares (or futures in this case) in the underlying asset to reduce the directional risk (meaning you do not want to be long or short in delta terms) of the portfolio as price changes. The number of shares (or futures) to buy or sell depends on the price movement of the underlying asset, in this case the Nikkei 225 index.\r\nWhy is this important?\r\nGamma shows the potential amount of delta-hedging activity by the market making firms. It’s a source of one of the most significant structural flows in the equity markets and it is non-discretionary. Market makers need to hedge regardless of market liquidity. In the Nikkei225, the top market making firm in the options market is ABN AMRO Clearing Tokyo followed by Societe Generale Securities Japan (in terms of volume). Here is a list of appointed market makers in JPX.\r\nDue to its nature, Gamma can exacerbate how the market moves, and that is largely dependent on how market makers are positioned. If the market maker is “long gamma” (ie: on net they are long options), they tend to be sellers when markets rally, and buyers when markets drop, thus dampening volatility. Generally, if you are net long options, you are gamma positive. If you are a net seller of options, you are short gamma.\r\nThe assumption in our calculation is that market-makers are short puts and long calls. The basis of this assumption is that you, as as investor, tends to buy puts for protection, and sell calls for yield. Hence, the market maker’s book is opposite to yours.\r\nFiguring out the book of ABN AMRO Clearing Tokyo is not a trivial task. Its probably impossibe, but we can make educated guesses. Every evening, at around 2000 hrs, JPX releases 2 reports on its website.\r\nToday’s Trading Overview, which details the “Derivative Open Interest” for all products traded, Nikkei 225 include, and\r\nTrading Volume by Trading Participant, which gives us information on net buying/selling of call and put options by trading members in option contracts within 750 points of the previous closing price, for the front month contract.\r\nAdditionally, every weekend, JPX releases a report on Open Interest by Trading Participants.\r\nThat’s quite a bit of clues given to us for FREE by JPX. Thank you JPX. How you decide to parse that information to figure out the market maker’s book is up to you, but the base assumption I’ve made is that the market-maker is short put and long calls. Depending on their net book, ABN AMRO Clearing Tokyo could be short or long gamma. This is where the concept of Gamma Exposure comes in.\r\nGamma Exposure\r\nWhat’s Gamma Exposure? Also known as “dollar gamma”, Gamma Exposure measures the price sensitivity of a portfolio of options (in this case, we are trying to estimate the market maker’s book of Nikkei 225 options) to changes in the price of the underlying security (Nikkei 225 futures).\r\nA “how-to” or “step by step” article might be written soon, but meanwhile, let’s take a minute to credit Sergei Perfiliev and Squeeze Metrics, where I first learnt of this concept.\r\nGetting back to Gamma Exposure. As the market moves, there is a price point where market maker’s gamma exposure “flips” from positive to negative (or negative to positive). If the market maker’s book is in positive gamma territory, market’s tend to be calm and volatility tends to be low. Rallies are sold, and dips are bought by the market maker (as long as they stay within positive gamma). However, when the market maker’s book flips to negative gamma, that’s when it starts to get exciting.\r\nWhen a market maker’s book is negative gamma, expect delta hedging flows in the direction of market moves. Meaning selling as the market falls, and buying back as the market bounces. These flows tend to amplify already market moves, leading to increased volatility.\r\nThere’s a lot of research out there on Gamma Exposure for S&P500, but not much about Asian indices. Hence, when I started learning R, I decided to put my skills to the test and developed a similar Gamma Exposure Profile for the Nikkei 225 index.\r\nThis is a work in progress, meaning you will (hopefully) see the quality of my posts get better as my skills in R improve. Thank you for reading!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-12-06T17:42:17+08:00"
    },
    {
      "path": "README.html",
      "author": [],
      "contents": "\r\n\r\nContents\r\nblog\r\n\r\nblog\r\nlets make a change and see if it updates\r\n\r\n\r\n",
      "last_modified": "2023-12-06T17:42:17+08:00"
    },
    {
      "path": "README2.html",
      "author": [],
      "contents": "\r\nlets see if this is uploaded to github\r\n\r\n\r\n",
      "last_modified": "2023-12-01T17:02:45+08:00"
    },
    {
      "path": "whatsapp.html",
      "title": "maRk's blog",
      "description": "something something...\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntroduction\r\nProject\r\nExporting Whatsapp Chat\r\nImport and Data Wrangling\r\nData Visualization\r\nTokenize the words\r\nAFINN, BING and NRC\r\nLets do some very basic EDA\r\nHave fun with word cloud\r\nSentiment Analysis using AFINN Index\r\nCumulative Average Daily Sentiment\r\nSentiment Analysis: Marc vs Andrew\r\nTokenizing by n-grams\r\nSentiment Analysis using Bing\r\nNet Sentiment Using Bing\r\nSentiment Analysis using NRC\r\nFinal Thoughts\r\n\r\nIntroduction\r\nI’m attending the Certified Data Analytics (R) Specialist course taught by Professor Sungjong Roh at SMU Academy. In Module 3: Web scraping and Data Insights, we learnt the basics of Natural Language Processing (NLP) and Sentiment Analysis of text.\r\nI was particularly intrigued by the topic of Sentiment Analysis, and furthered my knowledge by reading the book “Text Mining with R: A Tidy Approach” by Julia Silge & David Robinson.\r\n\r\nYou may access read the book by following this link.\r\nProject\r\nTo further hone my skills in R, basic natural language processing (NLP) and sentiment analysis, I thought that it would be cool to do a sentiment analysis of my family’s Whatsapp group chat. This group chat was started in June 2019 and comprises 4 members, my wife (Angelina) , my two sons Marc (age 18) and Andrew (age 13) and me.\r\nThe chatgroup contains approximately 65,000 messages, and is used mainly to coordinate schedules, update whereabouts of each member, and chatting about anything under the sun. My end goal is to see if the tools I learnt in class could be used to provide an insight into the:\r\noverall sentiment of our chatgroup. Do we generally use positive language (I hope so) or is the group environment toxic (that would be a disappointment!).\r\nindividual sentiment of each member of the chatgroup. Are there members who are positive/negative and does that “gel” with my knowledge of their personality? Were there periods of positive or negative sentiment throughout the year(s), and does that tie-in with any significant events the family was experiencing?\r\nExporting Whatsapp Chat\r\nThis is fairly easy to do, and the chat exports as a single text file with date/timestamp. Unfortunately, stickers and emojis which are often used in our chatgroup cannot be exported.\r\nImport and Data Wrangling\r\nThe chat file can be imported into R using read_csv(). The chat.txt file imported from Whatsapp was relatively “clean”, and required minimal wrangling.\r\nThe date of message sent, name of sender, and text message were extracted using a combination of str_extract() and regex patterns. Date was subsequently formatted using dmy(). NA data was filtered using na.omit(). Finally, a “message_id” was assigned to each row using the function rowid_to_column()\r\nData Visualization\r\nLet’s find out who sent the most messages within the chatgroup.\r\n\r\nMirror mirror on the wall, who’s the most chatty of them all?\r\n\r\nAnyone want’s to make a guess?\r\nSurprise surpise! My wife sent the most messages within the group! Let’s take a look at the number of daily messages sent in the chatgroup since its inception.\r\n\r\n\r\n\r\nFigure 1: Daily Number of Messages Sent in chatgroup (2019 to date)\r\n\r\n\r\n\r\nInteresting! The number of daily number of messages within the chatgroup increased after 2022. Perhaps as life returned to a “post-covid normal”, messaging within the group increased as we were “out and about” much more, and required more coordination amidst our activities. Or maybe we were just more active on our phones?\r\nTokenize the words\r\nIn order to run sentiment analysis on the chat, each message needed to be parsed into individual words. This process of “tokenizing” the words is easily done within R using the function unnest_tokens(). This resulted in approximately 515,000 tokens.\r\nNext, I removed “stop-words” using tidytext::stop_words and anti_join(). These stop-words carry “no meaning” in sentiment analysis and are therefore removed prior to analysis. You are also able to remove names or other words you deem irrelevant by adding them to a custom lexicon. You will read more about that later…\r\nAFINN, BING and NRC\r\nHere’s a short introduction of the 3 sentiment lexicons I used to assist with sentiment analysis:\r\nAFINN is a lexicon of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. You can read more about AFINN here.\r\nBing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative. It was first published by Minqing Hu and Bing Liu in 2004. You can read more about Bing here.\r\nThe NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). You can read more about NRC here.\r\nI used these 3 “dictionaries” to perform sentiment analysis on the word tokens.\r\nLets do some very basic EDA\r\nUsing the AFINN indexed list of words (data_for_NLP_AFINN_indexed), here is the top 20 most frequently used words in our chatgroup:\r\n\r\n\r\nTable 1: Top 20 Most Frequently Used Words in Chatgroup\r\n\r\n\r\nWord\r\n\r\n\r\nCount\r\n\r\n\r\nlove\r\n\r\n\r\n1241\r\n\r\n\r\nleave\r\n\r\n\r\n794\r\n\r\n\r\nnice\r\n\r\n\r\n478\r\n\r\n\r\nwow\r\n\r\n\r\n332\r\n\r\n\r\ndrop\r\n\r\n\r\n272\r\n\r\n\r\nbad\r\n\r\n\r\n244\r\n\r\n\r\nfun\r\n\r\n\r\n225\r\n\r\n\r\nhaha\r\n\r\n\r\n214\r\n\r\n\r\nyeah\r\n\r\n\r\n212\r\n\r\n\r\nsafe\r\n\r\n\r\n201\r\n\r\n\r\njoin\r\n\r\n\r\n186\r\n\r\n\r\nfree\r\n\r\n\r\n184\r\n\r\n\r\nhope\r\n\r\n\r\n184\r\n\r\n\r\ncut\r\n\r\n\r\n179\r\n\r\n\r\nhappy\r\n\r\n\r\n156\r\n\r\n\r\nclean\r\n\r\n\r\n155\r\n\r\n\r\nshit\r\n\r\n\r\n142\r\n\r\n\r\nfine\r\n\r\n\r\n140\r\n\r\n\r\nluck\r\n\r\n\r\n128\r\n\r\n\r\nenjoy\r\n\r\n\r\n125\r\n\r\n\r\nNow here’s a slight problem. The word “leave”, which has a frequency of 794 and carries a sentiment score of “-1” in AFINN. As our family frequently uses the chatgroup to coordinate our schedules, the word “leave” is likely taken our of context here.\r\nFor example: “Let’s leave at 9am” or “We plan to leave at 7pm today.” As opposed to ” I have going to leave you!” which has a negative context.\r\nRemember I mentioned above about adding “custom” stopwords? To avoid potentially skewing the results, I decided removed the word “leave” by including it as a “custom lexicon” stop-word. Thereafter, I ran the analysis using AFINN again.\r\nSimilarly, the word “swim” is found within the NRC sentiments of anticipation, fear (really? why?) and joy. As both my sons are competitive swimmers and the word “swim” is used very frequently within our chat (760 times to be exact!), “swim” was removed as well.\r\nJust for fun, and probably to show off what I learnt in class, here is a word cloud after removing the word “leave”.\r\n\r\n\r\n\r\n\r\nFigure 2: WordCloud: Top 50 words after removing “leave”\r\n\r\n\r\n\r\nAs you can see, its a rather nice chatgroup to be a part of, with positive words such as “love”, “nice” and “happy”. Of course, there are also negative words being used, of which “shit” stands out. I’m cautiously optimistic that the overall AFINN index of our chat will be positive. 🤞\r\nHave fun with word cloud\r\nWord cloud is rather fun and it’s easy to get carried away. So here’s a couple more. This is a word cloud of our top 50 most frequently used words with a sentiment score <0.\r\n\r\n\r\n\r\n\r\nFigure 3: Top 50 Frequently used words with negative sentiment score\r\n\r\n\r\n\r\nThis is a word cloud of my top 50 most frequently used words. Hey, I use “love” quite a lot :)\r\n\r\n\r\n\r\n\r\nFigure 4: WordCloud: Mark’s Top 50 most frequently used words\r\n\r\n\r\n\r\nHere is Marc’s top 50 most frequently used words. Now we know who uses the word “shit” frequently. Still, it looks relatively positive.\r\n\r\n\r\n\r\n\r\nFigure 5: WordCloud: Marc’s Top 50 most frequently used words\r\n\r\n\r\n\r\nThis is Andrew’s top 50 most frequently used words. It would appear that he uses the word “love” quite frequently. 💖\r\n\r\n\r\n\r\n\r\nFigure 6: WordCloud: Andrew’s Top 50 most frequently used words\r\n\r\n\r\n\r\nLast but not least, here is a word cloud of my wife’s top 50 most frequently used words.\r\n\r\n\r\n\r\n\r\nFigure 7: WordCloud: Angelina’s Top 50 most frequently used words\r\n\r\n\r\n\r\nSentiment Analysis using AFINN Index\r\nLet’s shift gears and take a deeper look at what the AFINN indexed list of words reveal about the sentiment within our chatgroup.\r\nThe list of tokenized words were analyzed using the AFINN dictionary, where each word is given a sentiment score ranging from -5 (negative sentiment) to +5 (positive sentiment).\r\nI calculated an average sentiment score and standard error for each member of the family.\r\n\r\n\r\nTable 2: Average Sentiment of family members\r\n\r\n\r\nName\r\n\r\n\r\nAverage Sentiment\r\n\r\n\r\nStandard Error\r\n\r\n\r\nAndrew\r\n\r\n\r\n1.378\r\n\r\n\r\n0.074\r\n\r\n\r\nMark\r\n\r\n\r\n0.659\r\n\r\n\r\n0.032\r\n\r\n\r\nAaangelina\r\n\r\n\r\n0.639\r\n\r\n\r\n0.031\r\n\r\n\r\nMarc\r\n\r\n\r\n0.112\r\n\r\n\r\n0.058\r\n\r\n\r\nFamily\r\n\r\n\r\n0.627\r\n\r\n\r\n0.020\r\n\r\n\r\nMy wife and I had an average sentiment score similar to that of the overall “Family”. Andrew had the highest average sentiment score (perhaps he uses the word “love” too often?) while Marc had the lowest average sentiment score.\r\nHere is a boxplot summarizing the results,\r\n\r\n\r\n\r\nFigure 8: Sentiment Analysis of Yeo Family chatgroup (AFINN)\r\n\r\n\r\n\r\nThe difference in average sentiment score between Marc and Andrew stands out. Andrew’s AFINN sentiment score is much higher than Marc’s. Does this simply imply the use of more positive language within the chat, or does it make inference to a difference in their personalities? Can the difference be attributed to age where a younger child tends to be more openly expressive with his feelings, while an older child is more “reserved”?\r\nPersonally, I am surprised that the AFINN sentiment analysis of our chats was able to quite accutately make inference to the personality of my children. Marc tends to be more reserved and neutral, which may explain his use of words that are less “emotionally charged” and balanced. On the other hand, Andrew is more outgoing, bubbly and optimistic, which may explain his higher average sentiment score. Or maybe he simply loves the word “love” which carries a sentiment score of +3.\r\nCumulative Average Daily Sentiment\r\nNext, I plotted the cumulative average daily sentiment for each member of the family. I’m not sure what I was hoping to find. If you had a neutral average daily sentiment score, the line should be flat. If you were generally positive, I’d expect an upward sloping line. Conversely, periods of sustained negative sentiment would result in a downward trend.\r\n\r\n\r\n\r\nFigure 9: Cumulative Average Daily Sentiment Score (AFINN)\r\n\r\n\r\n\r\nA couple of interesting observations:\r\nAndrew had a significant increase in cumulative average daily sentiment score between 2022 and 2023. We discussed this, and brainstormed for possible reasons. I felt that possibly “covid reopening” and “life back to normal” was a contributing factor to his improved sentiment. Being more outgoing, perhaps covid-restrictions had a larger impact on his sentiment. Andrew thinks that his move to a different competitive swim club with a “less toxic” culture and people might have been a contributing factor in lifting his general mood.\r\nThe plot for Marc is almost flat throughout the year, reaching a peak score of only 75. This could imply days of positive average sentiment cancelling out days of negative sentiment, or simply the use of language that is “emotionally neutral”. Alarmingly, the plot showed a decline from mid-2022 to the start of 2023. We brainstormed for possible reasons to explain this, but none come to mind.\r\nHere is Marc’s plot.\r\n\r\n\r\n\r\nFigure 10: Marc’s Cumulative Average Daily Sentiment Score (AFINN)\r\n\r\n\r\n\r\nSentiment Analysis: Marc vs Andrew\r\nI am curious to explore the difference in sentiment scores between Marc and Andrew. To investigate further, I looked for words that they used in common, and analyzed to see if the frequency of usage of these words contributed to the difference in their sentiment scores.\r\nTo do this, I first created individual AFINN-indexed word lists for each child. Marc used 1783 AFINN-indexed words, while Andrew used 921 AFINN-indexed words. I then calculated their respective frequency of AFINN-indexed word usage.\r\nNext, I created a list of AFINN-indexed words that they used in common using inner_join(). This resulted in a list of 138 AFINN-indexed words that they used in common.\r\nI created a plot of these 138 words that they used in common, with the frequency of word usage by Andrew on the x-axis, and the frequency of word usage by Marc on the y-axis.\r\n\r\n\r\n\r\nFigure 11: Frequency of Usage (Words in Common) Marc vs Andrew\r\n\r\n\r\n\r\nIts quite apparent that the frequency of usage of the word “love” by Andrew is an outlier (32% vs 2% usage by Marc). In order to get a clearer picture of the other words in use, I removed the word “love” before generating the same plot again.\r\n\r\n\r\n\r\nFigure 12: Frequency of Word Usage (Words in Common) Marc vs Andrew. The word love has been removed.\r\n\r\n\r\n\r\nYou may notice (or if you actually counted), not all 138 (or 137 minus the word “love”) words have been plotted. That is because “check_overlap” in geom_text() has been set to TRUE to avoid the text from overlapping.\r\nWords that are close to or on the blue line (AB Line with slope = 1, intercept = 0) represent words that both children use with similar frequency. Examples of these words are “super”, “died”, “stupid”, “happy”, and “lol”. Marc uses the words “nice” and “shit” more frequently than Andrew, while Andrew uses the words “haha”, “yeah”, and “care” more frequently than Marc.\r\nIs it apparent now why Andrew might have a higher AFINN sentiment score in his chats? Maybe the next plot will give a clearer indication. I multiplied the frequency of each word used by its AFINN sentiment value.\r\n\r\n\r\n\r\nFigure 13: Frequency of word used x AFINN Sentiment Value\r\n\r\n\r\n\r\nAgain, we see the use or excessive use of the word “love” most likely skewing Andrew’s sentiment score vs Marc, which is not a bad thing :). Let’s look at the same plot with the word ‘love’ removed.\r\n\r\n\r\n\r\nFigure 14: Frequency of word used x AFINN Sentiment Value. The word ‘love’ has been removed.\r\n\r\n\r\n\r\nTokenizing by n-grams\r\nThus far, I’ve only considered words as individual units, and evaluated their relationships to sentiments. However, tokenizing by single-words has its drawbacks because relationships exist between words that follow one-another.\r\nThis is because words correlate with one another, and the choice of first word can significantly change the meaning of the second word. For example, putting “not” before “love” significantly changes the meaning and therefore sentiment.\r\nFor the purpose of this project, I shall limit my analysis to pairs of words, or “bigrams”. You can certainly analyze further by using “trigrams” (n=3). Hence more generally, a token comprising n words is called an “n-gram or ngram”. Tokenizing on bigrams would allow me to capture and examine the immediate context around each word.\r\nThe output is word-pairs in the column bigram. The bigrams are then separated into columns word1 and word2, before filtering for numbers and stopwords. You can choose to recombine word1 and word2 back into bigrams, but I chose to leave them separated.\r\nHere’s a table of the top 20 “paired words” in terms of frequency of usage:\r\n\r\n\r\nTable 3: Top 20 Bigrams\r\n\r\n\r\nword1\r\n\r\n\r\nword2\r\n\r\n\r\nfrequency\r\n\r\n\r\nlove\r\n\r\n\r\ny’all\r\n\r\n\r\n160\r\n\r\n\r\nsleep\r\n\r\n\r\ntight\r\n\r\n\r\n154\r\n\r\n\r\ny’all\r\n\r\n\r\ntmr\r\n\r\n\r\n144\r\n\r\n\r\nbed\r\n\r\n\r\nbugs\r\n\r\n\r\n106\r\n\r\n\r\nbugs\r\n\r\n\r\nbite\r\n\r\n\r\n106\r\n\r\n\r\ncoming\r\n\r\n\r\nhome\r\n\r\n\r\n105\r\n\r\n\r\ntight\r\n\r\n\r\ndon’t\r\n\r\n\r\n103\r\n\r\n\r\ntmr\r\n\r\n\r\nsleep\r\n\r\n\r\n103\r\n\r\n\r\nbye\r\n\r\n\r\nbye\r\n\r\n\r\n100\r\n\r\n\r\nkor\r\n\r\n\r\nkor\r\n\r\n\r\n79\r\n\r\n\r\nmom\r\n\r\n\r\nlove\r\n\r\n\r\n76\r\n\r\n\r\nwater\r\n\r\n\r\npolo\r\n\r\n\r\n68\r\n\r\n\r\notw\r\n\r\n\r\nhome\r\n\r\n\r\n66\r\n\r\n\r\nbowling\r\n\r\n\r\nalley\r\n\r\n\r\n63\r\n\r\n\r\ncar\r\n\r\n\r\npark\r\n\r\n\r\n63\r\n\r\n\r\npls\r\n\r\n\r\ndon’t\r\n\r\n\r\n61\r\n\r\n\r\nmarcy\r\n\r\n\r\npls\r\n\r\n\r\n58\r\n\r\n\r\ndon’t\r\n\r\n\r\nworry\r\n\r\n\r\n57\r\n\r\n\r\nsweet\r\n\r\n\r\ndreams\r\n\r\n\r\n55\r\n\r\n\r\nyeah\r\n\r\n\r\nyeah\r\n\r\n\r\n54\r\n\r\n\r\nNope! We don’t have bed bugs at home. Instead, the bigrams come from our favorite good-night phrase “sleep tight, don’t let the bed bugs bite”.\r\nGoing beyond counting, we can actually visualize the connections and correlations between words in the chatgroup thanks to igraph.\r\n\r\n\r\n\r\nFigure 15: igraph Plot of Top 50 bigrams\r\n\r\n\r\n\r\nAnd here’s the cool looking igraph, where arrows show the direction of correlation between words. The weights of the arrows shoe the number of times each bigram is used. The graph plots the top 50 bigrams used in our chat. That’s so cool!\r\nAs you can see, there aren’t any “negating words” such as “not love” or bigrams with “negative-positive” sentiment within the top 50 bigrams. As the bulk of this chat happened during the covid and post-covid period, you’ll see interesting bigrams like “wear mask”, “stay safe” and “sore throat”.\r\nSentiment Analysis using Bing\r\nRecall that Bing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\r\nHere is the top 10 most frequently used words in our chatgroup, indexed using Bing. Within this list, there are 9 positive sentiment words and 1 negative sentiment word.\r\n\r\n\r\nTable 4: Top 10 most frequently used words, indexed by Bing\r\n\r\n\r\nword1\r\n\r\n\r\nword2\r\n\r\n\r\nfrequency\r\n\r\n\r\npositive\r\n\r\n\r\nlove\r\n\r\n\r\n1241\r\n\r\n\r\npositive\r\n\r\n\r\nnice\r\n\r\n\r\n478\r\n\r\n\r\npositive\r\n\r\n\r\nready\r\n\r\n\r\n370\r\n\r\n\r\npositive\r\n\r\n\r\nwow\r\n\r\n\r\n332\r\n\r\n\r\nnegative\r\n\r\n\r\nbad\r\n\r\n\r\n244\r\n\r\n\r\npositive\r\n\r\n\r\nfun\r\n\r\n\r\n225\r\n\r\n\r\npositive\r\n\r\n\r\nsafe\r\n\r\n\r\n201\r\n\r\n\r\npositive\r\n\r\n\r\nfree\r\n\r\n\r\n184\r\n\r\n\r\npositive\r\n\r\n\r\nhappy\r\n\r\n\r\n156\r\n\r\n\r\npositive\r\n\r\n\r\nclean\r\n\r\n\r\n155\r\n\r\n\r\nHere is a plot comparing the top 10 negative and positive sentiment words used in our chat. I’m happy to see that the use of positive sentiment words far overwhelm the use of negative sentiment words.\r\n\r\n\r\n\r\nFigure 16: Top 10 Negative and Positive Sentiment Words, indexed by Bing\r\n\r\n\r\n\r\nHere’s a different way of looking at the same data, this time the usage of negative sentiment words is represented on the negative x-axis. Which way of visualization do you prefer?\r\n\r\n\r\n\r\nFigure 17: Top 10 Negative and Positive Sentiment Words, indexed by Bing, represented on negative-positive scale\r\n\r\n\r\n\r\nNet Sentiment Using Bing\r\nAlthough Bing classifies words in a binary manner, positive or negative, it is still possible to calculate a score that gives you an indication of the overall sentiment. Each occurrence of a positive word is given a sentiment score of “+1”, while a negative word is given a sentiment score of “-1”. This allows me to calculate a net sentiment score, and in this case, I did so for each member of the family.\r\nWhat do you think this analysis will reveal? Are you surprised by the result?\r\n\r\n\r\nTable 5: Net Sentiment Score, indexed by Bing\r\n\r\n\r\nFamily Member\r\n\r\n\r\nNet Sentiment\r\n\r\n\r\nAaangelina\r\n\r\n\r\n635\r\n\r\n\r\nAndrew\r\n\r\n\r\n184\r\n\r\n\r\nMarc\r\n\r\n\r\n-168\r\n\r\n\r\nMark\r\n\r\n\r\n734\r\n\r\n\r\nNext, I investigated the top 10 most frequently used positive and negative words by each member of the family. The chart was easily created using facet_wrap().\r\n\r\n\r\n\r\nFigure 18: Top 10 Frequently used Positive and Negative words, indexed by Bing, for each family member\r\n\r\n\r\n\r\nRecall from earlier, I am the top user of the word “love” within the chat group. Well, Bing picked that up too. Now, why isn’t my AFINN average sentiment index much higher? Hmmm, the only plausible explanation is that I must be using negative sentiment words quite frequently as well. 🤷‍♂️\r\nAs for Marc, his number of negative sentiment words used (1038) outnumbers the number of positive sentiment words used (870). But his top 10 negative and positive words used is quite balanced.\r\nSentiment Analysis using NRC\r\nThe NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). A word can be listed under multiple emotions. For example, the word “bad” is listed under the emotions anger, disgust, fear, and sadness, as well as the negative sentiment.\r\nA total of 35,688 words from the chatgroup were indexed by NRC, and here is a summary by the eight basic emotions:\r\n\r\n\r\nTable 6: Proportion of words in each NRC sentiment\r\n\r\n\r\nSentiment\r\n\r\n\r\nNumber of Words\r\n\r\n\r\nFrequency\r\n\r\n\r\nanticipation\r\n\r\n\r\n9556\r\n\r\n\r\n26.78\r\n\r\n\r\ntrust\r\n\r\n\r\n6775\r\n\r\n\r\n18.98\r\n\r\n\r\njoy\r\n\r\n\r\n5563\r\n\r\n\r\n15.59\r\n\r\n\r\nfear\r\n\r\n\r\n3549\r\n\r\n\r\n9.94\r\n\r\n\r\nsadness\r\n\r\n\r\n3127\r\n\r\n\r\n8.76\r\n\r\n\r\nanger\r\n\r\n\r\n2459\r\n\r\n\r\n6.89\r\n\r\n\r\nsurprise\r\n\r\n\r\n2404\r\n\r\n\r\n6.74\r\n\r\n\r\ndisgust\r\n\r\n\r\n2255\r\n\r\n\r\n6.32\r\n\r\n\r\nI always prefer to look at things visually, so here’s a plot of the Top 10 words for each of the eight emotions.\r\n\r\n\r\n\r\nFigure 19: Top 10 Words Categorized by NRC into Eight Basic Emotions\r\n\r\n\r\n\r\nIts comes as no surprise to me that about 27 percent of words were categorized under the “anticipation” emotion. After all, we do use the chat quite frequently to coordinate our schedules and transportation needs. Its interesting that NRC was able to pick this up.\r\nHowever, I noticed that NRC does have its quirks. Take a look at the top word in the “fear” emotion. Mum?? I know my wife is a scary person, but how did NRC know that? “Mum” to refer to my wife or my mother (or their mother, if you were my children). Perhaps in this case, NRC mistook it as “to keep quiet”? Perhaps we should spell it as “mom” from now on in our chats?\r\nDo you notice any other strange classifications? battery <-> Anger, mother <-> Sadness are a few more that may have been incorrectly classified.\r\nThe next plot shows the relative frequency of each family member’s word usage by the eight basic NRC sentiments.\r\n\r\n\r\n\r\nFigure 20: Relative Frequency of Family Members’ Word Usage by Sentiment, indexed by NRC\r\n\r\n\r\n\r\nWhat do you make of the results? Well, as parents, I suppose its normal that Angelina and I have the highest frequency of words in the “anticipation” category. Its also heartening to see that categories associated with negative sentiment (fear, anger, sadness and disgust) do not rank highly among our family members.\r\nFinal Thoughts\r\nI had fun working on this project. Not only did my skills in R impRove, it also revealed some interesting insights about my family members.\r\nWas the analysis ground-breaking? Definitely not, and admittedly, there was a lot more I could have done, eg: tf-idf, analysis by tri-grams, by topic, or even at the sentence level. Maybe I’ll do a more comprehensive analysis after I complete the next module on Machine Learning next year. Thanks for reading!\r\n\r\n\r\n\r\n",
      "last_modified": "2023-12-04T23:30:26+08:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
