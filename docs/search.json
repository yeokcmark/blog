{
  "articles": [
    {
      "path": "about.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\nContents\r\nWhy this blog?\r\nHow I got started with R?\r\nA little about me\r\nThank you\r\n\r\n\r\n\r\nWhy this blog?\r\nI always wanted to start oneâ€¦if fact, Iâ€™ve started close to 10, but gave up on all of them. blogger, wordpress, tumblrâ€¦ Iâ€™ve tried them all. Will I give up on this one? Only time will tellâ€¦\r\nBut in the short time that Iâ€™ve been learning R, Iâ€™ve grown to love the ecosystem, and creating a post within Distill is quite easy. When I first considered blogging within the R ecosystem, I gave blogdown/Hugo a try. But I found it difficult to set up.\r\nThis blog is hosted on Netlify (for FREE), and configured to deploy from Github. There were some teething pains, and it took me a while to figure out Github. Maybe one day, I shall write a dummyâ€™s dummy-guide. But in the meantime, themockup and The Distillery provided sufficient resources to get me started. Finally, I modified the site theme from Infrequently Frequentist so thanks a bunch!\r\nHow I got started with R?\r\nI started learning R in September 2023, so as of writing this blog, my experience with R spans barely 3 months. With all the talk about â€œbig dataâ€ and â€œmachine learningâ€, I was curious about the field of data science. In addition to that, I had skillsfuture credits that were expiring. A bit of googling later, I signed up for the Certified Data Analytics (R) Specialist offered by SMU Academy.\r\nWhy R? Everyone seemed to be talking about python, so I decided to go â€œagainst the herdâ€ and do something different. Why this particular class? Firstly, it was open enrollment, meaning it didnâ€™t have much/any pre-requisites to meet. Secondly, I read the instructorâ€™s bio and realized that we both graduated from the same University. It was a done deal!\r\nA little about me\r\n\r\n\r\nIâ€™m approaching 50, so if I can learn R, itâ€™s proof that you can teach an old dog new tricks! Wooof! I graduated from an Ivy-League university some 30 years ago, and have a bit of programming background in Java and HTML. My work experience has been in tourism, hotel development, and finance.\r\nIâ€™m married with 2 grown-up children, love swimming and have supported Arsenal FC since George Graham was a manager. My favorite Arsenal player is the â€œnon-flying dutchmanâ€.\r\nThank you\r\nThank you to Prof Roh for being such an inspiration in this jouRney. Your enthusiasm and energy is infectious! Thank you to my wife and family for supporting me as I struggled with R.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-05-13T13:20:58+08:00"
    },
    {
      "path": "calculate_gamma_exposure_profile_nikkei225.html",
      "title": "How to calculate Gamma Exposure Profile",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntroduction\r\nWhy is this important?\r\nGamma Exposure\r\n\r\n\r\n\r\nIntroduction\r\nWhat happens in the options market strongly influences the behavior of the underlying asset. Market-making firms quote two-sided markets (ie: they provide liquidity on the bid and ask) and need to constantly delta-hedge their position to minimize any directional risks that they might have. (ie: maintain delta neutral portfolio) If all this sounds greek to you, well, thatâ€™s because it is. Hereâ€™s a primer on option greeks.\r\nIn options trading, delta hedging is defined as the process of buying or selling shares (or futures in this case) in the underlying asset to reduce the directional risk (meaning you do not want to be long or short in delta terms) of the portfolio as price changes. The number of shares (or futures) to buy or sell depends on the price movement of the underlying asset, in this case the Nikkei 225 index.\r\nWhy is this important?\r\nGamma shows the potential amount of delta-hedging activity by the market making firms. Itâ€™s a source of one of the most significant structural flows in the equity markets and it is non-discretionary. Market makers need to hedge regardless of market liquidity. In the Nikkei225, the top market making firm in the options market is ABN AMRO Clearing Tokyo, followed by Societe Generale Securities Japan (in terms of volume). Here is a list of appointed market makers in JPX. As of the date of publishing this article, it appears that ABN AMRO is more active in the options market, hence we will try and monitor its activities more closely.\r\nDue to its nature, Gamma can exacerbate how the market moves, and that is largely dependent on how market makers are positioned. If the market maker is â€œlong gammaâ€ (ie: on net they are long options), they tend to be sellers when markets rally, and buyers when markets drop, thus dampening volatility. Generally, if you are net long options, you are gamma positive. If you are a net seller of options, you are short gamma.\r\nThe basic assumption in our calculation is that market-makers are short puts and long calls. The basis of this assumption is that you, as as investor, tend to buy puts for protection, and sell calls for yield. Hence, the market makerâ€™s book is opposite to yours. This assumption seems rudimentary (because it is!), but it would suffice for the purpose of this article. I do have more sophisticated algorithms to monitor if â€œbids are hitâ€ or â€œoffers are liftedâ€ to provide a better guess if the market-maker bought or sold an option.\r\nFiguring out the book of ABN AMRO Clearing Tokyo is not a trivial task. Its probably impossible, but we can make educated guesses. But take heed, all we are doing is guess-work. Fortunately for us, JPX releases several pieces of information on its website, for FREE. Our job is to parse these releases, and hopefully glean some insights to guide our trading activities. Hereâ€™s where R comes in.\r\nAt the end of each trading day, JPX releases 3 reports on its website (Iâ€™m sure there are more reports released, but these are the 3 that weâ€™re most interested in):\r\nTodayâ€™s Trading Overview, which details the â€œDerivative Open Interestâ€ for all products traded, including the Nikkei 225. From this report, we are able to obtain the trading volume, change in open interest, and current/previous day open interest for every Nikkei 225 options contract traded that day. This report is released at 2000 hrs every business day.\r\nOption Theoretical Price, which provides us information on the theoretical price and volatility of every option listed, based on the dayâ€™s closing price of the underlying asset. No time of release indicated, but its available at 2000 hrs when I download â€œTodayâ€™s Trading Overviewâ€.\r\nTrading Volume by Trading Participant, which gives us information on total trading activity of call and put options by trading participants for about 10 call and 10 put strikes, spanning approximately 1250 points. This information is provided only for the front-month contract. Itâ€™s important to note here that the information is total trading activity, and not net position. For example: If the information tells you that XYZ Firm traded 1000 contracts in the Dec 23 33500 call, the firm could possibly have bought 500 contracts and sold 500 contracts within the trading day, giving it a net position of 0. This report is released at 1715 every business day.\r\nAdditionally, Monday at 1530, JPX releases a report on Open Interest by Trading Participants for the previous trading week. Now this report contains some real gems! Unfortunately, its not available daily, and it is quite delayed (by one week!). Nevertheless, it gives you a glimpse into the book for each trading participant. But, it only provides information on the front-month contract, and for only 5 call and 5 put strikes. ğŸ˜¡\r\nThatâ€™s quite a bit of clues given to us for FREE by JPX. Thank you JPX. Parsing this information manually in Excel (even with the use of macros) used to take me about an hour each day. Thanks to R, the process now takes 5 minutes. And that includes generating plots, and updates to the website.\r\nGetting back to the concept of â€œGamma Exposureâ€. How you decide to parse the available information to â€œguesstimateâ€ the market makerâ€™s book is up to you, but the base assumption Iâ€™ve made is that the market-maker is short put and long calls. Depending on their net book, ABN AMRO Clearing Tokyo could be short or long gamma. Generally, you are short gamma (theta positive) when you are nett short options, and you are long gamma when you are nett long options. Many factors influence the gamma of an option, such as volatility of the underlying asset, and time to expiration.\r\nGamma Exposure\r\nWhatâ€™s Gamma Exposure? Also known as â€œdollar gammaâ€, Gamma Exposure measures the price sensitivity of a portfolio of options (in this case, we are trying to estimate the market makerâ€™s book of Nikkei 225 options) to changes in the price of the underlying security (Nikkei 225 futures).\r\nA â€œhow-toâ€ or â€œstep by stepâ€ article might be written soon, but meanwhile, letâ€™s take a minute to credit Sergei Perfiliev and Squeeze Metrics, where I first learnt of this concept.\r\nGetting back to Gamma Exposure. As the market moves, there is a price point where market makerâ€™s gamma exposure â€œflipsâ€ from positive to negative (or negative to positive). If the market makerâ€™s book is in positive gamma territory, marketâ€™s tend to be calm and volatility tends to be low. Rallies are sold, and dips are bought by the market maker (as long as they stay within positive gamma). However, when the market makerâ€™s book flips to negative gamma, thatâ€™s when it starts to get exciting.\r\nWhen a market makerâ€™s book is negative gamma, expect delta hedging flows in the direction of market moves. Meaning selling as the market falls, and buying back as the market bounces. These flows tend to amplify already market moves, leading to increased volatility.\r\nThereâ€™s a lot of research out there on Gamma Exposure for S&P500, but not much about Asian indices. Hence, when I started learning R, I decided to put my skills to the test and developed a similar Gamma Exposure Profile for the Nikkei 225 index.\r\nThis is a work in progress, meaning you will (hopefully) see the quality of my posts get better as my skills in R improve. Thank you for reading!\r\n\r\n\r\n\r\n",
      "last_modified": "2024-05-13T13:20:59+08:00"
    },
    {
      "path": "capstone_home_ownership_optimism.html",
      "title": "Capstone Project: Does home ownership lead to improved outlook and optimism for one's future?",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntroduction\r\nProblem Statement\r\nWhy is this important?\r\nData Source\r\nImporting our Dataset\r\nPreparing the Data for Regression\r\nResponse and Explanatory Variables\r\nExploratory Data Analysis\r\nCorrelation Analysis\r\nOrdinary Least Square Model\r\nâ€œBarebonesâ€ Base Model\r\nModel with Demographic Control Variables\r\nModel with Current Economic Status\r\nModel Substituting Age with Generations\r\nModel with Interactions\r\nSimple Slopes Analysis\r\n\r\nReporting of Results\r\nEvaluating Models using ANOVA\r\nAssumption Check using GVLMA\r\nCheck Multicollinearity using VIF\r\n\r\nInterpretation of the Results\r\nThreats to Causal Inference\r\nImplications\r\nLimitations and Future Directions\r\nReferences\r\nRun sessionInfo()\r\n\r\n\r\n\r\nIntroduction\r\nThis is the capstone project my partner (Ms Dxxx Nx) and I worked on for the Certified Data Analytics (R) Specialist course offered by Singapore Management University, taught by an inspiring Professor Sungjong Roh.\r\nIt is a culmination of about 6 months of learning, and about 1 monthâ€™s work.\r\nProblem Statement\r\n\r\nâ€œIf a man owns a little property, that property is himâ€¦ it is a part of himâ€¦ in some ways heâ€™s bigger because he owns itâ€ - John Steinbeck, The Grapes of Wrath\r\n\r\n\r\nProperty rights are the most fundamental institution in any economy and society. Apart from economic gains that property ownership confers, individual owners are more confident, self-reliant, and entrepreneurial than non-property owners. In countries where property ownership is highly skewed and accessible only to elites, income inequality could lead to uncertainty and a lack of overall optimism among non-property owners.\r\nOur Capstone Project will attempt to determine if John Steinbeck is correct. Does property ownership lead to a better perception of oneself and greater optimism for the future?\r\nWhy is this important?\r\nThe findings of this project may influence how policies are implemented in Indonesia. If home-ownership leads to greater optimism and entrepreneurship among its people, this could positively impact the economy and political stability.\r\nFor our data science project, we activated the following packages, using the Tidyverse approach.\r\n\r\n\r\n# Load necessary packages\r\npacman::p_load(tidyverse, lubridate ,glue, forcats, knitr, rmarkdown, gganimate, transformr,\r\n               scales, gridExtra, ggthemes, ggrepel, patchwork, DT, \r\n               jtools, huxtable, broom, modelr, skimr, psych, Hmisc, \r\n               gvlma, ggfortify, sandwich, car, ggstance, broom.mixed,\r\n               interactions)\r\n\r\n\r\nWe chose theme_minimal() as the default theme for our plots.\r\n\r\n\r\ntheme_set(theme_minimal())\r\n\r\n\r\nWe wrote a function to make it easier to present results from our models. Letâ€™s load it here.\r\n\r\n\r\n# Function for model reporting (to make things look pretty)\r\n\r\nmodel_variable_true_regression_function <-\r\n  function(name_of_model)\r\n  {\r\n    model_name <-\r\n      glue(\"{name_of_model}_variables\")\r\n    model_name <-\r\n      name_of_model %>%\r\n      jtools::summ() %>%\r\n      broom::tidy()\r\n\r\n    model_regression_name <-\r\n      glue(\"{name_of_model}_regression\")\r\n\r\n    model_regression_name <-\r\n      model_name %>%\r\n      .[,1:3] %>%\r\n      rename(b = estimate,\r\n             SE = std.error) %>%\r\n      mutate(Z95_x_SE = 1.96 * SE,\r\n             CI_95_upper_bound = b+Z95_x_SE,\r\n             significance_95 = ifelse(abs(b) > Z95_x_SE,\r\n                                      \"sig\", \"NULL\")\r\n             )\r\n\r\n  }\r\n\r\n\r\nData Source\r\nThe RAND Indonesian Family Life Survey (IFLS) is an on-going longitudinal survey in Indonesia. The sample is representative of about 83% of the Indonesian population and contains over 30,000 individuals living in 13 of the 27 provinces in the country.\r\nThe fifth wave, IFLS-5, where our Capstone Project draws data from, was fielded 2014-15. More information about the survey can be found at this link.\r\nImporting our Dataset\r\nOur dataset was imported in a two-step process:\r\nStep 1: The IFLS contained over 150 separate datasets in Stata (DTA) format. Our variables were drawn from 7 datasets. As the process of importing, extracting, recoding (wrangling), and combining datasets is only necessary to be performed once, this process is detailed in a separate Rmarkdown document named capstone_Import_data.Rmd. Click here for more details about this process.\r\nThe final dataset for use in our analysis is saved as capstone_data.csv.\r\nStep 2: Here we import the dataset.\r\n\r\n\r\ndata_recoded<-\r\n  read_csv(\"capstone_data.csv\")\r\nglimpse(data_recoded)\r\n\r\nRows: 36,581\r\nColumns: 16\r\n$ hhid14_9 <chr> \"001060000\", \"001060004\", \"001060000\", \"001060000\",â€¦\r\n$ age      <dbl> 59, 28, 39, 16, 30, 36, 26, 40, 55, 54, 34, 28, 24,â€¦\r\n$ marstat  <dbl> 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 5, â€¦\r\n$ sex      <dbl> 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 3, â€¦\r\n$ pidlink  <chr> \"001060001\", \"001060004\", \"001060007\", \"001060008\",â€¦\r\n$ dl01f    <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"â€¦\r\n$ dl06     <dbl> 1, 1, 1, 2, 2, 1, 1, NA, 1, 1, 4, 4, 4, 4, 1, 2, 1,â€¦\r\n$ sw00     <dbl> 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 1, 2, 2, 2, 2, 4, 3, â€¦\r\n$ sw01     <dbl> 3, 2, 3, 3, 2, 2, 2, 2, 4, 3, 2, 2, 1, 3, 2, 3, 3, â€¦\r\n$ sw03     <dbl> 4, 2, 3, 3, 2, 2, 3, 2, NA, 2, 2, 3, 5, 5, 3, 5, NAâ€¦\r\n$ tk01a    <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, â€¦\r\n$ tr11     <dbl> 3, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, â€¦\r\n$ kk01     <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, â€¦\r\n$ kr03     <dbl> 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, â€¦\r\n$ female   <dbl> 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, â€¦\r\n$ fo01     <dbl> 1, 0, 0, 0, 0, 0, 1, 0, NA, -1, 0, 1, 4, 2, 1, 2, Nâ€¦\r\n\r\nThe data_recoded contained 36581 observations.\r\nPreparing the Data for Regression\r\nWe prepare the data for regression by:\r\nremoving NA data,\r\nrenaming variables to more intuitive names (eg: employ vs tk01a for employment status),\r\nusing mutate to recode each variable,\r\nusing cut to create a new variable generations from age, to hopefully provide opportunity for deeper analysis.\r\n\r\n\r\n# Here we produce the final data set for regression\r\n# There is an opportunity to rename variables here\r\n\r\ndata_regression<-\r\n  data_recoded %>% \r\n  drop_na() %>% \r\n  rename(home_own = kr03, #home ownership\r\n         employ = tk01a, # employment\r\n         religion = tr11, #religion\r\n         health = kk01, # health\r\n         edu = dl06, # education\r\n         outlook = fo01, # future outlook\r\n         life_sat = sw00, # current life satisfaction\r\n         current_econ = sw01, # current economic step\r\n         future_econ = sw03, # future economic step expectation\r\n         ethnicity = dl01f # ethnicity\r\n         ) %>% \r\n  mutate(ethnicity = as.factor(ethnicity), \r\n         marstat = as.factor(marstat),\r\n         sex = as.factor(sex),\r\n         edu = as.factor(edu),\r\n         employ = as.factor(employ),\r\n         religion = as.factor(religion),\r\n         health = as.factor(health),\r\n         home_own = as.factor(home_own),\r\n         female = as.factor(female)\r\n         ) %>% \r\n  mutate(birth_year = 2014 - age,\r\n         generations = cut(birth_year,\r\n                           breaks = c(1900, 1927, 1945, 1964, 1980, 1996, 2012, 2020),\r\n                           labels = c(\"Greatest Generation\",\r\n                                      \"Silent Generation\",\r\n                                      \"Baby Boomers\",\r\n                                      \"Gen X\",\r\n                                      \"Millenials\",\r\n                                      \"Gen Z\",\r\n                                      \"Gen Alpha\")\r\n                           )\r\n\r\n         )%>% \r\n  select(-hhid14_9, -pidlink, -birth_year) # removed hhid14_9 and pidlink\r\n\r\n\r\nAfter wrangling and removal of missing data, it yielded 28432 responses.\r\nResponse and Explanatory Variables\r\nThe project attempts to answer the question â€œDoes property ownership lead to a better perception of oneself and greater optimism for the future?â€. As a measure of each individualâ€™s outlook for the future, we extracted two variables from the survey:\r\nsw01 (named current_econ): On which economic step do you consider yourself now? Respondents were asked to rate themselves on a scale of 1 (poorest) to 5 (richest).\r\nsw03 (named future_econ): On which economic step do you consider yourself to be in 5 years? Respondents were again asked to rate themselves on a scale of 1 (poorest) to 5 (richest).\r\nFrom these 2 variables, we constructed a response variable Future Outlook (named outlook in the dataset), where:\r\n\r\noutlook = future_econ - current_econ\r\n\r\n\r\nA positive outlook score implies optimism about oneâ€™s future, while a negative score implies pessimism.\r\nWe explored the impact of 10 explanatory variables on oneâ€™s outlook. They are:\r\nhome_own: Home ownership, where a response of 0 indicated that the respondent did not own a property, while a response of 1 indicated property ownership.This is our variable of interest, and serves to answer our research question on the impact of property ownership on our outcome variable.\r\nsex: Gender of respondents were coded 1 for male, and 3 for female.\r\nmarstat: Maritial status of respondents, coded 1 never married, 2 married, 3 separated, 4 divorced, 5 widow(er), 6 cohabitate\r\nage/generations: responded as a numeric variable. In addition to age, we classified each respondent into his/her respective social generation using their year of birth. Greatest Generation (1900 to 1927), Silent Generation (1928 to 1945), Baby Boomers (1946 to 1964), Gen X (1965 to 1980), Millenials also known as Gen Y (1981 to 1996), Gen Z (1997 to 2012), and Generation Alpha (2020 to present).\r\nlife_sat: Life Satisfaction, where responses ranged from 1 for â€œNot at all satisfiedâ€ to 5 for â€œcompletely satisfiedâ€.\r\nedu: Education level of respondent, coded 0 for â€œdonâ€™t know, no schooling, or school for the disabledâ€, 1 for kindergarten, elementary, islamic elementary, 2 for junior high (normal, vocational and Islamic), 3 for senior hi(normal, vocational and Islamic), 4 for college and university (undergraduate, masters, doctorate), and 5 for adult education, open university and Islamic school.\r\nemploy: Employment, coded 0 for â€œunemployedâ€ or â€œprefer not to sayâ€ and 1 for â€œemployedâ€\r\nhealth: Respondents were asked to rank their physical health status. Responses were recoded as 1 for â€œunhealthyâ€, 2 â€œsomewhat unhealthyâ€, 3 â€œsomewhat healthyâ€ and 4 â€œhealthyâ€\r\nreligion: Respondents were asked â€œhow religious do they see themselvesâ€. Reponses were recoded as 0 â€œrefused to sayâ€, 1 â€œnot religiousâ€, 2 â€œsomewhat religiousâ€, 3 â€œreligiousâ€ and 4 â€œvery religiousâ€.\r\nethnicity: We attempted to perform the analysis using ethnicity as a variable. However, as there were over 20 different classifications and sometimes multiple classifications per individual (mixed ethnicity), and given our lack of familiarity with ethnicity in Indonesia, this variable was subsequently dropped.\r\nWe use skim() from the skimr package to quickly provide a broad overview of the data.\r\n\r\n\r\nskim(data_regression)\r\n\r\nTable 1: Data summary\r\nName\r\ndata_regression\r\nNumber of rows\r\n28432\r\nNumber of columns\r\n15\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n10\r\nnumeric\r\n5\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nmarstat\r\n0\r\n1\r\nFALSE\r\n6\r\n2: 20653, 1: 5966, 5: 1128, 4: 554\r\nsex\r\n0\r\n1\r\nFALSE\r\n2\r\n3: 14926, 1: 13506\r\nethnicity\r\n0\r\n1\r\nFALSE\r\n82\r\nA: 12370, B: 3593, I: 1530, H: 1286\r\nedu\r\n0\r\n1\r\nFALSE\r\n6\r\n3: 9628, 1: 8551, 2: 5623, 4: 4190\r\nemploy\r\n0\r\n1\r\nFALSE\r\n2\r\n1: 19042, 0: 9390\r\nreligion\r\n0\r\n1\r\nFALSE\r\n5\r\n3: 17004, 2: 6041, 4: 4635, 1: 719\r\nhealth\r\n0\r\n1\r\nFALSE\r\n4\r\n3: 17183, 2: 5488, 4: 5463, 1: 298\r\nhome_own\r\n0\r\n1\r\nFALSE\r\n2\r\n1: 21092, 0: 7340\r\nfemale\r\n0\r\n1\r\nFALSE\r\n2\r\n1: 14926, 0: 13506\r\ngenerations\r\n0\r\n1\r\nFALSE\r\n6\r\nMil: 11556, Gen: 9427, Bab: 4584, Gen: 2179\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n36.19\r\n14.23\r\n14\r\n25\r\n34\r\n45\r\n101\r\nâ–‡â–‡â–ƒâ–â–\r\nlife_sat\r\n0\r\n1\r\n3.33\r\n0.80\r\n1\r\n3\r\n3\r\n4\r\n5\r\nâ–â–‚â–‡â–‡â–\r\ncurrent_econ\r\n0\r\n1\r\n3.05\r\n0.92\r\n1\r\n3\r\n3\r\n4\r\n6\r\nâ–ƒâ–‡â–…â–â–\r\nfuture_econ\r\n0\r\n1\r\n4.16\r\n1.19\r\n1\r\n3\r\n4\r\n5\r\n6\r\nâ–‚â–…â–‡â–‡â–ƒ\r\noutlook\r\n0\r\n1\r\n1.11\r\n1.00\r\n-5\r\n1\r\n1\r\n2\r\n5\r\nâ–â–â–‡â–ƒâ–\r\n\r\nExploratory Data Analysis\r\nThe mean age of our survey respondents is 36.2 years old. The plot below shows the age distribution of these respondents.\r\n\r\n\r\nage_dist<-\r\n  data_regression %>% \r\n    ggplot(aes(x=age, fill = generations)\r\n           ) +\r\n  geom_histogram(binwidth = 1,\r\n                 alpha = 0.7)+\r\n  geom_vline(aes(xintercept = mean(age)\r\n                 ),\r\n              color = \"blue\")+\r\n  geom_text(aes(x = mean(age)-2, # add a text label for mean age\r\n                y = 500,\r\n                label = glue(\"Mean Age = {round(mean(age),1)}\")\r\n                ),\r\n            color = \"blue\",\r\n            size = 3,\r\n            angle = 90)+ \r\n  scale_x_continuous(name = \"Age\",\r\n                     limits = c(min(data_regression$age), max(data_regression$age)+5),\r\n                     breaks = seq(from = min(data_regression$age), to = max(data_regression$age)+5, by = 5)\r\n                     )+\r\n  labs(title = \"Age Distribution of Survey Respondents\",\r\n       y = \"Number of Respondents\")+\r\n  theme(legend.text = element_text(size = rel(0.6)),\r\n        legend.title = element_blank(),\r\n        legend.position = \"bottom\",\r\n        plot.title = element_text(size = rel(0.9)),\r\n        plot.subtitle = element_text(size = rel(0.8)),\r\n        axis.text.y = element_text(size = rel(0.6)),\r\n        axis.text.x = element_text(angle = 45, size = rel(0.6)),\r\n        axis.title = element_text(size = rel(0.7)\r\n                                  )\r\n        )+\r\n  guides(fill = guide_legend(nrow = 1))\r\n\r\nage_dist\r\n\r\n\r\n\r\nFigure 1: Age Distribution of Survey Respondents\r\n\r\n\r\n\r\nNext, we explored respondentâ€™s outlook by current economic standing.\r\n\r\n\r\noutlook_dist2<-\r\n  data_regression %>% \r\n  ggplot(aes(x=outlook, fill = generations)\r\n         ) +\r\n  geom_histogram(binwidth = 1) +\r\n  geom_vline(aes(xintercept = 0),\r\n             color=\"black\") +\r\n  facet_wrap(~current_econ,\r\n             nrow=2,\r\n             scale = \"free_y\") +\r\n  labs(title = \"Future outlook by perception of current economic standing\",\r\n       y = \"Number of Respondents\")+\r\n  theme(legend.text = element_text(size = rel(0.7)),\r\n        legend.title = element_blank(),\r\n        legend.position = \"bottom\",\r\n        plot.title = element_text(size = rel(0.9)),\r\n        plot.subtitle = element_text(size = rel(0.8)),\r\n        axis.text.y = element_text(size = rel(0.7)),\r\n        axis.text.x = element_text(angle = 45, size = rel(0.7)),\r\n        axis.title = element_text(size = rel(0.7)\r\n                                  )\r\n        )+\r\n  guides(fill = guide_legend(nrow = 1))\r\n\r\noutlook_dist2  \r\n\r\n\r\n\r\nFigure 2: Future Outlook of Respondents by Perception of Current Economic Standing\r\n\r\n\r\n\r\nThere are a couple of interesting observations. A majority of respondents who perceive themselves of low current economic standing (score 1 to 3) are optimistic about their future outlook (score > 0). On the other hand, respondents who perceive themselves of high current economic standing (score 5 and 6) are less optimistic of their future outlook.\r\nLastly, we look at Outlook by Current Economic Standing, for each generation. Here we can see different slopes across some generations, implying that there are interaction effects between age and current_econ. This will be explored later in our model.\r\n\r\n\r\ndata_regression %>% \r\n  ggplot(aes(x = current_econ,\r\n             y = outlook,\r\n             color = generations)\r\n         ) +\r\n  geom_point(position = \"jitter\",\r\n             alpha = 0.1)+\r\n  geom_smooth(method = \"lm\",\r\n              se = FALSE,\r\n              color = \"black\")+\r\n  facet_wrap(.~generations)+\r\n  labs(title = \"Outlook by perception of current economic standing\",\r\n       y = \"Outlook\",\r\n       x = \"Current Economic Standing\")+\r\n  theme(legend.text = element_text(size = rel(0.7)),\r\n        legend.title = element_blank(),\r\n        legend.position = \"none\",\r\n        plot.title = element_text(size = rel(0.9)),\r\n        plot.subtitle = element_text(size = rel(0.8)),\r\n        axis.text.y = element_text(size = rel(0.7)),\r\n        axis.text.x = element_text(angle = 45, size = rel(0.7)),\r\n        axis.title = element_text(size = rel(0.7)\r\n                                  )\r\n        )\r\n\r\n\r\n\r\nFigure 3: Future Outlook of Respondents (facet by Generations) by Perception of Current Economic Standing\r\n\r\n\r\n\r\nCorrelation Analysis\r\nFor the preparation of the model, we created and ran a correlational matrix, to see how our variables of interest (within the model) are related.\r\n\r\n\r\ndata_regression %>% \r\n  select(-ethnicity, -generations) %>% \r\n  as.matrix() %>% \r\n  Hmisc::rcorr() %>% \r\n  broom::tidy() %>% \r\n  rename(variable_1 = column1,\r\n       variable_2 = column2,\r\n       corr = estimate) %>% \r\n  mutate(abs_corr = round(abs(corr),3),\r\n         corr = round(corr,3),\r\n         p.value = round(p.value,3)\r\n         ) %>% \r\n  relocate(abs_corr, .after = corr) %>% \r\n  select(-n) %>% \r\n  arrange(-abs_corr) %>% \r\n  DT::datatable()\r\n\r\n\r\n\r\nWe noted that life satisfaction had positive correlation of around 0.241 with current economic status, there is reason to believe they capture roughly something similar. We will exclude life satisfaction to reduce multicollinearity in our model.\r\nOrdinary Least Square Model\r\nWe ran four regression models.\r\nâ€œBarebonesâ€ Base Model\r\nIn our basic model, we regressed home ownership (home_own) onto outlook (model1), to see the â€˜barebonesâ€™ relationship between home ownership and future outlook.\r\n\\[\r\n\\begin{eqnarray}\r\n\\widehat{outlook} = intercept + b_1home\\_own + \\epsilon\r\n\\end{eqnarray}\r\n\\]\r\n\r\n\r\nmodel1 <-\r\n  lm(outlook ~ home_own,\r\n     data = data_regression)\r\nsummary(model1)\r\n\r\n\r\nCall:\r\nlm(formula = outlook ~ home_own, data = data_regression)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6.0609 -0.2635 -0.0609  0.7365  3.9391 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  1.26349    0.01160  108.88   <2e-16 ***\r\nhome_own1   -0.20256    0.01347  -15.04   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9942 on 28430 degrees of freedom\r\nMultiple R-squared:  0.007888,  Adjusted R-squared:  0.007853 \r\nF-statistic:   226 on 1 and 28430 DF,  p-value: < 2.2e-16\r\n\r\nThe results of the regression analysis was contrary to our expectations (John Steinbeck is WRONG)! Outlook appears to diminish (-0.203) if you own a property. We will investigate if the direction and magnitude of this relationship will continue when control variables are added to the model.\r\nModel with Demographic Control Variables\r\nWe went on to add demographic controls to the modelâ€” such as age and gender.\r\n\\[\r\n\\begin{eqnarray}\r\n\\widehat{outlook} = intercept + b_1home\\_own  + b_2age + b_3female + \\epsilon\r\n\\end{eqnarray}\r\n\\]\r\n\r\n\r\nmodel2 <-\r\n  lm(outlook ~ home_own + age + female,\r\n     data = data_regression)\r\nsummary(model2)\r\n\r\n\r\nCall:\r\nlm(formula = outlook ~ home_own + age + female, data = data_regression)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6.4856 -0.4892 -0.1379  0.5530  4.9017 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  1.876092   0.018318 102.416  < 2e-16 ***\r\nhome_own1   -0.097145   0.013152  -7.386 1.55e-13 ***\r\nage         -0.019318   0.000405 -47.699  < 2e-16 ***\r\nfemale1      0.015765   0.011374   1.386    0.166    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9565 on 28428 degrees of freedom\r\nMultiple R-squared:  0.08183,   Adjusted R-squared:  0.08173 \r\nF-statistic: 844.5 on 3 and 28428 DF,  p-value: < 2.2e-16\r\n\r\nAs age and gender are statistically significant in predicting future economic outlook, it is a good thing we controlled for them. Controlling for these characteristics, the effect size of home ownership increased. Here we use the model_variable_true_regression_function function which we wrote to prepare the models for reporting.\r\n\r\n\r\n# Prepare models for reporting\r\nmodel_used <- list(model1,\r\n                   model2\r\n                   )\r\nlapply(model_used,\r\n       model_variable_true_regression_function)\r\n\r\n[[1]]\r\n# A tibble: 2 Ã— 6\r\n  term             b     SE Z95_x_SE CI_95_upper_bound significance_95\r\n  <chr>        <dbl>  <dbl>    <dbl>             <dbl> <chr>          \r\n1 (Intercept)  1.26  0.0116   0.0227             1.29  sig            \r\n2 home_own1   -0.203 0.0135   0.0264            -0.176 sig            \r\n\r\n[[2]]\r\n# A tibble: 4 Ã— 6\r\n  term            b      SE Z95_x_SE CI_95_upper_bound significance_95\r\n  <chr>       <dbl>   <dbl>    <dbl>             <dbl> <chr>          \r\n1 (Interceâ€¦  1.88   1.83e-2 0.0359              1.91   sig            \r\n2 home_own1 -0.0971 1.32e-2 0.0258             -0.0714 sig            \r\n3 age       -0.0193 4.05e-4 0.000794           -0.0185 sig            \r\n4 female1    0.0158 1.14e-2 0.0223              0.0381 NULL           \r\n\r\nexport_summs(model1, \r\n             model2,\r\n             model.names = c(\"Base Model\",\r\n                             \"Base Model with \\nDemographic Controls\"),\r\n             digits = 3)\r\n\r\n    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n                     Base Model       Base Model with     \r\n                                    Demographic Controls  \r\n                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n      (Intercept)       1.263 ***              1.876 ***  \r\n                       (0.012)Â Â Â              (0.018)Â Â Â   \r\n      home_own1        -0.203 ***             -0.097 ***  \r\n                       (0.013)Â Â Â              (0.013)Â Â Â   \r\n      age                Â Â Â Â Â Â Â Â              -0.019 ***  \r\n                         Â Â Â Â Â Â Â Â              (0.000)Â Â Â   \r\n      female1            Â Â Â Â Â Â Â Â               0.016Â Â Â Â   \r\n                         Â Â Â Â Â Â Â Â              (0.011)Â Â Â   \r\n                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n      N             28432Â Â Â Â Â Â Â Â           28432Â Â Â Â Â Â Â Â   \r\n      R2                0.008Â Â Â Â               0.082Â Â Â Â   \r\n    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n      *** p < 0.001; ** p < 0.01; * p < 0.05.             \r\nColumn names: names, Base Model, Base Model with Demographic Controls\r\n\r\nInitially, we had included marital status but when this variable was explored, we noticed that there are very few observations in some of the categories, this would make its inclusion in the model difficult as there is too little variation within group for analysis to take place.\r\nWe also considered ethnicity but there were some complications in analysing this. Did you know Indonesia has >1000 ethnic groups? Though the Javanese are the largest ethnic group, they do not form the majority. While we considered merging some groups together to reduce the number of groups as well as to increase the number of respondents within each group (some groups were very sparse with only a handful of respondents), without understanding the historical and cultural nuance, it was difficult to merge groups together. In addition, there were individuals who identified as belonging to more than one ethnic group.\r\nModel with Current Economic Status\r\nIn our next model (model3), we added current economic outlook to the model with our two demographic controls. This allows us to compare the effect of home ownership on outlook while controlling for individuals who may be at different economic statuses currently.\r\n\\[\r\n\\begin{eqnarray}\r\n\\widehat{outlook} = intercept + b_1home\\_own + b_2current\\_econ + b_3age + b_4female + \\epsilon\r\n\\end{eqnarray}\r\n\\]\r\n\r\n\r\nmodel3 <-\r\n  lm(outlook ~ home_own + current_econ + age + female,\r\n     data = data_regression)\r\nsummary(model3)\r\n\r\n\r\nCall:\r\nlm(formula = outlook ~ home_own + current_econ + age + female, \r\n    data = data_regression)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.7103 -0.5180 -0.0838  0.5364  4.4047 \r\n\r\nCoefficients:\r\n               Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   2.7309365  0.0252729 108.058  < 2e-16 ***\r\nhome_own1    -0.0625284  0.0126863  -4.929 8.32e-07 ***\r\ncurrent_econ -0.2810045  0.0059494 -47.233  < 2e-16 ***\r\nage          -0.0205986  0.0003909 -52.692  < 2e-16 ***\r\nfemale1       0.0575173  0.0109880   5.235 1.67e-07 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.921 on 28427 degrees of freedom\r\nMultiple R-squared:  0.1486,    Adjusted R-squared:  0.1485 \r\nF-statistic:  1241 on 4 and 28427 DF,  p-value: < 2.2e-16\r\n\r\nModel Substituting Age with Generations\r\nWe took our final model one step further (model3_gen) by replacing age with generations, which allowed us to investigate if there were any changes to outlook by generations.\r\n\r\n\r\nmodel3_gen <-\r\n  lm(outlook ~ home_own + current_econ + generations + female,\r\n     data = data_regression)\r\nsummary(model3_gen)\r\n\r\n\r\nCall:\r\nlm(formula = outlook ~ home_own + current_econ + generations + \r\n    female, data = data_regression)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.6021 -0.4911 -0.0922  0.6299  4.0562 \r\n\r\nCoefficients:\r\n                              Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)                   1.520254   0.247518   6.142 8.26e-10\r\nhome_own1                    -0.061593   0.012769  -4.823 1.42e-06\r\ncurrent_econ                 -0.275801   0.005973 -46.174  < 2e-16\r\ngenerationsSilent Generation -0.300680   0.249642  -1.204  0.22843\r\ngenerationsBaby Boomers      -0.045644   0.247478  -0.184  0.85367\r\ngenerationsGen X              0.401595   0.247293   1.624  0.10439\r\ngenerationsMillenials         0.670277   0.247284   2.711  0.00672\r\ngenerationsGen Z              0.738884   0.247939   2.980  0.00288\r\nfemale1                       0.059333   0.011038   5.376 7.70e-08\r\n                                \r\n(Intercept)                  ***\r\nhome_own1                    ***\r\ncurrent_econ                 ***\r\ngenerationsSilent Generation    \r\ngenerationsBaby Boomers         \r\ngenerationsGen X                \r\ngenerationsMillenials        ** \r\ngenerationsGen Z             ** \r\nfemale1                      ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9242 on 28423 degrees of freedom\r\nMultiple R-squared:  0.1428,    Adjusted R-squared:  0.1426 \r\nF-statistic: 591.9 on 8 and 28423 DF,  p-value: < 2.2e-16\r\n\r\nWhen analyzed across generations rather than age, we observe contrasting effects. Analyzing by generation is essentially a comparison across the means of each group. Grouping the age variable allows comparison across groups which may have experienced different economic conditions and are at different life stages. In this analysis, our reference group is the â€˜Greatest Generation.â€™ This group consists of those born before 1927 and are the oldest group of respondents in the sample.\r\nWe see that the generations born after them are slightly more pessimistic about their future outlook (Silent Generation and Baby Boomers), though this is not statistically significant, hence it is possible that all those born before 1964 do not have notable differences in terms of future outlook.\r\nHowever, when comparing the Greatest Generation with Millenials and Gen Z, we see that there are statistically significant differences. Compared to the Greatest Generation, Millenials are more optimistic about their future outlook. Similarly, Gen Zs are more optimistic than those in Greatest Generation. This may be expected, since younger generations have a longer way ahead of them and possess the energy to carve a road for themselves as compared to the Greatest Generation, who may be in their sunset years and do not see their outlook varying much in the coming years.\r\nModel with Interactions\r\nIn our final model (model4), we consider the interaction effect of age on current economic status. Logically, there is reason to believe there is an interaction effect between these two terms. The effect of current economic status on future economic outlook is likely to differ by age.\r\nFor example, those who are in poorer current economic status may have a more pessimistic outlook, but those who are younger may be more optimistic than those who are older (maybe youthful vigor?). As such, we added this interaction term to capture these effects.\r\n\\[\r\n\\begin{eqnarray}\r\n\\widehat{outlook} = intercept + b_1home\\_own + b_2current\\_econ + b_3age + b_4female + b_5current\\_econ * age + \\epsilon  \r\n\\end{eqnarray}\r\n\\]\r\n\r\n\r\nmodel4 <-\r\n  lm(outlook ~ home_own + age*current_econ + female,\r\n     data = data_regression)\r\nsummary(model4)\r\n\r\n\r\nCall:\r\nlm(formula = outlook ~ home_own + age * current_econ + female, \r\n    data = data_regression)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-5.4906 -0.5291 -0.0693  0.5213  4.7493 \r\n\r\nCoefficients:\r\n                   Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       3.1314562  0.0521275  60.073  < 2e-16 ***\r\nhome_own1        -0.0614472  0.0126699  -4.850 1.24e-06 ***\r\nage              -0.0311293  0.0012611 -24.683  < 2e-16 ***\r\ncurrent_econ     -0.4117695  0.0160326 -25.683  < 2e-16 ***\r\nfemale1           0.0575191  0.0109733   5.242 1.60e-07 ***\r\nage:current_econ  0.0034563  0.0003936   8.781  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9198 on 28426 degrees of freedom\r\nMultiple R-squared:  0.1509,    Adjusted R-squared:  0.1508 \r\nF-statistic:  1011 on 5 and 28426 DF,  p-value: < 2.2e-16\r\n\r\nWe find the interaction term to be statistically significant ğŸ˜€\r\nWe will take a deeper look into this interaction effect in a way that is easier to visualise. Since current economic status runs from 1 to 6, we will treat it as a continuous variable.\r\n\r\n\r\ninteract_plot(model = model4, \r\n              pred= age, \r\n              modx = current_econ,\r\n              interval = T, \r\n              legend.main = \"Current Economic Status\",\r\n              main.title = \"Interaction Plot for Model 4\",\r\n              y.label = \"Future Outlook\",\r\n              x.label = \"Age\")+\r\n  theme(legend.position = \"bottom\")\r\n\r\n\r\n\r\nFigure 4: Interaction Plot of Model 4\r\n\r\n\r\n\r\nWe see that those whose current economic statuses are better, seem to report lower future outlook scores even when compared across the same age. Based on this interaction plot, we see that the interaction effect is strongest when the respondent is young. As they get older, the effect of economic status on future outlook seems to diminish.\r\nSimple Slopes Analysis\r\nWe also ran Simple Slopes Analysis to get a greater understanding of interacting effects in model4, using the sim_slopes() function from the interactions package.\r\n\r\n\r\nsim_slopes(model = model4,\r\n          pred = current_econ, \r\n          modx = age, \r\n          johnson_neyman = T)\r\n\r\nJOHNSON-NEYMAN INTERVAL \r\n\r\nWhen age is OUTSIDE the interval [103.99, 142.81], the slope of\r\ncurrent_econ is p < .05.\r\n\r\nNote: The range of observed values of age is [14.00, 101.00]\r\n\r\nSIMPLE SLOPES ANALYSIS \r\n\r\nSlope of current_econ when age = 21.96236 (- 1 SD): \r\n\r\n   Est.   S.E.   t val.      p\r\n------- ------ -------- ------\r\n  -0.34   0.01   -38.96   0.00\r\n\r\nSlope of current_econ when age = 36.18873 (Mean): \r\n\r\n   Est.   S.E.   t val.      p\r\n------- ------ -------- ------\r\n  -0.29   0.01   -47.97   0.00\r\n\r\nSlope of current_econ when age = 50.41510 (+ 1 SD): \r\n\r\n   Est.   S.E.   t val.      p\r\n------- ------ -------- ------\r\n  -0.24   0.01   -30.71   0.00\r\n\r\nBased on the result, interaction effect is not significant above the age of 103.\r\nReporting of Results\r\nHere is a summary of our four regression models:\r\n\r\n\r\nmodel_used <- list(model1,\r\n                   model2,\r\n                   model3,\r\n                   model4)\r\n\r\nlapply(model_used,\r\n       model_variable_true_regression_function)\r\n\r\n[[1]]\r\n# A tibble: 2 Ã— 6\r\n  term             b     SE Z95_x_SE CI_95_upper_bound significance_95\r\n  <chr>        <dbl>  <dbl>    <dbl>             <dbl> <chr>          \r\n1 (Intercept)  1.26  0.0116   0.0227             1.29  sig            \r\n2 home_own1   -0.203 0.0135   0.0264            -0.176 sig            \r\n\r\n[[2]]\r\n# A tibble: 4 Ã— 6\r\n  term            b      SE Z95_x_SE CI_95_upper_bound significance_95\r\n  <chr>       <dbl>   <dbl>    <dbl>             <dbl> <chr>          \r\n1 (Interceâ€¦  1.88   1.83e-2 0.0359              1.91   sig            \r\n2 home_own1 -0.0971 1.32e-2 0.0258             -0.0714 sig            \r\n3 age       -0.0193 4.05e-4 0.000794           -0.0185 sig            \r\n4 female1    0.0158 1.14e-2 0.0223              0.0381 NULL           \r\n\r\n[[3]]\r\n# A tibble: 5 Ã— 6\r\n  term            b      SE Z95_x_SE CI_95_upper_bound significance_95\r\n  <chr>       <dbl>   <dbl>    <dbl>             <dbl> <chr>          \r\n1 (Interceâ€¦  2.73   2.53e-2 0.0495              2.78   sig            \r\n2 home_own1 -0.0625 1.27e-2 0.0249             -0.0377 sig            \r\n3 current_â€¦ -0.281  5.95e-3 0.0117             -0.269  sig            \r\n4 age       -0.0206 3.91e-4 0.000766           -0.0198 sig            \r\n5 female1    0.0575 1.10e-2 0.0215              0.0791 sig            \r\n\r\n[[4]]\r\n# A tibble: 6 Ã— 6\r\n  term            b      SE Z95_x_SE CI_95_upper_bound significance_95\r\n  <chr>       <dbl>   <dbl>    <dbl>             <dbl> <chr>          \r\n1 (Intercâ€¦  3.13    5.21e-2 0.102              3.23    sig            \r\n2 home_owâ€¦ -0.0614  1.27e-2 0.0248            -0.0366  sig            \r\n3 age      -0.0311  1.26e-3 0.00247           -0.0287  sig            \r\n4 currentâ€¦ -0.412   1.60e-2 0.0314            -0.380   sig            \r\n5 female1   0.0575  1.10e-2 0.0215             0.0790  sig            \r\n6 age:curâ€¦  0.00346 3.94e-4 0.000771           0.00423 sig            \r\n\r\nexport_summs(model1, model2, model3, model4,\r\n             model.names = c(\"Base Model\",\r\n                             \"Base Model with Demographic Controls\",\r\n                             \"Base Model, Demographic Controls,\\nCurrent Economic Outlook\",\r\n                             \"Base Model, Demographic Controls,\\nCurrent Economic Outlook\\nwith interactions\"),\r\n             error_format = \"[{conf.low}:{conf.high}]\",\r\n             digits = 3)\r\n\r\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nBase Model Base Model Base Base\r\nwith Model, Model,\r\nDemographi Demographi Demographi\r\nc Controls c c\r\nControls, Controls,\r\nCurrent Current\r\nEconomic Economic\r\nOutlook Outlook\r\nwith\r\ninteractio\r\nns\r\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n(Intercept 1.263 *** 1.876 *** 2.731 *** 3.131 ***\r\n)\r\n[1.241:1.2 [1.840:1.9 [2.681:2.7 [3.029:3.2\r\n86]Â Â Â  12]Â Â Â  80]Â Â Â  34]Â Â Â \r\nhome_own1 -0.203 *** -0.097 *** -0.063 *** -0.061 ***\r\n[-0.229:-0 [-0.123:-0 [-0.087:-0 [-0.086:-0\r\n.176]Â Â Â  .071]Â Â Â  .038]Â Â Â  .037]Â Â Â \r\nage Â Â Â Â Â Â Â Â  -0.019 *** -0.021 *** -0.031 ***\r\nÂ Â Â Â Â Â Â Â  [-0.020:-0 [-0.021:-0 [-0.034:-0\r\n.019]Â Â Â  .020]Â Â Â  .029]Â Â Â \r\nfemale1 Â Â Â Â Â Â Â Â  0.016Â Â Â Â  0.058 *** 0.058 ***\r\nÂ Â Â Â Â Â Â Â  [-0.007:0. [0.036:0.0 [0.036:0.0\r\n038]Â Â Â  79]Â Â Â  79]Â Â Â \r\ncurrent_ec Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â  -0.281 *** -0.412 ***\r\non\r\nÂ Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â  [-0.293:-0 [-0.443:-0\r\n.269]Â Â Â  .380]Â Â Â \r\nage:curren Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â  0.003 ***\r\nt_econ\r\nÂ Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â  [0.003:0.0\r\n04]Â Â Â \r\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nN 28432Â Â Â Â Â  28432Â Â Â Â Â  28432Â Â Â Â Â  28432Â Â Â Â Â \r\nÂ Â Â  Â Â Â  Â Â Â  Â Â Â \r\nR2 0.008Â Â Â Â  0.082Â Â Â Â  0.149Â Â Â Â  0.151Â Â Â Â \r\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n*** p < 0.001; ** p < 0.01; * p < 0.05.\r\nColumn names: names, Base Model, Base Model with Demographic\r\nControls, Base Model, Demographic Controls, Current Economic Outlook,\r\nBase Model, Demographic Controls, Current Economic Outlook with\r\ninteractions\r\n\r\nThe results of the models can also be shown visually. Here, we use plot_summs to show the estimates of models 1-4 visually.\r\n\r\n\r\nplot_summs(model1, model2, model3, model4,\r\n           plot.distributions = T,\r\n           scale = T,\r\n           model.names = c(\"Base Model\",\r\n                             \"Base Model with Demographic Controls\",\r\n                             \"Base Model, Demographic Controls,\\nCurrent Economic Outlook\",\r\n                             \"Base Model, Demographic Controls,\\nCurrent Economic Outlook\\nwith interactions\"),\r\n           point.size = 2)+\r\n  labs(title = \"Comparing our Model Estimates : Models 1 to 4\")+\r\n  theme(legend.position = \"bottom\",\r\n        legend.text = element_text(size = rel(0.6)),\r\n        legend.title = element_blank(),\r\n        plot.title = element_text(size = rel(0.9)),\r\n        axis.text.y = element_text(size = rel(0.7)),\r\n        axis.text.x = element_text(angle = 45, size = rel(0.7)),\r\n        axis.title = element_text(size = rel(0.7)\r\n                                  )\r\n        )\r\n\r\n\r\n\r\nFigure 5: Comparing Models 1 to 4 Estimates\r\n\r\n\r\n\r\nEvaluating Models using ANOVA\r\nWe analyzed the explanatory power of each model using the ANOVA function. The anova() function will take each modelâ€™s objects as arguments, and return an ANOVA testing whether the more complex model is significantly better than the simpler model. If the resulting p-value is sufficiently low (usually less than 0.05), we conclude that the more complex model is significantly better than the simpler model, and thus favor the more complex model. If the p-value is not sufficiently low (usually greater than 0.05), we should favor the simpler model.\r\nWe start by comparing between model1 and model2.\r\n\r\n\r\nanova(model1, model2) %>% \r\n  tidy()\r\n\r\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nâ”‚ term       df.resid        rss   df      sumsq   statisti  \r\nâ”‚                 ual                                     c  \r\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nâ”‚ outlook    2.84e+04   2.81e+04         Â Â Â Â Â Â Â     Â Â Â Â Â Â Â   \r\nâ”‚ ~                                                          \r\nâ”‚ home_own                                                   \r\nâ”‚ outlook    2.84e+04   2.6e+04Â     2   2.09e+03   1.14e+03  \r\nâ”‚ ~                                                          \r\nâ”‚ home_own                                                   \r\nâ”‚ + age +                                                    \r\nâ”‚ female                                                     \r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nColumn names: term, df.residual, rss, df, sumsq, statistic, p.value\r\n6/7 columns shown.\r\n\r\nThe ANOVA result has a very small p-value (<0.001), indicating that model2 (with addition of age and gender) did lead to a significantly improved fit over model1.\r\nNext we compare between model2 and model3.\r\n\r\n\r\nanova(model2, model3) %>% \r\n  tidy()\r\n\r\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nâ”‚ term       df.resid        rss   df      sumsq   statisti  \r\nâ”‚                 ual                                     c  \r\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nâ”‚ outlook    2.84e+04   2.6e+04Â          Â Â Â Â Â Â Â     Â Â Â Â Â Â Â   \r\nâ”‚ ~                                                          \r\nâ”‚ home_own                                                   \r\nâ”‚ + age +                                                    \r\nâ”‚ female                                                     \r\nâ”‚ outlook    2.84e+04   2.41e+04    1   1.89e+03   2.23e+03  \r\nâ”‚ ~                                                          \r\nâ”‚ home_own                                                   \r\nâ”‚ +                                                          \r\nâ”‚ current_                                                   \r\nâ”‚ econ +                                                     \r\nâ”‚ age +                                                      \r\nâ”‚ female                                                     \r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nColumn names: term, df.residual, rss, df, sumsq, statistic, p.value\r\n6/7 columns shown.\r\n\r\nOnce again, the ANOVA result has a very small p-value (<0.001), indicating that model3 (with addition of current_econ) led to a significantly improved fit over model2.\r\nLastly, we compare between model3 and model4, to ascertain whether adding interaction effects improved the model.\r\n\r\n\r\nanova(model3, model4) %>% \r\n  tidy()\r\n\r\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n  â”‚ term       df.resid        rss   df   sumsq   statisti  \r\n  â”‚                 ual                                  c  \r\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n  â”‚ outlook    2.84e+04   2.41e+04           Â Â          Â Â   \r\n  â”‚ ~                                                       \r\n  â”‚ home_own                                                \r\n  â”‚ +                                                       \r\n  â”‚ current_                                                \r\n  â”‚ econ +                                                  \r\n  â”‚ age +                                                   \r\n  â”‚ female                                                  \r\n  â”‚ outlook    2.84e+04   2.4e+04Â     1    65.2       77.1  \r\n  â”‚ ~                                                       \r\n  â”‚ home_own                                                \r\n  â”‚ + age *                                                 \r\n  â”‚ current_                                                \r\n  â”‚ econ +                                                  \r\n  â”‚ female                                                  \r\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\nColumn names: term, df.residual, rss, df, sumsq, statistic, p.value\r\n6/7 columns shown.\r\n\r\nThe ANOVA test result tells us that model4 (with interaction effects), improves on model3. We will report model4 as our final result.\r\nAssumption Check using GVLMA\r\nWe checked the linearity assumptions of model4 using the Global Validation of Linear Model Assumption (gvlma) package. Unfortunately, the model failed global stats, skewness, kurtosis, link function and heteroskedasticity assumptions.\r\n\r\n\r\ngvlma (model4) \r\n\r\n\r\nCall:\r\nlm(formula = outlook ~ home_own + age * current_econ + female, \r\n    data = data_regression)\r\n\r\nCoefficients:\r\n     (Intercept)         home_own1               age  \r\n        3.131456         -0.061447         -0.031129  \r\n    current_econ           female1  age:current_econ  \r\n       -0.411770          0.057519          0.003456  \r\n\r\n\r\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\r\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\r\nLevel of Significance =  0.05 \r\n\r\nCall:\r\n gvlma(x = model4) \r\n\r\n                     Value   p-value                   Decision\r\nGlobal Stat        2396.96 0.000e+00 Assumptions NOT satisfied!\r\nSkewness            456.70 0.000e+00 Assumptions NOT satisfied!\r\nKurtosis           1868.33 0.000e+00 Assumptions NOT satisfied!\r\nLink Function        58.05 2.565e-14 Assumptions NOT satisfied!\r\nHeteroscedasticity   13.88 1.945e-04 Assumptions NOT satisfied!\r\n\r\nWe tested model3 using gvlma. There was some improvement with link function assumptions being acceptable.\r\n\r\n\r\ngvlma (model3) \r\n\r\n\r\nCall:\r\nlm(formula = outlook ~ home_own + current_econ + age + female, \r\n    data = data_regression)\r\n\r\nCoefficients:\r\n (Intercept)     home_own1  current_econ           age       female1  \r\n     2.73094      -0.06253      -0.28100      -0.02060       0.05752  \r\n\r\n\r\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\r\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\r\nLevel of Significance =  0.05 \r\n\r\nCall:\r\n gvlma(x = model3) \r\n\r\n                       Value   p-value                   Decision\r\nGlobal Stat        2.258e+03 0.0000000 Assumptions NOT satisfied!\r\nSkewness           4.820e+02 0.0000000 Assumptions NOT satisfied!\r\nKurtosis           1.761e+03 0.0000000 Assumptions NOT satisfied!\r\nLink Function      7.020e-03 0.9332272    Assumptions acceptable.\r\nHeteroscedasticity 1.453e+01 0.0001379 Assumptions NOT satisfied!\r\n\r\nWe attempted to transform the data (squaring and log transformation), but it still resulted in â€œAssumptions not satisfiedâ€.\r\nWe concluded that this is likely due to the fact that most of our variables are categorical variables.\r\nHere is a visual representation of the core information of assumption checks for model4.\r\n\r\n\r\nautoplot(gvlma(model4))\r\n\r\n\r\n\r\nCheck Multicollinearity using VIF\r\nWe perform a check for multicollinearity using the Variance Inflation Factor or vif from the car package. Multicollinearity occurs when there is a correlation between multiple independent variables in a multiple regression model. This can negatively impact the regression results. The VIF test measures the correlation and strength of correlation between the independent variables in a regression model.\r\n\r\n\r\nvif(model4,\r\n    type = \"predictor\")\r\n\r\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\n    â”‚ GVIF    Df   GVIF^(1/(2   Interacts    Other      â”‚\r\n    â”‚                   *Df))   With         Predictors â”‚\r\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\r\n    â”‚ 1.03     1         1.02   -- Â          age,       â”‚\r\n    â”‚                                        current_ec â”‚\r\n    â”‚                                        on, female â”‚\r\n    â”‚ 1.04     3         1.01   current_ec   home_own,  â”‚\r\n    â”‚                           on           female     â”‚\r\n    â”‚ 1.04     3         1.01   age          home_own,  â”‚\r\n    â”‚                                        female     â”‚\r\n    â”‚ 1.01     1         1Â Â Â    -- Â          home_own,  â”‚\r\n    â”‚                                        age,       â”‚\r\n    â”‚                                        current_ec â”‚\r\n    â”‚                                        on         â”‚\r\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\nColumn names: GVIF, Df, GVIF^(1/(2*Df)), Interacts With, Other\r\nPredictors\r\n\r\nWithin model4, all 4 variables report a GVIF of close to 1, suggesting that multicollinearity does not exist within the model.\r\nInterpretation of the Results\r\nFrom our data science project, we found the following findings:\r\nThe relationship between oneâ€™s outlook and ownership of property, contrary to our expectations, appears to be a negative one. In our â€œbarebonesâ€ model1, owning a property resulted in a decrease in outlook by -0.203.\r\nThe effects size of home ownership on outlook increased (-0.097) when we added demographic control variables, age and gender, to the model (model2). Age appeared to have a negative effect on outlook (-0.019), while females were marginally more positive than males (0.016). These findings persisted across all models.\r\nThe effect of home ownership on outlook improved further (-0.063) when we considered the respondentâ€™s current economic background. Interestingly, the model suggests that outlook decreased (-0.281) as oneâ€™s current economic status increased. As discussed earlier, this makes sense as a respondent who is already leading a very successful and well life (and indicated it as the maximum score on the scale), would either see it remaining that way (0 change) or it getting worse (negative score). Adding current economic status to our model allows us to compare between groups of differing current economic status (i.e.Â those of the same baseline).\r\nLastly, we concluded that there was an interaction effect between age and current economic status. The effect size of home ownership on outlook increased further (-0.061) and the estimated effect of interaction term age * current_econ is positive (0.003) and significant, though it diminishes with age. Across the same age, those who are worse off now report a better future economic outlook. I guess when you are down, the only way is up :)\r\nThreats to Causal Inference\r\nConfounders: Demographic variables (age and gender) and current economic status were identified as `confounders that could influence both home ownership and outlook. These were controlled and included in the model. Additionally, the interacting effect between age and current economic status was identified and accounted for in our final model4. In spite of our best efforts, there may still be omitted variable bias in our model are likely unobservable variables which we failed to capture. For example, terminal illnesses and degree of autonomy are other factors that could affect future outlook.\r\nColliders: Among the variables we have considered, we did not identify any colliders that we know of. Given that our predictor variable is home ownership, it could be that certain policies which are based on home ownership that could affect the future outlook of respondents are not captured.\r\nCounterfactuals: The role of counterfactuals, ie: what would have happened to your unit of analysis if the treatment had not occurred, could be explored through simulations.\r\nImplications\r\nThe results of our data science project was surprising, as we had expected home ownership to have a positive effect on outlook. The â€œbarebonesâ€ model1 rejected this hypothesis, but the effects decreased after the inclusion of age, gender, and current economic status.\r\nLimitations and Future Directions\r\nIndonesia is a large country with a relatively small urban population of 58 percent (as a percentage of total population). It is ranked 126th in terms of urban population percentage by the World Bank.(https://data.worldbank.org/indicator/SP.URB.TOTL.IN.ZS)\r\nIt would be interesting to expand the scope of this study to other countries or cities with greater urban population percentages. Would the findings be the same if survey respondents lived in land-scarce urbanized cities such as New York, Shanghai, or Singapore where housing is very expensive? The bar to owning a home in rural parts of Indonesia, where land is comparatively much cheaper, may be much lower than in urbanized cities, and hence not have much impact at all on oneâ€™s outlook for the future.\r\nSingapore would be an ideal candidate to repeat this study. In recent years, property prices have skyrocketed and the younger generation are fretting over their ability to purchase their own home. We can already see the Government tweaking it policies to rein in property prices. A study done here could provide the Government with more information about how to further tweak its policies.\r\nGrapes of Wrath was published in 1939, while the IFLS was conducted in 2014-15. Perhaps attitudes towards property ownership might have changed over the course of 75 years? Maybe Steinbeck is correct after all, but in his era of 1930s-40s. Attitudes and importance of home ownership may likely have changed through the years.\r\nLastly, 8,149 observations (22 percent) were â€œdroppedâ€ from the study due to incomplete data, or duplicate entries. This could have impacted the results of the study, or the statistical power of our models. However, we are limited in our ability to handle missing data as we are not involved in study design, data collection, or data entry.\r\nReferences\r\n[RAND Indonesian Family Life Survey] (https://www.rand.org/well-being/social-and-behavioral-policy/data/FLS/IFLS.html).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nRun sessionInfo()\r\n\r\n\r\npackages_utilized <-\r\n  sessionInfo()\r\npackages_utilized\r\n\r\nR version 4.3.2 (2023-10-31 ucrt)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19045)\r\n\r\nMatrix products: default\r\n\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_United States.utf8 \r\n[2] LC_CTYPE=English_United States.utf8   \r\n[3] LC_MONETARY=English_United States.utf8\r\n[4] LC_NUMERIC=C                          \r\n[5] LC_TIME=English_United States.utf8    \r\n\r\ntime zone: Asia/Singapore\r\ntzcode source: internal\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods  \r\n[7] base     \r\n\r\nother attached packages:\r\n [1] interactions_1.1.5  broom.mixed_0.2.9.4 ggstance_0.3.6     \r\n [4] car_3.1-2           carData_3.0-5       sandwich_3.1-0     \r\n [7] ggfortify_0.4.16    gvlma_1.0.0.3       Hmisc_5.1-1        \r\n[10] psych_2.4.1         skimr_2.1.5         modelr_0.1.11      \r\n[13] broom_1.0.5         huxtable_5.5.3      jtools_2.2.2       \r\n[16] DT_0.31             patchwork_1.2.0     ggrepel_0.9.5      \r\n[19] ggthemes_5.0.0      gridExtra_2.3       scales_1.3.0       \r\n[22] transformr_0.1.4    gganimate_1.0.8     rmarkdown_2.25     \r\n[25] knitr_1.45          glue_1.7.0          lubridate_1.9.3    \r\n[28] forcats_1.0.0       stringr_1.5.1       dplyr_1.1.4        \r\n[31] purrr_1.0.2         readr_2.1.5         tidyr_1.3.0        \r\n[34] tibble_3.2.1        ggplot2_3.4.4       tidyverse_2.0.0    \r\n\r\nloaded via a namespace (and not attached):\r\n  [1] rstudioapi_0.15.0   jsonlite_1.8.8      magrittr_2.0.3     \r\n  [4] farver_2.1.1        vctrs_0.6.5         memoise_2.0.1      \r\n  [7] base64enc_0.1-3     htmltools_0.5.7     dials_1.2.0        \r\n [10] progress_1.2.3      Formula_1.2-5       sass_0.4.8         \r\n [13] parallelly_1.36.0   KernSmooth_2.23-22  bslib_0.6.1        \r\n [16] htmlwidgets_1.6.4   fontawesome_0.5.2   zoo_1.8-12         \r\n [19] cachem_1.0.8        lifecycle_1.0.4     pkgconfig_2.0.3    \r\n [22] Matrix_1.6-5        R6_2.5.1            fastmap_1.1.1      \r\n [25] future_1.33.1       digest_0.6.34       colorspace_2.1-0   \r\n [28] furrr_0.3.1         rprojroot_2.0.4     crosstalk_1.2.1    \r\n [31] labeling_0.4.3      fansi_1.0.6         yardstick_1.3.0    \r\n [34] timechange_0.3.0    mgcv_1.9-1          abind_1.4-5        \r\n [37] compiler_4.3.2      proxy_0.4-27        bit64_4.0.5        \r\n [40] withr_3.0.0         pander_0.6.5        htmlTable_2.4.2    \r\n [43] backports_1.4.1     DBI_1.2.1           highr_0.10         \r\n [46] MASS_7.3-60.0.1     lava_1.7.3          classInt_0.4-10    \r\n [49] tools_4.3.2         units_0.8-5         foreign_0.8-86     \r\n [52] future.apply_1.11.1 nnet_7.3-19         nlme_3.1-164       \r\n [55] grid_4.3.2          sf_1.0-15           checkmate_2.3.1    \r\n [58] cluster_2.1.6       generics_0.1.3      lpSolve_5.6.20     \r\n [61] recipes_1.0.9       gtable_0.3.4        tzdb_0.4.0         \r\n [64] class_7.3-22        data.table_1.14.10  hms_1.1.3          \r\n [67] xml2_1.3.6          utf8_1.2.4          pillar_1.9.0       \r\n [70] vroom_1.6.5         splines_4.3.2       tweenr_2.0.2       \r\n [73] lattice_0.22-5      bit_4.0.5           survival_3.5-7     \r\n [76] tidyselect_1.2.0    downlit_0.4.3       bookdown_0.37      \r\n [79] distill_1.6         xfun_0.41           hardhat_1.3.0      \r\n [82] timeDate_4032.109   stringi_1.8.3       DiceDesign_1.10    \r\n [85] yaml_2.3.8          pacman_0.5.1        evaluate_0.23      \r\n [88] codetools_0.2-19    cli_3.6.2           rpart_4.1.23       \r\n [91] repr_1.1.6          munsell_0.5.0       jquerylib_0.1.4    \r\n [94] Rcpp_1.0.12         globals_0.16.2      png_0.1-8          \r\n [97] parallel_4.3.2      ellipsis_0.3.2      gower_1.0.1        \r\n[100] assertthat_0.2.1    prettyunits_1.2.0   gifski_1.12.0-2    \r\n[103] listenv_0.9.0       ipred_0.9-14        prodlim_2023.08.28 \r\n[106] e1071_1.7-14        crayon_1.5.2        rlang_1.1.3        \r\n[109] mnormt_2.1.1       \r\n\r\n#save.image(\"capstone.RData\")\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-05-13T13:20:59+08:00"
    },
    {
      "path": "capstone_Import_data.html",
      "title": "Capstone Project: Does home ownership lead to greater optimism for one's future",
      "author": [],
      "contents": "\r\n\r\nContents\r\nImporting the Data\r\n\r\n\r\n\r\nImporting the Data\r\nThe following code details how we imported, extracted relevant variables, recoded these variables to suit our analysis, and combined them into one dataset named capstone_data.csv.\r\nThis code was separated from the main document as it is a process that only needs to be performed once.\r\n\r\n\r\n# Load necessary packages\r\nlibrary(tidyverse)\r\nlibrary(haven) # Package for reading Stata (DTA) format.\r\nlibrary(skimr)\r\nrm(list = ls())\r\n\r\n## Import individual datasets\r\n\r\n# Import Demographics variable: marital status, gender, age from b3a_cov.dta\r\n# Common link: pidlink\r\nb3a_cov <- read_dta(\"b3a_cov.dta\")\r\nb3a_cov_reduced <-\r\n  b3a_cov %>%\r\n  select(1, 7:9, 32)\r\n\r\n# Import Home Ownership Status kr03\r\n\r\nb2_kr <- read_dta(\"b2_kr.dta\")\r\nb2_kr_reduced <-\r\n  b2_kr %>%\r\n  select(1:2)\r\n \r\n# Import Highest Education Level (dl06) and Ethinic group (dl01f)\r\n\r\nb3a_dl1 <- read_dta(\"b3a_dl1.dta\")\r\nb3a_dl1_reduced <-\r\n  b3a_dl1 %>%\r\n  select(1, 4, 17, 47)\r\n\r\n# Import life satisfaction, current economic outlook, and outlook for future\r\n# sw00, sw01, sw03\r\n\r\nb3a_sw <- read_dta(\"b3a_sw.dta\")\r\nb3a_sw_reduced <-\r\n  b3a_sw %>%\r\n  select(1, 3:4, 6,19) #joined\r\n\r\n# Import employment tk01a\r\n\r\nb3a_tk1 <- read_dta(\"b3a_tk1.dta\")\r\nb3a_tk1_reduced <-\r\n  b3a_tk1 %>%\r\n  select(1, 3, 41) #joined\r\n\r\n\r\n# Import Religion tr11\r\n\r\nb3a_tr <- read_dta(\"b3a_tr.dta\")\r\nb3a_tr_reduced <-\r\n  b3a_tr %>%\r\n  select(1, 16, 40)\r\n\r\n# Import Physical Health Status kk01\r\n\r\nb3b_kk1 <- read_dta(\"b3b_kk1.dta\")\r\nb3b_kk1_reduced <-\r\n  b3b_kk1 %>%\r\n  select(1, 3, 13)\r\n\r\n\r\nHere we combine the separate datasets into 1. This was done in a step-wise process, at each step checking for integrity.\r\n\r\n\r\n# Combining data\r\n# We will mutate and rename variables later\r\n\r\n# # Data combined: age, maritial status, sex, ethnicity, education\r\n# # Ethnicity added 20 Dec\r\ndata_combined <-\r\n  full_join(b3a_cov_reduced, b3a_dl1_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data combined_1: age, maritial status, sex, ethnicity, education\r\n# # Add: sw00, sw01, sw03\r\ndata_combined_1 <-\r\n  full_join(data_combined,b3a_sw_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_2: age, maritial status, sex, education, sw00, sw01, sw03\r\n# # Add: employment tk01a\r\n\r\ndata_combined_2 <-\r\n  full_join(data_combined_1, b3a_tk1_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_3: age, maritial status, sex, education, sw00, sw01, sw03, employment tk01a\r\n# # Add religion tr11\r\n\r\ndata_combined_3 <-\r\n  full_join (data_combined_2, b3a_tr_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_4: age, maritial status, sex, education, sw00, sw01, sw03, employment tk01a, religion tr11, \r\n# # Add health kk01\r\n\r\ndata_combined_4 <-\r\n  full_join (data_combined_3, b3b_kk1_reduced, by = \"pidlink\") %>%\r\n  select(-hhid14_9.y) %>%\r\n  rename(hhid14_9 = hhid14_9.x)\r\n\r\n# # Data_combined_5: age, maritial status, sex, education, sw00, sw01, sw03, employment tk01a, religion tr11, health kk01\r\n# # Add: home ownership kr03\r\n\r\ndata_combined_5 <-\r\n  full_join(data_combined_4, b2_kr_reduced, by = \"hhid14_9\") # note joined by hhid14_9 (household level) instead of pidlink (individual level)\r\n\r\n# data_combined_5 is the final data set comprising:\r\n# age, maritial status, sex, education, sw00, sw01, sw03,\r\n# employment tk01a, religion tr11, health kk01, home ownership kr03\r\n# \r\nskim(data_combined_5)\r\n\r\n(#tab:Combining Dataset)Data summary\r\nName\r\ndata_combined_5\r\nNumber of rows\r\n36581\r\nNumber of columns\r\n14\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n3\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nhhid14_9\r\n0\r\n1.00\r\n9\r\n9\r\n0\r\n15350\r\n0\r\npidlink\r\n190\r\n0.99\r\n8\r\n9\r\n0\r\n36391\r\n0\r\ndl01f\r\n2117\r\n0.94\r\n0\r\n5\r\n2804\r\n85\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n196\r\n0.99\r\n38.50\r\n18.92\r\n14\r\n26\r\n35\r\n48\r\n998\r\nâ–‡â–â–â–â–\r\nmarstat\r\n196\r\n0.99\r\n2.05\r\n0.94\r\n1\r\n2\r\n2\r\n2\r\n6\r\nâ–‡â–â–â–â–\r\nsex\r\n196\r\n0.99\r\n2.03\r\n1.00\r\n1\r\n1\r\n3\r\n3\r\n3\r\nâ–‡â–â–â–â–‡\r\ndl06\r\n3904\r\n0.89\r\n17.07\r\n25.52\r\n2\r\n2\r\n5\r\n6\r\n99\r\nâ–‡â–â–â–‚â–\r\nsw00\r\n4926\r\n0.87\r\n2.68\r\n0.81\r\n1\r\n2\r\n3\r\n3\r\n9\r\nâ–†â–‡â–â–â–\r\nsw01\r\n4926\r\n0.87\r\n3.05\r\n1.01\r\n1\r\n3\r\n3\r\n4\r\n9\r\nâ–ƒâ–‡â–â–â–\r\nsw03\r\n4926\r\n0.87\r\n4.36\r\n1.49\r\n1\r\n3\r\n4\r\n5\r\n9\r\nâ–‚â–‡â–…â–‚â–\r\ntk01a\r\n2148\r\n0.94\r\n1.69\r\n0.95\r\n1\r\n1\r\n1\r\n3\r\n8\r\nâ–‡â–…â–â–â–\r\ntr11\r\n4987\r\n0.86\r\n2.10\r\n0.72\r\n1\r\n2\r\n2\r\n2\r\n7\r\nâ–‡â–‚â–â–â–\r\nkk01\r\n2310\r\n0.94\r\n2.05\r\n0.68\r\n1\r\n2\r\n2\r\n2\r\n4\r\nâ–‚â–‡â–â–ƒâ–\r\nkr03\r\n441\r\n0.99\r\n1.71\r\n4.33\r\n1\r\n1\r\n1\r\n1\r\n95\r\nâ–‡â–â–â–â–\r\n\r\nIn this next step we recoded the variables to suit our analysis.\r\n\r\n\r\n# This next section will focusing recoding the data. Renaming each variable will be performed in the main script.\r\n\r\n# Recoding the data ----\r\nnames(data_combined_5)\r\n\r\n [1] \"hhid14_9\" \"age\"      \"marstat\"  \"sex\"      \"pidlink\"  \"dl01f\"   \r\n [7] \"dl06\"     \"sw00\"     \"sw01\"     \"sw03\"     \"tk01a\"    \"tr11\"    \r\n[13] \"kk01\"     \"kr03\"    \r\n\r\n# Please see below for a brief description of each variable.\r\ndata_recoded <-\r\n  data_combined_5 %>%\r\n  mutate(sw00 = as.numeric(case_when(sw00 == 5 ~ \"1\", # Recode sw00. 1: Not at all satisfied\r\n                          sw00 == 4 ~ \"2\",\r\n                          sw00 == 3 ~ \"3\",\r\n                          sw00 == 2 ~ \"4\",\r\n                          sw00 == 1 ~ \"5\", #5 will be \"completely satisfied\"\r\n                          sw00 == 9 ~ NA_character_)\r\n                          ),\r\n         sw01 = as.numeric(case_when(sw01 == 8 ~ NA_character_, # 8: (172) dont know, coded as NA to be removed later\r\n                                     sw01 == 9 ~ NA_character_, # 9: (2)maybe data entry error, found a couple of 9\r\n                                     .default = as.character(sw01)\r\n                                     )\r\n                           ),\r\n         sw03 = as.numeric(case_when(sw03 == 8 ~ NA_character_, # 8: (1882)dont know, coded as NA to be removed later\r\n                                     sw03 == 9 ~ NA_character_, # 9: (3) maybe data entry error, found a couple of 9\r\n                                     .default = as.character(sw03)\r\n                                     )\r\n                           ),\r\n         kr03 = as.factor(case_when(kr03 == 1 ~ \"1\", #own\r\n                                    kr03 == 2 ~ \"0\", # occupying\r\n                                    kr03 == 5 ~ \"0\", # rented\r\n                                    kr03 == 95 ~ \"0\" # others\r\n                                    )\r\n                          ),\r\n         tk01a = as.factor(case_when(tk01a == 1 ~ \"1\", # employed\r\n                                     tk01a == 3 ~ \"0\", # unemployed\r\n                                     tk01a == 8 ~ \"0\" # prefer not to say\r\n                                     )\r\n                           ),\r\n         # tr11: Respondents are asked to rate how religious they see themselves\r\n         tr11 = as.factor(case_when(tr11 == 7 ~ \"0\", # refused to say\r\n                                    tr11 == 4 ~ \"1\", # not religious\r\n                                    tr11 == 3 ~ \"2\", # somewhat religious\r\n                                    tr11 == 2 ~ \"3\", # religious\r\n                                    tr11 == 1 ~ \"4\" # very religious\r\n                                    )\r\n                          ),\r\n         kk01 = as.factor(case_when(kk01 == 4 ~ \"1\", # unhealthy\r\n                                    kk01 == 3 ~ \"2\", # somewhat unhealthy\r\n                                    kk01 == 2 ~ \"3\", # somewhat healthy\r\n                                    kk01 == 1 ~ \"4\" # healthy\r\n                                    )\r\n                          ), # Education recoded to 5 broad levels.\r\n         dl06 = as.factor(case_when(dl06 == 99 ~ \"0\", # dont know, others, missing, school for disabled\r\n                                    dl06 == 98 ~ \"0\",\r\n                                    dl06 == 95 ~ \"0\",\r\n                                    dl06 == 17 ~ \"0\",\r\n                                    dl06 == 90 ~ \"1\", # kindergarten, elementary, islamic elementary\r\n                                    dl06 == 2 ~ \"1\",\r\n                                    dl06 == 72 ~ \"1\",\r\n                                    dl06 == 3 ~ \"2\", # junior high, jh vocational, islamic jh\r\n                                    dl06 == 4 ~ \"2\",\r\n                                    dl06 == 73 ~ \"2\",\r\n                                    dl06 == 5 ~ \"3\", # senior high, sh vocational, islamic sh\r\n                                    dl06 == 6 ~ \"3\",\r\n                                    dl06 == 74 ~ \"3\",\r\n                                    dl06 == 60 ~ \"4\", # college, university (under, master, doctorate)\r\n                                    dl06 == 61 ~ \"4\",\r\n                                    dl06 == 62 ~ \"4\",\r\n                                    dl06 == 63 ~ \"4\",\r\n                                    dl06 == 11 ~ \"5\", # adult ed, open uni, islamic school\r\n                                    dl06 == 12 ~ \"5\",\r\n                                    dl06 == 13 ~ \"5\",\r\n                                    dl06 == 14 ~ \"5\",\r\n                                    dl06 == 15 ~ \"5\"\r\n                                    )\r\n                          ),\r\n         age = as.numeric(age), # age as numeric\r\n         marstat = as.factor(marstat), # marital status as factor\r\n         female = as.factor(case_when(sex == 1 ~ \"0\", # sex=1 means male, 3=female, so i made male the baseline\r\n                                   sex == 3 ~ \"1\"\r\n                                   )\r\n                         ),\r\n         fo01 = sw03 - sw01 # Outlook score\r\n         )\r\nglimpse(data_recoded)\r\n\r\nRows: 36,581\r\nColumns: 16\r\n$ hhid14_9 <chr> \"001060000\", \"001060004\", \"001060000\", \"001060000\",â€¦\r\n$ age      <dbl> 59, 28, 39, 16, 30, 36, 26, 40, 55, 54, 34, 28, 24,â€¦\r\n$ marstat  <fct> 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 5, â€¦\r\n$ sex      <dbl+lbl> 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3,â€¦\r\n$ pidlink  <chr> \"001060001\", \"001060004\", \"001060007\", \"001060008\",â€¦\r\n$ dl01f    <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"â€¦\r\n$ dl06     <fct> 1, 1, 1, 2, 2, 1, 1, NA, 1, 1, 4, 4, 4, 4, 1, 2, 1,â€¦\r\n$ sw00     <dbl> 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 1, 2, 2, 2, 2, 4, 3, â€¦\r\n$ sw01     <dbl> 3, 2, 3, 3, 2, 2, 2, 2, 4, 3, 2, 2, 1, 3, 2, 3, 3, â€¦\r\n$ sw03     <dbl> 4, 2, 3, 3, 2, 2, 3, 2, NA, 2, 2, 3, 5, 5, 3, 5, NAâ€¦\r\n$ tk01a    <fct> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, â€¦\r\n$ tr11     <fct> 3, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, â€¦\r\n$ kk01     <fct> 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 2, â€¦\r\n$ kr03     <fct> 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, â€¦\r\n$ female   <fct> 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, â€¦\r\n$ fo01     <dbl> 1, 0, 0, 0, 0, 0, 1, 0, NA, -1, 0, 1, 4, 2, 1, 2, Nâ€¦\r\n\r\n# DATA WRANGLING COMPLETE \r\n# saved as csv file. \r\n\r\n#write_csv(data_recoded, \"capstone_data.csv\")\r\n\r\n\r\nAgain, the above processes need only be done once. Thereafter, the dataset should be imported for analysis using a simple read_csv(capstone_data.csv) command. Iâ€™m sure youâ€™d like to return to the main document now and read all about our analysis. You may do so by clicking on this link\r\n\r\n\r\n\r\n",
      "last_modified": "2024-05-13T13:20:59+08:00"
    },
    {
      "path": "gallery.html",
      "title": "gallery",
      "author": [],
      "contents": "\r\n\r\nContents\r\nGallery of Images\r\n\r\nGallery of Images\r\nThis software â€œminiturizesâ€ the images into thumbnails automatically.\r\nWhen you click on individual images, it â€œpops-upâ€ in a lightbox.\r\n\r\n\r\n\r\nOr it can be arranged as a scrollbar.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:07+08:00"
    },
    {
      "path": "index.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n\r\n\r\nWelcome! And thank you for stopping by. This blog was created using Distill for R Markdown and written entirely in R Studio. Hopefully, it will be a growing showcase of all projects related to R which I embark on. \r\n2023 felt really hot in Singapore? Letâ€™s see if thatâ€™s really true.\r\n\r\n\r\nSee the code here\r\nNumber of visitors since inception:\r\n\r\n\r\n\r\nThis site is hosted FREE by Netlify.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:07+08:00"
    },
    {
      "path": "ml.html",
      "title": "Machine LeaRning",
      "description": "ML\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nI started Module 1 of SMU Academyâ€™s Predictive Analytics and Machine Learning class in January 2024. This blog will serve as a repository for various Machine Learning projects I embark on.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-05-13T13:20:58+08:00"
    },
    {
      "path": "nikkei225.html",
      "title": "Nikkei225 Gamma Exposure Profile",
      "description": "Gamma Exposure Profile\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n\r\nCheck here for the article on â€œHow to calculate Gamma Exposure Profileâ€\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:08+08:00"
    },
    {
      "path": "snowman2023.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\n\r\nI made this snowman in Dec 2023 to celebrate Christmas and to learn gganimate. It may not look like much, but it did take a number of hours and about 100 lines of code. Enjoy!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:12+08:00"
    },
    {
      "path": "spiral.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\n\r\nI made this spiral animation as part of my first exploration into digital art using R.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:12+08:00"
    },
    {
      "path": "temperature_spiral.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\n\r\nDo you think its been getting hotter and hotter in recent years? What about rain? Have we been getting more erratic rainfall in recent years? Hereâ€™s a cool way to visualize both.\r\nI stumbled upon Pat Schlossâ€™ youtube channel the other day, while searching for R tutorials online. I watched his video on creating an animated climate spiral, and was inspired to try and do the same, using data for Singapore.\r\nThis is the link to Patâ€™s blog. I â€œborrowedâ€ his idea of using â€œnext_janâ€ so that the lines for December and January would touch and appear seamless. Thanks Pat! This is the climate spiral created by the smart folks at NASAâ€™s Scientific Visualization Studio.\r\nHere is my attempt at creating a temperature spiral for Singapore. It is slightly different from Patâ€™s. My temperature lines are colored by temperature difference, rather than by year.\r\n\r\nThis is a different way of visualizing the same data. Which is more effective? Which do you prefer?\r\n\r\n\r\nI didnt stop there. Since there was rainfall data available, I created something similar for rainfall in Singapore.\r\n\r\nThe code for generating the temperature spiral. WARNING! It takes while to render, which is why Iâ€™ve included them as animated gifs in this page, rather than having the code render them â€œon the flyâ€. Data was obtained from data.gov.sg. Wouldnâ€™t it be great to have data going back to the early 1900s?\r\nYou may use or modify the code for your own animation, but I would greatly appreciate a link back, please.\r\n\r\n\r\n# Remove all objects in workspace ----\r\nrm(list = ls())\r\n\r\npacman::p_load(tidyverse, lubridate, glue, ggthemes, readr, gganimate, gifski, magick)\r\n\r\ndf <- read_csv(\"SurfaceAirTemperatureMonthlyMean.csv\") #obtain data from data.gov.sg\r\n\r\ndata <-\r\n  df %>% \r\n  separate_wider_delim(month,\r\n                       delim = \"-\",\r\n                       names = c(\"year\", \"mth\")\r\n                       ) %>% \r\n  as.tibble() %>% \r\n  mutate(year = as.numeric(year),\r\n         mth = as.numeric(mth),\r\n         month = month.abb[mth]\r\n         )\r\n\r\ndf_8289 <- \r\n  data %>% \r\n  filter(year >= 1982 & year <= 1989) %>% \r\n  group_by(mth) %>% \r\n  summarise(m_temp = mean(mean_temp))\r\n\r\ndata_joined <-\r\n  left_join(data, df_8289, by = \"mth\") %>% \r\n  mutate(temp_dev = round((mean_temp - m_temp), digits = 2))\r\n  \r\nnext_jan <-\r\n  data_joined %>% \r\n  filter(month == \"Jan\") %>% \r\n  mutate(year = year -1,\r\n         month = \"next_jan\")\r\n\r\ntemp_data <-\r\n  bind_rows(data_joined, next_jan) %>% \r\n  mutate(month = factor(month,\r\n                        levels = c(month.abb, \"next_jan\")\r\n                        ),\r\n         mth = as.numeric(month)\r\n         ) \r\n\r\np <- \r\n  ggplot(data = temp_data,\r\n       aes(x = mth,\r\n           y = temp_dev,\r\n           group = year)\r\n       ) +\r\n  geom_line(aes(color = temp_dev) #plot lines for each year\r\n            )+\r\n  geom_hline(yintercept = c(-1:2), # concentric lines indicating scale\r\n             color = \"grey\")+\r\n  geom_label(aes(x = 12, # year in the middle\r\n                y = -2.8,\r\n                label = year), \r\n            fill = \"black\",\r\n            color = \"white\",\r\n            size = 9,\r\n            label.size = 0) +\r\n  geom_label(aes(x = 12, #labels for each concentric line\r\n                 y = -1),\r\n             label = \"-1 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"blue\",\r\n             label.size = 0,\r\n             size = 5) +\r\n  \r\n  geom_label(aes(x = 12,\r\n                 y = 0),\r\n             label = \"+0 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"yellow\",\r\n             label.size = 0,\r\n             size = 5) +\r\n  \r\n  geom_label(aes(x = 12,\r\n                 y = 1),\r\n             label = \"+1 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"orange\",\r\n             label.size = 0,\r\n             size = 5) +\r\n  geom_label(aes(x = 12,\r\n                 y = 2),\r\n             label = \"+2 \\u00B0C\",\r\n             fill = \"black\",\r\n             color = \"red\",\r\n             label.size = 0,\r\n             size = 5) +\r\n\r\n  coord_polar(start = 2*pi/12)+\r\n  scale_x_continuous(breaks = 1:12,\r\n                     labels = month.abb\r\n                     ) +\r\n  scale_y_continuous(breaks = seq(-1.5, 2.5, 0.2),\r\n                     limits = c(-3, 2.5)\r\n                     )+\r\n  scale_color_gradient(low = \"blue\",\r\n                       high = \"red\")+\r\n  labs(title = \"Temperature change in Singapore (1982-2023)\",\r\n       subtitle = \"Monthly Mean Surface Air Temperature.\",\r\n       caption = \"Data from https//data.gov.sg\\n\r\n       Baseline: Average Monthly Temperature from 1982-1989\")+\r\n  theme(legend.position = \"none\",\r\n        plot.title = element_text(color = \"white\", \r\n                                  size = rel(1.2)),\r\n        plot.subtitle = element_text(color = \"white\",\r\n                                    size = rel(1)),\r\n        plot.caption = element_text(color = \"white\",\r\n                                    size = rel(0.8)),\r\n        axis.text.x = element_text(color = \"white\",\r\n                                   size = rel(2)),\r\n        axis.text.y = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        panel.background = element_rect(fill = \"black\"),\r\n        panel.grid = element_blank(),\r\n        plot.background = element_rect(fill = \"black\")\r\n        ) +\r\n  transition_time(year)+\r\n  shadow_mark(past = TRUE, alpha = 0.9)\r\nanimate(p, fps = 10, nframes = 200, end_pause = 60)\r\nanim_save(\"name_of_your_choice\", p)\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:12+08:00"
    },
    {
      "path": "thankyouprofroh.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\n\r\nIâ€™ve finally graduated from SMUâ€™s Certified Data Analytics (R) Specialist course. It was an amazing 5 months, full of ups and downs. I kept my fingers crossed over the first three modules of the course, praying not to fail at each assessment. My code was plagued with errors which took many many hours to resolve.\r\nIt wasnâ€™t till November 2023 that I was able to follow along in class, and complete each lesson without any coding errors. I made this animation as a graduation present for Professor Roh. He is an amazing person.\r\n\r\nProf Roh: Thank you for inspiring me in this learning journey. Your energy and enthusiasm kept me going. Hereâ€™s my graduation present to you.\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe code for generating this animation can be found below. The most difficult part I experienced was writing the code for the fireworks display. Yes, the colorful dots at the top of the animation are about 10 clusters of fireworks. I used a kmeans algorithm to â€œpartitionâ€ a selection of â€œsparklesâ€ into clusters about a center. Subsequently, I had to figure out how to sequence the appearance of each sparkle. The desired effect I wanted to achieve was for each sparkle closest to their respective center to appear.\r\nYou may use or modify the code for your own animation, but I would greatly appreciate a link back, please.\r\n\r\n\r\n# Clear global environment ----\r\nrm(list = ls())\r\n# Load packages ----\r\nlibrary(tidyverse)\r\nlibrary(imager) #package for image processing\r\nlibrary(gganimate)\r\nlibrary(randomcoloR) #generate random colors\r\n\r\n#set.seed(20240114)\r\n\r\n# Read in coordinates of photo ----\r\ndf <- read_csv(\"prof_rohV2_coord.csv\")\r\ndf_length <- nrow(df)\r\ndf <-\r\n  df%>% \r\n  rename(order = n) %>% \r\n  mutate(order = rep(1:72, length.out = df_length))\r\n# 72 represents what WAS the max sparkles per cluster. In the next revision \r\n# this will be the first item to fix. Unfortunately, this project was assembled\r\n# 3 parts (photo, fireworks, thank you) in a very short time\r\n\r\n# Find min max x,y of photo to determine subsequent position of thank you\r\n# and fireworks elements.\r\nxmin <- min(df$x)\r\nxmax <- max(df$x)\r\nymin <- min(-df$y)\r\nymax <- max(-df$y)\r\n\r\n\r\n# Creating fireworks ----\r\n# Generate data for fireworks\r\n\r\ni <- 10 # number of clusters\r\ns <- 50 # number of sparkles per center\r\n\r\n# generate 10,000 random dots per cluster\r\ndots <-\r\n  tibble(x = runif(n = 10000*i, min = xmin, max = xmax),\r\n         y = runif(n = 10000*i, min = ymax, max = 100)) #adjust relative positioning\r\n# s * i gives number of dots per display\r\n\r\n\r\n# generate color table. c controls color, cluster_n is the cluster number\r\n# you could always specify your own colors\r\ncolor_table <-\r\n  tibble(cluster_n = 1:i,\r\n         c = randomColor(i)\r\n         )\r\n# sample s*i number of sparkles per cluster of fireworks\r\nsparkles <-\r\n  tibble(x = sample(dots$x, s*i),\r\n         y = sample(dots$y, s*i)\r\n         )\r\n# use kmeans algorithm to partition sparkles to each center\r\nkm <- kmeans(sparkles,\r\n             centers = i,\r\n             iter.max = 50,\r\n             nstart = 1)\r\n\r\n# Information on centers of each fireworks cluster\r\ncenter <- \r\n  tibble(c_x = km$centers[,1],\r\n         c_y = km$centers[,2]) %>% \r\n  mutate(cluster_n = 1:n())\r\n\r\n# Information on cluster allocation ie: tells you which sparkle is assigned to each cluster\r\ncluster <- \r\n  tibble(cluster_n = km$cluster)\r\n\r\n# generate the fireworks by assembling all plot info\r\nfireworks <-\r\n  bind_cols(sparkles, cluster) %>% # sparkles assigned to cluster\r\n  left_join(., color_table, by = \"cluster_n\") %>% # color assigned to cluster\r\n  left_join(., center, by = \"cluster_n\") %>% # center assigned to cluster\r\n  # next 4 lines of code have to do with sequencing of sparkles for animation\r\n  # i wanted sparkles closest to each center to start together in the animation\r\n  mutate(distance = sqrt((x - c_x)^2 + (y - c_y)^2)) %>% #calculate distance between sparkle and center\r\n  group_by(cluster_n) %>% \r\n  arrange(distance) %>% #arrange by ascending distance\r\n  mutate(order = 1:n()) %>% # ordering for animation sequence\r\n  ungroup()\r\n\r\n\r\n# Read in coordinates of thank you ----\r\ndf_t <- read_csv(\"thankyou.csv\")\r\ndf_t_length <- nrow(df_t)\r\ndf_t <-\r\n  df_t %>% \r\n  rename(order = n) %>% \r\n  mutate(order = rep(1:72, length.out = df_t_length))\r\n\r\n# Finally we plot ----\r\np <-\r\n  ggplot() +\r\n  # plot the face\r\n  geom_point(data = df,\r\n             aes(x = x,\r\n                 y = -y, # right-side the image. imagine reflecting off x-axis\r\n                 group = order),\r\n             size = 1.25,\r\n             color = \"white\") + # change back to white\r\n  # plot fireworks\r\n  geom_point(data = fireworks %>% filter(cluster_n == 1), #one geom per cluster\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 2),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 3),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 4),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 5),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 6),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 7),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 8),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 9),\r\n             aes(x = x, y = y, color = c), size = 2) +\r\n  geom_point(data = fireworks %>% filter(cluster_n == 10),\r\n             aes(x = x, y = y, color = c), size = 2)+\r\n  # plot thank you\r\n  geom_point(data = df_t,\r\n             aes(x = x,\r\n                 y = -y+90, # positioning adjustment for relative positioning of items\r\n                 group = order),\r\n             size = 2,\r\n             color = \"white\") +\r\n  theme_set(theme_void()) + \r\n  theme(panel.background = element_rect(fill = 'black'),\r\n        legend.position = \"none\",\r\n        # rmarkdown refused to knit without gridlines or recognize theme_void() so I am removing them manually\r\n        axis.text.x = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        panel.border = element_blank(),\r\n        panel.grid = element_blank(),\r\n        axis.title.x = element_blank(),\r\n        axis.title.y = element_blank()\r\n        ) +\r\n  # set animation transition\r\n  transition_time(order)+\r\n  shadow_mark(alpha = 0.8, size = rel(0.8))\r\n\r\n# Render animation ----\r\nanimate(p, fps = 20, nframes = 200, end_pause = 60, height = 600, width = 600)\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:12+08:00"
    },
    {
      "path": "whatsapp.html",
      "title": "maRk's blog",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntroduction\r\nProject\r\nExporting Whatsapp Chat\r\nImport and Data Wrangling\r\nData Visualization\r\nTokenize the words\r\nAFINN, BING and NRC\r\nLets do some very basic EDA\r\nHave fun with word cloud\r\nSentiment Analysis using AFINN Index\r\nCumulative Average Daily Sentiment\r\nSentiment Analysis: Marc vs Andrew\r\nTokenizing by n-grams\r\nSentiment Analysis using Bing\r\nNet Sentiment Using Bing\r\nSentiment Analysis using NRC\r\nFinal Thoughts\r\n\r\n\r\n\r\nIntroduction\r\nIâ€™m attending the Certified Data Analytics (R) Specialist course taught by Professor Sungjong Roh at SMU Academy. In Module 3: Web scraping and Data Insights, we learnt the basics of Natural Language Processing (NLP) and Sentiment Analysis of text.\r\nI was particularly intrigued by the topic of Sentiment Analysis, and furthered my knowledge by reading the book â€œText Mining with R: A Tidy Approachâ€ by Julia Silge & David Robinson.\r\n\r\nYou may access read the book by following this link.\r\nProject\r\nTo further hone my skills in R, basic natural language processing (NLP) and sentiment analysis, I thought that it would be cool to do a sentiment analysis of my familyâ€™s Whatsapp group chat. This group chat was started in June 2019 and comprises 4 members, my wife (Angelina) , my two sons Marc (age 18) and Andrew (age 13) and me.\r\nThe chatgroup contains approximately 65,000 messages, and is used mainly to coordinate schedules, update whereabouts of each member, and chatting about anything under the sun. My end goal is to see if the tools I learnt in class could be used to provide an insight into the:\r\noverall sentiment of our chatgroup. Do we generally use positive language (I hope so) or is the group environment toxic (that would be a disappointment!).\r\nindividual sentiment of each member of the chatgroup. Are there members who are positive/negative and does that â€œgelâ€ with my knowledge of their personality? Were there periods of positive or negative sentiment throughout the year(s), and does that tie-in with any significant events the family was experiencing?\r\nExporting Whatsapp Chat\r\nThis is fairly easy to do, and the chat exports as a single text file with date/timestamp. Unfortunately, stickers and emojis which are often used in our chatgroup cannot be exported.\r\nImport and Data Wrangling\r\nThe chat file can be imported into R using read_csv(). The chat.txt file imported from Whatsapp was relatively â€œcleanâ€, and required minimal wrangling.\r\nThe date of message sent, name of sender, and text message were extracted using a combination of str_extract() and regex patterns. Date was subsequently formatted using dmy(). NA data was filtered using na.omit(). Finally, a â€œmessage_idâ€ was assigned to each row using the function rowid_to_column()\r\nData Visualization\r\nLetâ€™s find out who sent the most messages within the chatgroup.\r\n\r\nMirror mirror on the wall, whoâ€™s the most chatty of them all?\r\n\r\nAnyone wantâ€™s to make a guess?\r\nSurprise surpise! My wife sent the most messages within the group! Letâ€™s take a look at the number of daily messages sent in the chatgroup since its inception.\r\n\r\n\r\n\r\nFigure 1: Daily Number of Messages Sent in chatgroup (2019 to date)\r\n\r\n\r\n\r\nInteresting! The number of daily number of messages within the chatgroup increased after 2022. Perhaps as life returned to a â€œpost-covid normalâ€, messaging within the group increased as we were â€œout and aboutâ€ much more, and required more coordination amidst our activities. Or maybe we were just more active on our phones?\r\nTokenize the words\r\nIn order to run sentiment analysis on the chat, each message needed to be parsed into individual words. This process of â€œtokenizingâ€ the words is easily done within R using the function unnest_tokens(). This resulted in approximately 515,000 tokens.\r\nNext, I removed â€œstop-wordsâ€ using tidytext::stop_words and anti_join(). These stop-words carry â€œno meaningâ€ in sentiment analysis and are therefore removed prior to analysis. You are also able to remove names or other words you deem irrelevant by adding them to a custom lexicon. You will read more about that laterâ€¦\r\nAFINN, BING and NRC\r\nHereâ€™s a short introduction of the 3 sentiment lexicons I used to assist with sentiment analysis:\r\nAFINN is a lexicon of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Ã…rup Nielsen in 2009-2011. You can read more about AFINN here.\r\nBing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative. It was first published by Minqing Hu and Bing Liu in 2004. You can read more about Bing here.\r\nThe NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). You can read more about NRC here.\r\nI used these 3 â€œdictionariesâ€ to perform sentiment analysis on the word tokens.\r\nLets do some very basic EDA\r\nUsing the AFINN indexed list of words (data_for_NLP_AFINN_indexed), here is the top 20 most frequently used words in our chatgroup:\r\n\r\n\r\nTable 1: Table 2: Top 20 Most Frequently Used Words in Chatgroup\r\n\r\n\r\nWord\r\n\r\n\r\nCount\r\n\r\n\r\nlove\r\n\r\n\r\n1241\r\n\r\n\r\nleave\r\n\r\n\r\n794\r\n\r\n\r\nnice\r\n\r\n\r\n478\r\n\r\n\r\nwow\r\n\r\n\r\n332\r\n\r\n\r\ndrop\r\n\r\n\r\n272\r\n\r\n\r\nbad\r\n\r\n\r\n244\r\n\r\n\r\nfun\r\n\r\n\r\n225\r\n\r\n\r\nhaha\r\n\r\n\r\n214\r\n\r\n\r\nyeah\r\n\r\n\r\n212\r\n\r\n\r\nsafe\r\n\r\n\r\n201\r\n\r\n\r\njoin\r\n\r\n\r\n186\r\n\r\n\r\nfree\r\n\r\n\r\n184\r\n\r\n\r\nhope\r\n\r\n\r\n184\r\n\r\n\r\ncut\r\n\r\n\r\n179\r\n\r\n\r\nhappy\r\n\r\n\r\n156\r\n\r\n\r\nclean\r\n\r\n\r\n155\r\n\r\n\r\nshit\r\n\r\n\r\n142\r\n\r\n\r\nfine\r\n\r\n\r\n140\r\n\r\n\r\nluck\r\n\r\n\r\n128\r\n\r\n\r\nenjoy\r\n\r\n\r\n125\r\n\r\n\r\nNow hereâ€™s a slight problem. The word â€œleaveâ€, which has a frequency of 794 and carries a sentiment score of â€œ-1â€ in AFINN. As our family frequently uses the chatgroup to coordinate our schedules, the word â€œleaveâ€ is likely taken our of context here.\r\nFor example: â€œLetâ€™s leave at 9amâ€ or â€œWe plan to leave at 7pm today.â€ As opposed to â€ I have going to leave you!â€ which has a negative context.\r\nRemember I mentioned above about adding â€œcustomâ€ stopwords? To avoid potentially skewing the results, I decided removed the word â€œleaveâ€ by including it as a â€œcustom lexiconâ€ stop-word. Thereafter, I ran the analysis using AFINN again.\r\nSimilarly, the word â€œswimâ€ is found within the NRC sentiments of anticipation, fear (really? why?) and joy. As both my sons are competitive swimmers and the word â€œswimâ€ is used very frequently within our chat (760 times to be exact!), â€œswimâ€ was removed as well.\r\nJust for fun, and probably to show off what I learnt in class, here is a word cloud after removing the word â€œleaveâ€.\r\n\r\n\r\n\r\n\r\nFigure 2: WordCloud: Top 50 words after removing â€œleaveâ€\r\n\r\n\r\n\r\nAs you can see, its a rather nice chatgroup to be a part of, with positive words such as â€œloveâ€, â€œniceâ€ and â€œhappyâ€. Of course, there are also negative words being used, of which â€œshitâ€ stands out. Iâ€™m cautiously optimistic that the overall AFINN index of our chat will be positive. ğŸ¤\r\nHave fun with word cloud\r\nWord cloud is rather fun and itâ€™s easy to get carried away. So hereâ€™s a couple more. This is a word cloud of our top 50 most frequently used words with a sentiment score <0.\r\n\r\n\r\n\r\n\r\nFigure 3: Top 50 Frequently used words with negative sentiment score\r\n\r\n\r\n\r\nThis is a word cloud of my top 50 most frequently used words. Hey, I use â€œloveâ€ quite a lot :)\r\n\r\n\r\n\r\n\r\nFigure 4: WordCloud: Markâ€™s Top 50 most frequently used words\r\n\r\n\r\n\r\nHere is Marcâ€™s top 50 most frequently used words. Now we know who uses the word â€œshitâ€ frequently. Still, it looks relatively positive.\r\n\r\n\r\n\r\n\r\nFigure 5: WordCloud: Marcâ€™s Top 50 most frequently used words\r\n\r\n\r\n\r\nThis is Andrewâ€™s top 50 most frequently used words. It would appear that he uses the word â€œloveâ€ quite frequently. ğŸ’–\r\n\r\n\r\n\r\n\r\nFigure 6: WordCloud: Andrewâ€™s Top 50 most frequently used words\r\n\r\n\r\n\r\nLast but not least, here is a word cloud of my wifeâ€™s top 50 most frequently used words.\r\n\r\n\r\n\r\n\r\nFigure 7: WordCloud: Angelinaâ€™s Top 50 most frequently used words\r\n\r\n\r\n\r\nSentiment Analysis using AFINN Index\r\nLetâ€™s shift gears and take a deeper look at what the AFINN indexed list of words reveal about the sentiment within our chatgroup.\r\nThe list of tokenized words were analyzed using the AFINN dictionary, where each word is given a sentiment score ranging from -5 (negative sentiment) to +5 (positive sentiment).\r\nI calculated an average sentiment score and standard error for each member of the family.\r\n\r\n\r\nTable 3: Table 4: Average Sentiment of family members\r\n\r\n\r\nName\r\n\r\n\r\nAverage Sentiment\r\n\r\n\r\nStandard Error\r\n\r\n\r\nAndrew\r\n\r\n\r\n1.378\r\n\r\n\r\n0.074\r\n\r\n\r\nMark\r\n\r\n\r\n0.659\r\n\r\n\r\n0.032\r\n\r\n\r\nAaangelina\r\n\r\n\r\n0.639\r\n\r\n\r\n0.031\r\n\r\n\r\nMarc\r\n\r\n\r\n0.112\r\n\r\n\r\n0.058\r\n\r\n\r\nFamily\r\n\r\n\r\n0.627\r\n\r\n\r\n0.020\r\n\r\n\r\nMy wife and I had an average sentiment score similar to that of the overall â€œFamilyâ€. Andrew had the highest average sentiment score (perhaps he uses the word â€œloveâ€ too often?) while Marc had the lowest average sentiment score.\r\nHere is a boxplot summarizing the results,\r\n\r\n\r\n\r\nFigure 8: Sentiment Analysis of Yeo Family chatgroup (AFINN)\r\n\r\n\r\n\r\nThe difference in average sentiment score between Marc and Andrew stands out. Andrewâ€™s AFINN sentiment score is much higher than Marcâ€™s. Does this simply imply the use of more positive language within the chat, or does it make inference to a difference in their personalities? Can the difference be attributed to age where a younger child tends to be more openly expressive with his feelings, while an older child is more â€œreservedâ€?\r\nPersonally, I am surprised that the AFINN sentiment analysis of our chats was able to quite accutately make inference to the personality of my children. Marc tends to be more reserved and neutral, which may explain his use of words that are less â€œemotionally chargedâ€ and balanced. On the other hand, Andrew is more outgoing, bubbly and optimistic, which may explain his higher average sentiment score. Or maybe he simply loves the word â€œloveâ€ which carries a sentiment score of +3.\r\nCumulative Average Daily Sentiment\r\nNext, I plotted the cumulative average daily sentiment for each member of the family. Iâ€™m not sure what I was hoping to find. If you had a neutral average daily sentiment score, the line should be flat. If you were generally positive, Iâ€™d expect an upward sloping line. Conversely, periods of sustained negative sentiment would result in a downward trend.\r\n\r\n\r\n\r\nFigure 9: Cumulative Average Daily Sentiment Score (AFINN)\r\n\r\n\r\n\r\nA couple of interesting observations:\r\nAndrew had a significant increase in cumulative average daily sentiment score between 2022 and 2023. We discussed this, and brainstormed for possible reasons. I felt that possibly â€œcovid reopeningâ€ and â€œlife back to normalâ€ was a contributing factor to his improved sentiment. Being more outgoing, perhaps covid-restrictions had a larger impact on his sentiment. Andrew thinks that his move to a different competitive swim club with a â€œless toxicâ€ culture and people might have been a contributing factor in lifting his general mood.\r\nThe plot for Marc is almost flat throughout the year, reaching a peak score of only 75. This could imply days of positive average sentiment cancelling out days of negative sentiment, or simply the use of language that is â€œemotionally neutralâ€. Alarmingly, the plot showed a decline from mid-2022 to the start of 2023. We brainstormed for possible reasons to explain this, but none come to mind.\r\nHere is Marcâ€™s plot.\r\n\r\n\r\n\r\nFigure 10: Marcâ€™s Cumulative Average Daily Sentiment Score (AFINN)\r\n\r\n\r\n\r\nSentiment Analysis: Marc vs Andrew\r\nI am curious to explore the difference in sentiment scores between Marc and Andrew. To investigate further, I looked for words that they used in common, and analyzed to see if the frequency of usage of these words contributed to the difference in their sentiment scores.\r\nTo do this, I first created individual AFINN-indexed word lists for each child. Marc used 1783 AFINN-indexed words, while Andrew used 921 AFINN-indexed words. I then calculated their respective frequency of AFINN-indexed word usage.\r\nNext, I created a list of AFINN-indexed words that they used in common using inner_join(). This resulted in a list of 138 AFINN-indexed words that they used in common.\r\nI created a plot of these 138 words that they used in common, with the frequency of word usage by Andrew on the x-axis, and the frequency of word usage by Marc on the y-axis.\r\n\r\n\r\n\r\nFigure 11: Frequency of Usage (Words in Common) Marc vs Andrew\r\n\r\n\r\n\r\nIts quite apparent that the frequency of usage of the word â€œloveâ€ by Andrew is an outlier (32% vs 2% usage by Marc). In order to get a clearer picture of the other words in use, I removed the word â€œloveâ€ before generating the same plot again.\r\n\r\n\r\n\r\nFigure 12: Frequency of Word Usage (Words in Common) Marc vs Andrew. The word love has been removed.\r\n\r\n\r\n\r\nYou may notice (or if you actually counted), not all 138 (or 137 minus the word â€œloveâ€) words have been plotted. That is because â€œcheck_overlapâ€ in geom_text() has been set to TRUE to avoid the text from overlapping.\r\nWords that are close to or on the blue line (AB Line with slope = 1, intercept = 0) represent words that both children use with similar frequency. Examples of these words are â€œsuperâ€, â€œdiedâ€, â€œstupidâ€, â€œhappyâ€, and â€œlolâ€. Marc uses the words â€œniceâ€ and â€œshitâ€ more frequently than Andrew, while Andrew uses the words â€œhahaâ€, â€œyeahâ€, and â€œcareâ€ more frequently than Marc.\r\nIs it apparent now why Andrew might have a higher AFINN sentiment score in his chats? Maybe the next plot will give a clearer indication. I multiplied the frequency of each word used by its AFINN sentiment value.\r\n\r\n\r\n\r\nFigure 13: Frequency of word used x AFINN Sentiment Value\r\n\r\n\r\n\r\nAgain, we see the use or excessive use of the word â€œloveâ€ most likely skewing Andrewâ€™s sentiment score vs Marc, which is not a bad thing :). Letâ€™s look at the same plot with the word â€˜loveâ€™ removed.\r\n\r\n\r\n\r\nFigure 14: Frequency of word used x AFINN Sentiment Value. The word â€˜loveâ€™ has been removed.\r\n\r\n\r\n\r\nTokenizing by n-grams\r\nThus far, Iâ€™ve only considered words as individual units, and evaluated their relationships to sentiments. However, tokenizing by single-words has its drawbacks because relationships exist between words that follow one-another.\r\nThis is because words correlate with one another, and the choice of first word can significantly change the meaning of the second word. For example, putting â€œnotâ€ before â€œloveâ€ significantly changes the meaning and therefore sentiment.\r\nFor the purpose of this project, I shall limit my analysis to pairs of words, or â€œbigramsâ€. You can certainly analyze further by using â€œtrigramsâ€ (n=3). Hence more generally, a token comprising n words is called an â€œn-gram or ngramâ€. Tokenizing on bigrams would allow me to capture and examine the immediate context around each word.\r\nThe output is word-pairs in the column bigram. The bigrams are then separated into columns word1 and word2, before filtering for numbers and stopwords. You can choose to recombine word1 and word2 back into bigrams, but I chose to leave them separated.\r\nHereâ€™s a table of the top 20 â€œpaired wordsâ€ in terms of frequency of usage:\r\n\r\n\r\nTable 5: Table 6: Top 20 Bigrams\r\n\r\n\r\nword1\r\n\r\n\r\nword2\r\n\r\n\r\nfrequency\r\n\r\n\r\nlove\r\n\r\n\r\nyâ€™all\r\n\r\n\r\n160\r\n\r\n\r\nsleep\r\n\r\n\r\ntight\r\n\r\n\r\n154\r\n\r\n\r\nyâ€™all\r\n\r\n\r\ntmr\r\n\r\n\r\n144\r\n\r\n\r\nbed\r\n\r\n\r\nbugs\r\n\r\n\r\n106\r\n\r\n\r\nbugs\r\n\r\n\r\nbite\r\n\r\n\r\n106\r\n\r\n\r\ncoming\r\n\r\n\r\nhome\r\n\r\n\r\n105\r\n\r\n\r\ntight\r\n\r\n\r\ndonâ€™t\r\n\r\n\r\n103\r\n\r\n\r\ntmr\r\n\r\n\r\nsleep\r\n\r\n\r\n103\r\n\r\n\r\nbye\r\n\r\n\r\nbye\r\n\r\n\r\n100\r\n\r\n\r\nkor\r\n\r\n\r\nkor\r\n\r\n\r\n79\r\n\r\n\r\nmom\r\n\r\n\r\nlove\r\n\r\n\r\n76\r\n\r\n\r\nwater\r\n\r\n\r\npolo\r\n\r\n\r\n68\r\n\r\n\r\notw\r\n\r\n\r\nhome\r\n\r\n\r\n66\r\n\r\n\r\nbowling\r\n\r\n\r\nalley\r\n\r\n\r\n63\r\n\r\n\r\ncar\r\n\r\n\r\npark\r\n\r\n\r\n63\r\n\r\n\r\npls\r\n\r\n\r\ndonâ€™t\r\n\r\n\r\n61\r\n\r\n\r\nmarcy\r\n\r\n\r\npls\r\n\r\n\r\n58\r\n\r\n\r\ndonâ€™t\r\n\r\n\r\nworry\r\n\r\n\r\n57\r\n\r\n\r\nsweet\r\n\r\n\r\ndreams\r\n\r\n\r\n55\r\n\r\n\r\nyeah\r\n\r\n\r\nyeah\r\n\r\n\r\n54\r\n\r\n\r\nNope! We donâ€™t have bed bugs at home. Instead, the bigrams come from our favorite good-night phrase â€œsleep tight, donâ€™t let the bed bugs biteâ€.\r\nGoing beyond counting, we can actually visualize the connections and correlations between words in the chatgroup thanks to igraph.\r\n\r\n\r\n\r\nFigure 15: igraph Plot of Top 50 bigrams\r\n\r\n\r\n\r\nAnd hereâ€™s the cool looking igraph, where arrows show the direction of correlation between words. The weights of the arrows shoe the number of times each bigram is used. The graph plots the top 50 bigrams used in our chat. Thatâ€™s so cool!\r\nAs you can see, there arenâ€™t any â€œnegating wordsâ€ such as â€œnot loveâ€ or bigrams with â€œnegative-positiveâ€ sentiment within the top 50 bigrams. As the bulk of this chat happened during the covid and post-covid period, youâ€™ll see interesting bigrams like â€œwear maskâ€, â€œstay safeâ€ and â€œsore throatâ€.\r\nSentiment Analysis using Bing\r\nRecall that Bing is a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\r\nHere is the top 10 most frequently used words in our chatgroup, indexed using Bing. Within this list, there are 9 positive sentiment words and 1 negative sentiment word.\r\n\r\n\r\nTable 7: Table 8: Top 10 most frequently used words, indexed by Bing\r\n\r\n\r\nword1\r\n\r\n\r\nword2\r\n\r\n\r\nfrequency\r\n\r\n\r\npositive\r\n\r\n\r\nlove\r\n\r\n\r\n1241\r\n\r\n\r\npositive\r\n\r\n\r\nnice\r\n\r\n\r\n478\r\n\r\n\r\npositive\r\n\r\n\r\nready\r\n\r\n\r\n370\r\n\r\n\r\npositive\r\n\r\n\r\nwow\r\n\r\n\r\n332\r\n\r\n\r\nnegative\r\n\r\n\r\nbad\r\n\r\n\r\n244\r\n\r\n\r\npositive\r\n\r\n\r\nfun\r\n\r\n\r\n225\r\n\r\n\r\npositive\r\n\r\n\r\nsafe\r\n\r\n\r\n201\r\n\r\n\r\npositive\r\n\r\n\r\nfree\r\n\r\n\r\n184\r\n\r\n\r\npositive\r\n\r\n\r\nhappy\r\n\r\n\r\n156\r\n\r\n\r\npositive\r\n\r\n\r\nclean\r\n\r\n\r\n155\r\n\r\n\r\nHere is a plot comparing the top 10 negative and positive sentiment words used in our chat. Iâ€™m happy to see that the use of positive sentiment words far overwhelm the use of negative sentiment words.\r\n\r\n\r\n\r\nFigure 16: Top 10 Negative and Positive Sentiment Words, indexed by Bing\r\n\r\n\r\n\r\nHereâ€™s a different way of looking at the same data, this time the usage of negative sentiment words is represented on the negative x-axis. Which way of visualization do you prefer?\r\n\r\n\r\n\r\nFigure 17: Top 10 Negative and Positive Sentiment Words, indexed by Bing, represented on negative-positive scale\r\n\r\n\r\n\r\nNet Sentiment Using Bing\r\nAlthough Bing classifies words in a binary manner, positive or negative, it is still possible to calculate a score that gives you an indication of the overall sentiment. Each occurrence of a positive word is given a sentiment score of â€œ+1â€, while a negative word is given a sentiment score of â€œ-1â€. This allows me to calculate a net sentiment score, and in this case, I did so for each member of the family.\r\nWhat do you think this analysis will reveal? Are you surprised by the result?\r\n\r\n\r\nTable 9: Table 10: Net Sentiment Score, indexed by Bing\r\n\r\n\r\nFamily Member\r\n\r\n\r\nNet Sentiment\r\n\r\n\r\nAaangelina\r\n\r\n\r\n635\r\n\r\n\r\nAndrew\r\n\r\n\r\n184\r\n\r\n\r\nMarc\r\n\r\n\r\n-168\r\n\r\n\r\nMark\r\n\r\n\r\n734\r\n\r\n\r\nNext, I investigated the top 10 most frequently used positive and negative words by each member of the family. The chart was easily created using facet_wrap().\r\n\r\n\r\n\r\nFigure 18: Top 10 Frequently used Positive and Negative words, indexed by Bing, for each family member\r\n\r\n\r\n\r\nRecall from earlier, I am the top user of the word â€œloveâ€ within the chat group. Well, Bing picked that up too. Now, why isnâ€™t my AFINN average sentiment index much higher? Hmmm, the only plausible explanation is that I must be using negative sentiment words quite frequently as well. ğŸ¤·â€â™‚ï¸\r\nAs for Marc, his number of negative sentiment words used (1038) outnumbers the number of positive sentiment words used (870). But his top 10 negative and positive words used is quite balanced.\r\nSentiment Analysis using NRC\r\nThe NRC Emotion Lexicon is a list of 5,636 English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). A word can be listed under multiple emotions. For example, the word â€œbadâ€ is listed under the emotions anger, disgust, fear, and sadness, as well as the negative sentiment.\r\nA total of 35,688 words from the chatgroup were indexed by NRC, and here is a summary by the eight basic emotions:\r\n\r\n\r\nTable 11: Table 12: Proportion of words in each NRC sentiment\r\n\r\n\r\nSentiment\r\n\r\n\r\nNumber of Words\r\n\r\n\r\nFrequency\r\n\r\n\r\nanticipation\r\n\r\n\r\n9556\r\n\r\n\r\n26.78\r\n\r\n\r\ntrust\r\n\r\n\r\n6775\r\n\r\n\r\n18.98\r\n\r\n\r\njoy\r\n\r\n\r\n5563\r\n\r\n\r\n15.59\r\n\r\n\r\nfear\r\n\r\n\r\n3549\r\n\r\n\r\n9.94\r\n\r\n\r\nsadness\r\n\r\n\r\n3127\r\n\r\n\r\n8.76\r\n\r\n\r\nanger\r\n\r\n\r\n2459\r\n\r\n\r\n6.89\r\n\r\n\r\nsurprise\r\n\r\n\r\n2404\r\n\r\n\r\n6.74\r\n\r\n\r\ndisgust\r\n\r\n\r\n2255\r\n\r\n\r\n6.32\r\n\r\n\r\nI always prefer to look at things visually, so hereâ€™s a plot of the Top 10 words for each of the eight emotions.\r\n\r\n\r\n\r\nFigure 19: Top 10 Words Categorized by NRC into Eight Basic Emotions\r\n\r\n\r\n\r\nIts comes as no surprise to me that about 27 percent of words were categorized under the â€œanticipationâ€ emotion. After all, we do use the chat quite frequently to coordinate our schedules and transportation needs. Its interesting that NRC was able to pick this up.\r\nHowever, I noticed that NRC does have its quirks. Take a look at the top word in the â€œfearâ€ emotion. Mum?? I know my wife is a scary person, but how did NRC know that? â€œMumâ€ to refer to my wife or my mother (or their mother, if you were my children). Perhaps in this case, NRC mistook it as â€œto keep quietâ€? Perhaps we should spell it as â€œmomâ€ from now on in our chats?\r\nDo you notice any other strange classifications? battery <-> Anger, mother <-> Sadness are a few more that may have been incorrectly classified.\r\nThe next plot shows the relative frequency of each family memberâ€™s word usage by the eight basic NRC sentiments.\r\n\r\n\r\n\r\nFigure 20: Relative Frequency of Family Membersâ€™ Word Usage by Sentiment, indexed by NRC\r\n\r\n\r\n\r\nWhat do you make of the results? Well, as parents, I suppose its normal that Angelina and I have the highest frequency of words in the â€œanticipationâ€ category. Its also heartening to see that categories associated with negative sentiment (fear, anger, sadness and disgust) do not rank highly among our family members.\r\nFinal Thoughts\r\nI had fun working on this project. Not only did my skills in R impRove, it also revealed some interesting insights about my family members.\r\nWas the analysis ground-breaking? Definitely not, and admittedly, there was a lot more I could have done, eg: tf-idf, analysis by tri-grams, by topic, or even at the sentence level. Maybe Iâ€™ll do a more comprehensive analysis after I complete the next module on Machine Learning next year. Thanks for reading!\r\n\r\n\r\n\r\n",
      "last_modified": "2024-03-07T08:19:12+08:00"
    }
  ],
  "collections": ["ml/ml.json"]
}
